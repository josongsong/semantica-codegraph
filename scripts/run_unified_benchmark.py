#!/usr/bin/env python3
"""
UnifiedOrchestrator ë²¤ì¹˜ë§ˆí¬ ëŸ¬ë„ˆ

ì‹¤ì œ ëŒ€ê·œëª¨ ë¦¬í¬ì§€í† ë¦¬ë¡œ ì¸ë±ì‹± ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

Usage:
    python scripts/run_unified_benchmark.py
    python scripts/run_unified_benchmark.py --clone  # ë¦¬í¬ì§€í† ë¦¬ í´ë¡ ë¶€í„° ì‹œì‘
"""

import argparse
import json
import subprocess
import sys
from pathlib import Path
from typing import List, Dict, Any
import time


class BenchmarkRepo:
    """ë²¤ì¹˜ë§ˆí¬ ëŒ€ìƒ ë¦¬í¬ì§€í† ë¦¬"""

    def __init__(self, name: str, git_url: str, size_category: str):
        self.name = name
        self.git_url = git_url
        self.size_category = size_category  # small, medium, large

    def __repr__(self):
        return f"BenchmarkRepo({self.name}, {self.size_category})"


# ë²¤ì¹˜ë§ˆí¬ ëŒ€ìƒ ë¦¬í¬ì§€í† ë¦¬ë“¤
BENCHMARK_REPOS = [
    # Small (< 1MB, < 100 files)
    BenchmarkRepo("typer", "https://github.com/tiangolo/typer.git", "small"),
    BenchmarkRepo("attrs", "https://github.com/python-attrs/attrs.git", "small"),
    # Medium (1-10MB, 100-1000 files)
    BenchmarkRepo("rich", "https://github.com/Textualize/rich.git", "medium"),
    BenchmarkRepo("httpx", "https://github.com/encode/httpx.git", "medium"),
    # Large (> 10MB, > 1000 files)
    BenchmarkRepo("django", "https://github.com/django/django.git", "large"),
    BenchmarkRepo("flask", "https://github.com/pallets/flask.git", "large"),
    BenchmarkRepo("pydantic", "https://github.com/pydantic/pydantic.git", "large"),
]


class UnifiedBenchmarkRunner:
    """UnifiedOrchestrator ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë° ë¶„ì„"""

    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.repos_dir = base_dir / "tools" / "benchmark" / "repo-test"
        self.results_dir = base_dir / "packages" / "codegraph-ir" / "target" / "benchmark_results"
        self.results_dir.mkdir(parents=True, exist_ok=True)

    def clone_repositories(self, force: bool = False):
        """ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬ì§€í† ë¦¬ í´ë¡ """
        print("\n" + "=" * 60)
        print("ğŸ“¦ Cloning Benchmark Repositories")
        print("=" * 60 + "\n")

        for repo in BENCHMARK_REPOS:
            category_dir = self.repos_dir / repo.size_category
            repo_path = category_dir / repo.name

            if repo_path.exists() and not force:
                print(f"âœ“ {repo.name} already exists, skipping")
                continue

            category_dir.mkdir(parents=True, exist_ok=True)

            print(f"ğŸ”„ Cloning {repo.name} ({repo.size_category})...")
            try:
                subprocess.run(
                    ["git", "clone", "--depth=1", repo.git_url, str(repo_path)], check=True, capture_output=True
                )
                print(f"âœ“ {repo.name} cloned successfully")
            except subprocess.CalledProcessError as e:
                print(f"âœ— Failed to clone {repo.name}: {e}")

    def run_rust_benchmark(self) -> bool:
        """Rust ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (cargo test)"""
        print("\n" + "=" * 60)
        print("ğŸš€ Running Rust Benchmark Suite")
        print("=" * 60 + "\n")

        try:
            result = subprocess.run(
                [
                    "cargo",
                    "test",
                    "--package",
                    "codegraph-ir",
                    "--bench",
                    "unified_orchestrator_bench",
                    "--",
                    "--ignored",
                    "--nocapture",
                ],
                cwd=self.base_dir,
                check=True,
                capture_output=False,  # Print to console
            )
            return result.returncode == 0
        except subprocess.CalledProcessError as e:
            print(f"\nâŒ Benchmark failed: {e}")
            return False

    def run_single_repo_benchmark(self, repo: BenchmarkRepo) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¦¬í¬ì§€í† ë¦¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (Pythonì—ì„œ ì§ì ‘)"""
        category_dir = self.repos_dir / repo.size_category
        repo_path = category_dir / repo.name

        if not repo_path.exists():
            print(f"âš ï¸  Repository not found: {repo_path}")
            return {}

        print(f"\nğŸ” Benchmarking: {repo.name}")
        print(f"   Path: {repo_path}")

        # Rust ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        start_time = time.time()

        try:
            # TODO: Python ë°”ì¸ë”© êµ¬í˜„ í›„ ì§ì ‘ í˜¸ì¶œ ê°€ëŠ¥
            # from codegraph_ir import UnifiedOrchestrator, UnifiedOrchestratorConfig
            # config = UnifiedOrchestratorConfig(str(repo_path), repo.name)
            # orchestrator = UnifiedOrchestrator(config)
            # orchestrator.index_repository()

            # í˜„ì¬ëŠ” Rust ë²¤ì¹˜ë§ˆí¬ ì‚¬ìš©
            result = subprocess.run(
                [
                    "cargo",
                    "test",
                    "--package",
                    "codegraph-ir",
                    "--bench",
                    "unified_orchestrator_bench",
                    "bench_small_fixture",
                    "--",
                    "--nocapture",
                ],
                cwd=self.base_dir,
                capture_output=True,
                text=True,
                check=False,
            )

            duration = time.time() - start_time

            return {
                "repo_name": repo.name,
                "size_category": repo.size_category,
                "duration": duration,
                "success": result.returncode == 0,
                "output": result.stdout,
            }

        except Exception as e:
            print(f"âŒ Failed: {e}")
            return {
                "repo_name": repo.name,
                "size_category": repo.size_category,
                "success": False,
                "error": str(e),
            }

    def analyze_results(self):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±"""
        csv_path = self.results_dir / "benchmark_results.csv"

        if not csv_path.exists():
            print(f"\nâš ï¸  No results found at {csv_path}")
            return

        print("\n" + "=" * 60)
        print("ğŸ“Š Analyzing Benchmark Results")
        print("=" * 60 + "\n")

        # CSV íŒŒì‹±
        import csv

        results = []
        with open(csv_path, "r") as f:
            reader = csv.DictReader(f)
            for row in reader:
                results.append(row)

        if not results:
            print("âš ï¸  No results to analyze")
            return

        # ê²°ê³¼ ì¶œë ¥
        print(f"ğŸ“‹ Total benchmarks: {len(results)}\n")

        for result in results:
            print(f"â€¢ {result['repo_name']}")
            print(f"  - Size: {float(result['size_mb']):.2f} MB")
            print(f"  - Nodes: {result['nodes']}")
            print(f"  - Duration: {float(result['duration_sec']):.2f}s")
            print(f"  - Throughput: {float(result['throughput_nodes_sec']):.0f} nodes/sec")
            print()

        # í†µê³„
        total_nodes = sum(int(r["nodes"]) for r in results)
        total_duration = sum(float(r["duration_sec"]) for r in results)
        avg_throughput = sum(float(r["throughput_nodes_sec"]) for r in results) / len(results)

        print("ğŸ“ˆ Summary Statistics:")
        print(f"  - Total nodes processed: {total_nodes:,}")
        print(f"  - Total duration: {total_duration:.2f}s")
        print(f"  - Average throughput: {avg_throughput:.0f} nodes/sec")

        # JSON ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_benchmarks": len(results),
            "total_nodes": total_nodes,
            "total_duration": total_duration,
            "average_throughput": avg_throughput,
            "results": results,
        }

        report_path = self.results_dir / "benchmark_report.json"
        with open(report_path, "w") as f:
            json.dump(report, f, indent=2)

        print(f"\nğŸ“„ Report saved to: {report_path}")

    def generate_markdown_report(self):
        """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        csv_path = self.results_dir / "benchmark_results.csv"

        if not csv_path.exists():
            return

        import csv

        results = []
        with open(csv_path, "r") as f:
            reader = csv.DictReader(f)
            results = list(reader)

        if not results:
            return

        md_content = f"""# UnifiedOrchestrator Benchmark Report

**Generated**: {time.strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ“Š Summary

- **Total Benchmarks**: {len(results)}
- **Total Nodes**: {sum(int(r["nodes"]) for r in results):,}
- **Average Throughput**: {sum(float(r["throughput_nodes_sec"]) for r in results) / len(results):.0f} nodes/sec

## ğŸ“‹ Detailed Results

| Repository | Size (MB) | Files | Nodes | Edges | Duration (s) | Throughput (nodes/s) |
|------------|-----------|-------|-------|-------|--------------|---------------------|
"""

        for r in results:
            md_content += f"| {r['repo_name']} | {float(r['size_mb']):.2f} | {r['file_count']} | {r['nodes']} | {r['edges']} | {float(r['duration_sec']):.2f} | {float(r['throughput_nodes_sec']):.0f} |\n"

        md_content += """
## ğŸ† Performance Highlights

"""

        # Find best performers
        fastest = max(results, key=lambda r: float(r["throughput_nodes_sec"]))
        largest = max(results, key=lambda r: int(r["nodes"]))

        md_content += (
            f"- **Fastest**: {fastest['repo_name']} ({float(fastest['throughput_nodes_sec']):.0f} nodes/sec)\n"
        )
        md_content += f"- **Largest**: {largest['repo_name']} ({largest['nodes']} nodes)\n"

        md_path = self.results_dir / "BENCHMARK_REPORT.md"
        with open(md_path, "w") as f:
            f.write(md_content)

        print(f"ğŸ“„ Markdown report saved to: {md_path}")


def main():
    parser = argparse.ArgumentParser(description="UnifiedOrchestrator Benchmark Runner")
    parser.add_argument("--clone", action="store_true", help="Clone repositories first")
    parser.add_argument("--force-clone", action="store_true", help="Force re-clone repositories")
    parser.add_argument("--analyze-only", action="store_true", help="Only analyze existing results")

    args = parser.parse_args()

    # Find codegraph root
    base_dir = Path(__file__).parent.parent

    runner = UnifiedBenchmarkRunner(base_dir)

    # Clone repositories if requested
    if args.clone or args.force_clone:
        runner.clone_repositories(force=args.force_clone)

    # Run benchmarks (skip if analyze-only)
    if not args.analyze_only:
        success = runner.run_rust_benchmark()

        if not success:
            print("\nâŒ Benchmark failed!")
            sys.exit(1)

    # Analyze results
    runner.analyze_results()
    runner.generate_markdown_report()

    print("\nâœ… Benchmark complete!")


if __name__ == "__main__":
    main()
