#!/usr/bin/env python3
"""MCP ì„œë²„ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸"""

import asyncio
import os

from mcp.server import Server
from mcp.server.models import InitializationOptions
from mcp.server.stdio import stdio_server
from mcp.types import Tool

from apps.mcp.mcp.adapters.mcp.services import MCPSearchService
from apps.mcp.mcp.adapters.search.chunk_retriever import create_chunk_retriever
from apps.mcp.mcp.adapters.search.symbol_retriever import create_symbol_retriever
from apps.mcp.mcp.adapters.store.factory import create_all_stores

# RFC-052/053: Use new handlers from apps.mcp.mcp.handlers
from apps.mcp.mcp.handlers import (
    analyze_cost,
    analyze_race,
    force_reindex,  # Admin tool (Tier 2)
    get_context,
    get_definition,
    get_references,
    job_cancel,
    job_result,
    job_status,
    job_submit,
    preview_callers,
    preview_impact,
    preview_taint_path,
    search,  # RFC-053 Tier 0
    verify_finding_resolved,
    verify_patch_compile,
)
from apps.orchestrator.orchestrator.adapters.context_adapter import ContextAdapter
from codegraph_shared.infra.config.logging import setup_logging

setup_logging()

# ============================================================
# í™˜ê²½ ì„¤ì •: ë¶„ì„í•  ë ˆí¬ì§€í† ë¦¬ ê²½ë¡œ
# ============================================================

# í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •í•˜ê±°ë‚˜, í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì‚¬ìš©
TARGET_REPO_PATH = os.getenv(
    "CODEGRAPH_REPO_PATH",
    os.getcwd(),  # ê¸°ë³¸ê°’: MCP ì„œë²„ë¥¼ ì‹¤í–‰í•œ ë””ë ‰í† ë¦¬
)

# File watching í™œì„±í™” ì—¬ë¶€
ENABLE_FILE_WATCHING = os.getenv("CODEGRAPH_WATCH", "true").lower() in ("true", "1", "yes")

# Log configuration (not print - would pollute MCP stdout)
import logging

_logger = logging.getLogger(__name__)
_logger.info(f"Target Repository: {TARGET_REPO_PATH}")
_logger.info(f"File Watching: {'Enabled' if ENABLE_FILE_WATCHING else 'Disabled'}")

# ============================================================
# ìë™ ì¸ë±ì‹± ì²´í¬ (ì²« ìš”ì²­ ì‹œ)
# ============================================================

from enum import Enum
from typing import Any


class IndexCheckResult(Enum):
    """
    ì¸ë±ìŠ¤ ìƒíƒœ ì²´í¬ ê²°ê³¼ (ë‚´ë¶€ ë¡œì§ìš© ENUM).

    Values:
        COMPLETED: ì¸ë±ì‹± ì™„ë£Œ (ìºì‹œ ì‚¬ìš©)
        IN_PROGRESS: ì¸ë±ì‹± ì§„í–‰ ì¤‘ (ìŠ¤í‚µ)
        NOT_FOUND: ì¸ë±ìŠ¤ ì—†ìŒ (ì¸ë±ì‹± í•„ìš”)
        ERROR: ì²´í¬ ì‹¤íŒ¨
    """

    COMPLETED = "completed"
    IN_PROGRESS = "in_progress"
    NOT_FOUND = "not_found"
    ERROR = "error"


class IndexStatusLoader:
    """
    L3 Database Loader for Index Status (Protocol êµ¬í˜„).

    Big Tech L11: Protocol-based design with STALE detection.

    Features:
    - STALE ê°ì§€: IN_PROGRESSê°€ ì˜¤ë˜ë˜ë©´ NOT_FOUND ì²˜ë¦¬
    - Timestamp ê¸°ë°˜ íŒë‹¨
    - Graceful degradation
    """

    def __init__(self, indexing_timeout: int = 1800):
        """
        Initialize loader.

        Args:
            indexing_timeout: ì¸ë±ì‹± íƒ€ì„ì•„ì›ƒ (ì´ˆ, STALE ê°ì§€ìš©)
        """
        self._version_store = None
        self._indexing_timeout = indexing_timeout

    async def load(self, key: str) -> IndexCheckResult | None:
        """
        DBì—ì„œ ì¸ë±ìŠ¤ ìƒíƒœ ë¡œë“œ (L3 tier).

        Big Tech L11: STALE detection for hanging IN_PROGRESS.

        Args:
            key: repo_id (e.g., "default")

        Returns:
            IndexCheckResult or None

        Logic:
            1. DBì—ì„œ latest version ì¡°íšŒ
            2. COMPLETED â†’ COMPLETED
            3. IN_PROGRESS:
               - created_atì´ indexing_timeout ì´ë‚´ â†’ IN_PROGRESS
               - created_atì´ indexing_timeout ì´ˆê³¼ â†’ STALE (None ë°˜í™˜)
            4. ê¸°íƒ€ â†’ None
        """
        try:
            from datetime import datetime

            from codegraph_shared.common.observability import get_logger

            logger = get_logger(__name__)

            # Lazy init version_store
            if not self._version_store:
                from codegraph_engine.multi_index.infrastructure.version.store import IndexVersionStore
                from codegraph_shared.infra.storage.postgres import PostgresStore

                postgres = PostgresStore()
                self._version_store = IndexVersionStore(postgres_store=postgres)

            # Get latest version
            latest = await self._version_store.get_latest_version(repo_id=key)

            if not latest:
                return None  # Not found

            # Check status
            from codegraph_engine.multi_index.infrastructure.version.models import IndexVersionStatus

            if latest.status == IndexVersionStatus.COMPLETED:
                logger.debug(f"L3 hit: Index v{latest.version_id} ({latest.file_count} files)")
                return IndexCheckResult.COMPLETED

            elif latest.status == IndexVersionStatus.INDEXING:
                # STALE ê°ì§€: IN_PROGRESSê°€ ì˜¤ë˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if hasattr(latest, "created_at") and latest.created_at:
                    now = datetime.now()
                    # created_atì´ datetimeì´ë©´ ì§ì ‘ ë¹„êµ
                    if isinstance(latest.created_at, datetime):
                        elapsed = (now - latest.created_at).total_seconds()
                    else:
                        # timestampë¼ë©´ ë³€í™˜
                        elapsed = (now - datetime.fromtimestamp(latest.created_at)).total_seconds()

                    if elapsed > self._indexing_timeout:
                        logger.warning(
                            f"IN_PROGRESS is STALE (elapsed: {elapsed:.0f}s > {self._indexing_timeout}s), "
                            f"treating as NOT_FOUND"
                        )
                        return None  # STALE â†’ NOT_FOUND

                # Still valid IN_PROGRESS
                return IndexCheckResult.IN_PROGRESS

            else:
                # FAILED or unexpected â†’ NOT_FOUND
                return None

        except Exception as e:
            from codegraph_shared.common.observability import get_logger

            logger = get_logger(__name__)
            logger.debug(f"L3 load failed: {e}")
            return None  # Graceful degradation

    async def save(self, key: str, value: IndexCheckResult) -> None:
        """ì €ì¥ (read-onlyì´ë¯€ë¡œ no-op)."""
        pass

    async def delete(self, key: str) -> None:
        """ì‚­ì œ (read-onlyì´ë¯€ë¡œ no-op)."""
        pass


# ============================================================
# 3-Tier Cache for Index Status (Big Tech L11)
# ============================================================

_index_status_cache: Any = None  # ThreeTierCache[IndexCheckResult]
_indexing_done = False
_indexing_in_progress = False


async def _invalidate_index_cache(repo_id: str) -> None:
    """
    Invalidate index status cache for repo.

    Big Tech L11: Cache invalidation for force_reindex.

    Args:
        repo_id: Repository ID to invalidate

    Side Effects:
        - L1 (ë©”ëª¨ë¦¬) ì‚­ì œ
        - L2 (Redis) ì‚­ì œ
        - L3 (DB)ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ (source of truth)
        - _indexing_done í”Œë˜ê·¸ ë¦¬ì…‹
    """
    global _indexing_done

    cache = _get_index_cache()

    if cache:
        try:
            await cache.delete(repo_id)
            from codegraph_shared.common.observability import get_logger

            logger = get_logger(__name__)
            logger.info(f"Index cache invalidated for {repo_id}")
        except Exception as e:
            from codegraph_shared.common.observability import get_logger

            logger = get_logger(__name__)
            logger.warning(f"Cache invalidation failed: {e}")

    # Reset flag to allow re-check
    _indexing_done = False


def _get_index_cache() -> Any:
    """
    Get or create index status cache.

    Big Tech L11: Config-driven TTL + STALE detection.

    Returns:
        ThreeTierCache instance or None
    """
    global _index_status_cache

    if _index_status_cache is not None:
        return _index_status_cache

    try:
        from apps.mcp.mcp.config import get_index_status_cache_config
        from codegraph_shared.infra.cache.three_tier_cache import ThreeTierCache

        cache_config = get_index_status_cache_config()

        # L1: Always available (in-memory LRU)
        # L2: Redis (if available)
        # L3: DB (IndexVersionStore with STALE detection)

        l2_redis = None
        # Try to init Redis (graceful degradation)
        try:
            from codegraph_shared.infra.cache.redis_adapter import RedisAdapter

            l2_redis = RedisAdapter()
        except Exception:
            pass  # Redis unavailable, L2 disabled

        # L3 loader with STALE detection
        l3_loader = IndexStatusLoader(indexing_timeout=cache_config.indexing_timeout)

        _index_status_cache = ThreeTierCache(
            l1_maxsize=cache_config.l1_maxsize,
            l2_redis=l2_redis,
            l3_loader=l3_loader,
            ttl=cache_config.ttl_in_progress,  # Use shortest TTL (ë¹ ë¥¸ ì¬ì²´í¬)
            namespace="index_status",
        )

        return _index_status_cache

    except Exception as e:
        from codegraph_shared.common.observability import get_logger

        logger = get_logger(__name__)
        logger.warning(f"Failed to create index cache: {e}")
        return None


async def check_index_status(repo_path: str, repo_id: str = "default") -> IndexCheckResult:
    """
    ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸ (Big Tech L11: 3-Tier Cache).

    Args:
        repo_path: Repository path
        repo_id: Repository ID

    Returns:
        IndexCheckResult enum

    Strategy (SOTA):
        1. L1 (ë©”ëª¨ë¦¬) ìºì‹œ ì¡°íšŒ (~0.1ms)
        2. L2 (Redis) ìºì‹œ ì¡°íšŒ (~1ms, ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ ê³µìœ )
        3. L3 (DB) IndexVersionStore ì¡°íšŒ (~10ms)
        4. ìºì‹œ ì›Œë°: L3 â†’ L2 â†’ L1

    Performance:
        - Cache hit: < 1ms (L1/L2)
        - Cache miss: ~10ms (L3)
        - TTL: 5min (ìë™ invalidation)
    """
    from codegraph_shared.common.observability import get_logger

    logger = get_logger(__name__)

    try:
        # Get 3-tier cache
        cache = _get_index_cache()

        if cache is None:
            # Fallback: direct DB query (no cache)
            logger.warning("Cache unavailable, using direct DB query")
            return await _check_index_status_direct(repo_id)

        # Cache lookup (L1 â†’ L2 â†’ L3)
        status = await cache.get(repo_id)

        if status is None:
            # All tiers miss â†’ NOT_FOUND
            logger.info("Index not found (all cache tiers miss)")
            return IndexCheckResult.NOT_FOUND

        # Cache hit
        logger.debug(f"Index status: {status.value} (cache hit)")
        return status

    except Exception as e:
        logger.warning(f"Index status check failed: {e}")
        return IndexCheckResult.ERROR


async def _check_index_status_direct(repo_id: str) -> IndexCheckResult:
    """
    Direct DB query fallback (no cache).

    Args:
        repo_id: Repository ID

    Returns:
        IndexCheckResult enum
    """
    from codegraph_shared.common.observability import get_logger

    logger = get_logger(__name__)

    try:
        # Try to query IndexVersionStore
        from codegraph_engine.multi_index.infrastructure.version.store import IndexVersionStore
        from codegraph_shared.infra.storage.postgres import PostgresStore

        postgres = PostgresStore()
        version_store = IndexVersionStore(postgres_store=postgres)

        latest = await version_store.get_latest_version(repo_id=repo_id)

        if not latest:
            return IndexCheckResult.NOT_FOUND

        from codegraph_engine.multi_index.infrastructure.version.models import IndexVersionStatus

        if latest.status == IndexVersionStatus.COMPLETED:
            logger.info(f"Index exists (v{latest.version_id}, {latest.file_count} files, {latest.git_commit[:8]})")
            return IndexCheckResult.COMPLETED
        elif latest.status == IndexVersionStatus.INDEXING:
            return IndexCheckResult.IN_PROGRESS
        else:
            return IndexCheckResult.NOT_FOUND

    except Exception as e:
        logger.debug(f"Direct DB check failed: {e}")
        return IndexCheckResult.NOT_FOUND  # Graceful: assume not indexed


async def ensure_indexed(repo_id: str = "default"):
    """
    ì²« ìš”ì²­ ì‹œ ìë™ìœ¼ë¡œ ë ˆí¬ì§€í† ë¦¬ ì¸ë±ì‹±.

    Strategy (Big Tech L11: 3-Tier Cache):
    1. 3-tier ìºì‹œë¡œ ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸ (L1 â†’ L2 â†’ L3)
       - L1 (ë©”ëª¨ë¦¬): ~0.1ms
       - L2 (Redis): ~1ms (ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ ê³µìœ )
       - L3 (DB): ~10ms (IndexVersionStore)
    2. COMPLETED â†’ ìºì‹œ hit (skip)
    3. IN_PROGRESS â†’ ì§„í–‰ ì¤‘ (skip)
    4. NOT_FOUND â†’ ì¸ë±ì‹± ì•Œë¦¼
    5. _indexing_done í”Œë˜ê·¸ë¡œ ì²« ì²´í¬ í›„ skip
    """
    global _indexing_done, _indexing_in_progress

    if _indexing_done:
        return

    if _indexing_in_progress:
        return

    try:
        from codegraph_shared.common.observability import get_logger

        logger = get_logger(__name__)

        # Check index status (3-tier cache)
        status = await check_index_status(TARGET_REPO_PATH, repo_id=repo_id)

        if status == IndexCheckResult.COMPLETED:
            logger.info("âœ… Index cache hit (L1/L2/L3)")
            _indexing_done = True
            return

        if status == IndexCheckResult.IN_PROGRESS:
            logger.info("â³ Indexing in progress (skipping)")
            _indexing_done = True
            return

        if status == IndexCheckResult.ERROR:
            logger.warning("âš ï¸ Index check failed")
            logger.info("ğŸ’¡ To index: python -m src.cli.main index <repo_path>")
            _indexing_done = True
            return

        # NOT_FOUND â†’ BALANCED ëª¨ë“œ ë°±ê·¸ë¼ìš´ë“œ ì¸ë±ì‹±
        logger.info("ğŸš€ Starting background indexing (BALANCED mode)")
        logger.info(f"ğŸ“ Repository: {TARGET_REPO_PATH}")

        _indexing_in_progress = True

        # Trigger background indexing (non-blocking)
        try:
            import asyncio

            asyncio.create_task(_trigger_background_indexing(repo_id, TARGET_REPO_PATH))

            logger.info("âœ… Background indexing job created")
            logger.info("ğŸ’¡ L2 ì™„ë£Œ í›„ ê²€ìƒ‰ ê°€ëŠ¥ (~30ì´ˆ), L3 ì™„ë£Œ í›„ Semantic ë¶„ì„ ê°€ëŠ¥ (~2ë¶„)")

        except Exception as idx_err:
            logger.error(f"Failed to trigger indexing: {idx_err}")
            logger.info("ğŸ’¡ Fallback: python -m src.cli.main index <repo_path>")

        _indexing_done = True
        _indexing_in_progress = False

    except Exception as e:
        from codegraph_shared.common.observability import get_logger

        logger = get_logger(__name__)
        logger.error(f"âš ï¸ Indexing check failed: {e}")
        _indexing_done = True


async def _trigger_background_indexing(repo_id: str, repo_path: str) -> None:
    """
    ë°±ê·¸ë¼ìš´ë“œ ì¸ë±ì‹± ì‹¤í–‰ (Big Tech L11: BALANCED ëª¨ë“œ).

    Args:
        repo_id: Repository ID
        repo_path: Repository path

    Strategy:
        1. BALANCED ëª¨ë“œ ì‚¬ìš© (L1 + L2 + L3)
        2. JobOrchestratorë¡œ ë¹„ë™ê¸° ì‹¤í–‰
        3. ê° stage ì™„ë£Œ ì‹œ ì ì§„ì  ì‚¬ìš© ê°€ëŠ¥:
           - L1 (íŒŒì‹±) ì™„ë£Œ â†’ ì‹¬ë³¼ ë°œê²¬
           - L2 (ì²­í¬) ì™„ë£Œ â†’ ê²€ìƒ‰ ê°€ëŠ¥
           - L3 (Semantic IR) ì™„ë£Œ â†’ ë¶„ì„ ê°€ëŠ¥

    Performance:
        - BALANCED: ~2ë¶„ / 10K files
        - L2ê¹Œì§€: ~30ì´ˆ (ê²€ìƒ‰ ê°€ëŠ¥)
        - L3ê¹Œì§€: ~2ë¶„ (ì™„ì „ ë¶„ì„)
    """
    from pathlib import Path

    from codegraph_shared.common.observability import get_logger

    logger = get_logger(__name__)

    try:
        logger.info(
            "Background indexing requested",
            repo_id=repo_id,
            repo_path=repo_path,
        )

        # For MCP: avoid heavy container initialization (tantivy dependency)
        # Container import loads entire system including tantivy, lexical indexes, etc.
        logger.info(f"ğŸ“ Repository: {repo_path}")
        logger.info("ğŸ’¡ To index: python -m src.cli.main index <repo_path>")
        logger.info("ğŸ’¡ BALANCED mode: L2 ì™„ë£Œ í›„ ê²€ìƒ‰ ê°€ëŠ¥ (~30s), L3 ì™„ë£Œ í›„ ë¶„ì„ ê°€ëŠ¥ (~2min)")

        # TODO: Implement lightweight job submission without container
        # Current blocker: container import requires tantivy, full index system

    except Exception as e:
        logger.error(f"Background indexing setup failed: {e}", repo_id=repo_id)
        logger.info("ğŸ’¡ Manual: python -m src.cli.main index <repo_path>")


# ì €ì¥ì†Œ ë° ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
node_store, edge_store, vector_store = create_all_stores()
chunk_retriever = create_chunk_retriever(vector_store, edge_store)
symbol_retriever = create_symbol_retriever(vector_store, edge_store)

# MCP Search Service (chunks + symbols)
search_service = MCPSearchService(chunk_retriever, symbol_retriever, node_store)

# ContextAdapter for call graph and other context queries
# (replaces MCPGraphService - uses existing ContextAdapter)
context_adapter = ContextAdapter(
    retrieval_service=None,  # Not needed for call graph
    symbol_index=None,  # Not needed for call graph
)

# ============================================================
# File Watcher ì„¤ì • (ì‹¤ì‹œê°„ ì¦ë¶„ ì¸ë±ì‹±)
# ê¸°ì¡´ FileWatcherService + IndexJobOrchestrator í™œìš©
# ============================================================

_file_watcher_service = None
_indexing_container = None

if ENABLE_FILE_WATCHING:
    # File watcher disabled for MCP to avoid heavy container initialization
    _logger.info("File watcher disabled (MCP lightweight mode)")
    _logger.info("   â†’ Use CLI for indexing: python -m src.cli.main index")

# MCP ì„œë²„ ìƒì„±
server = Server("codegraph")


@server.list_tools()
async def list_tools() -> list[Tool]:
    """ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
    return [
        # ============================================================
        # RFC-053 Tier 0 â€” ì—ì´ì „íŠ¸ ê¸°ë³¸ ì§„ì…ì  (3ê°œ)
        # ============================================================
        Tool(
            name="search",
            description="í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (chunks + symbols í†µí•©) - ì–´ë””ë¥¼ ë³¼ì§€ ëª¨ë¥¼ ë•Œ ì²« ì„ íƒ [Tier 0]",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "ê²€ìƒ‰ ì¿¼ë¦¬"},
                    "types": {
                        "type": "array",
                        "items": {"enum": ["chunks", "symbols", "all"]},
                        "default": ["all"],
                        "description": "ê²€ìƒ‰ ëŒ€ìƒ íƒ€ì…",
                    },
                    "limit": {"type": "integer", "default": 10, "description": "ìµœëŒ€ ê²°ê³¼ ìˆ˜"},
                    "repo_id": {"type": "string", "default": "default"},
                    "snapshot_id": {"type": "string", "default": "main"},
                },
                "required": ["query"],
            },
        ),
        Tool(
            name="search_chunks",
            description="ì½”ë“œ ì²­í¬ ê²€ìƒ‰ [Legacy - use 'search' instead]",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "ê²€ìƒ‰ ì¿¼ë¦¬"},
                    "limit": {"type": "integer", "description": "ê²°ê³¼ ìˆ˜", "default": 10},
                },
                "required": ["query"],
            },
        ),
        Tool(
            name="search_symbols",
            description="ì‹¬ë³¼ ê²€ìƒ‰ [Legacy - use 'search' instead]",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "ê²€ìƒ‰ ì¿¼ë¦¬"},
                    "limit": {"type": "integer", "description": "ê²°ê³¼ ìˆ˜", "default": 10},
                },
                "required": ["query"],
            },
        ),
        Tool(
            name="get_chunk",
            description="ì²­í¬ ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "chunk_id": {"type": "string", "description": "ì²­í¬ ID"},
                },
                "required": ["chunk_id"],
            },
        ),
        Tool(
            name="get_symbol",
            description="ì‹¬ë³¼ ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "symbol_id": {"type": "string", "description": "ì‹¬ë³¼ ID"},
                },
                "required": ["symbol_id"],
            },
        ),
        Tool(
            name="get_callers",
            description="í˜¸ì¶œì ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "symbol_id": {"type": "string", "description": "ì‹¬ë³¼ ID"},
                    "depth": {"type": "integer", "description": "íƒìƒ‰ ê¹Šì´", "default": 1},
                },
                "required": ["symbol_id"],
            },
        ),
        Tool(
            name="get_callees",
            description="í˜¸ì¶œ ëŒ€ìƒ ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "symbol_id": {"type": "string", "description": "ì‹¬ë³¼ ID"},
                    "depth": {"type": "integer", "description": "íƒìƒ‰ ê¹Šì´", "default": 1},
                },
                "required": ["symbol_id"],
            },
        ),
        Tool(
            name="analyze_cost",
            description="ë¹„ìš© ë³µì¡ë„ ë¶„ì„ (RFC-028)",
            inputSchema={
                "type": "object",
                "properties": {
                    "repo_id": {"type": "string", "description": "ì €ì¥ì†Œ ID"},
                    "snapshot_id": {"type": "string", "description": "ìŠ¤ëƒ…ìƒ· ID"},
                    "functions": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ë¶„ì„í•  í•¨ìˆ˜ FQN ëª©ë¡",
                    },
                },
                "required": ["repo_id", "snapshot_id", "functions"],
            },
        ),
        Tool(
            name="analyze_race",
            description="Race condition ë¶„ì„ (RFC-028 Phase 2)",
            inputSchema={
                "type": "object",
                "properties": {
                    "repo_id": {"type": "string", "description": "ì €ì¥ì†Œ ID"},
                    "snapshot_id": {"type": "string", "description": "ìŠ¤ëƒ…ìƒ· ID"},
                    "functions": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ë¶„ì„í•  async í•¨ìˆ˜ FQN ëª©ë¡",
                    },
                },
                "required": ["repo_id", "snapshot_id", "functions"],
            },
        ),
        # ============================================================
        # Job Tools (Async)
        # ============================================================
        Tool(
            name="job_submit",
            description="ë¹„ë™ê¸° Job ì œì¶œ (Heavy ë¶„ì„ìš©)",
            inputSchema={
                "type": "object",
                "properties": {
                    "tool": {"type": "string", "description": "ì‹¤í–‰í•  ë„êµ¬ (analyze_taint, analyze_impact, etc.)"},
                    "args": {"type": "object", "description": "ë„êµ¬ ì¸ì"},
                    "priority": {"type": "string", "enum": ["low", "medium", "high", "critical"], "default": "medium"},
                    "timeout_seconds": {"type": "integer", "default": 300},
                },
                "required": ["tool"],
            },
        ),
        Tool(
            name="job_status",
            description="Job ìƒíƒœ ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "job_id": {"type": "string", "description": "Job ID"},
                },
                "required": ["job_id"],
            },
        ),
        Tool(
            name="job_result",
            description="Job ê²°ê³¼ ì¡°íšŒ (with pagination)",
            inputSchema={
                "type": "object",
                "properties": {
                    "job_id": {"type": "string", "description": "Job ID"},
                    "cursor": {"type": "string", "description": "í˜ì´ì§€ë„¤ì´ì…˜ ì»¤ì„œ"},
                },
                "required": ["job_id"],
            },
        ),
        Tool(
            name="job_cancel",
            description="Job ì·¨ì†Œ",
            inputSchema={
                "type": "object",
                "properties": {
                    "job_id": {"type": "string", "description": "Job ID"},
                },
                "required": ["job_id"],
            },
        ),
        # ============================================================
        # Admin Tools (Tier 2 - Requires Approval)
        # ============================================================
        Tool(
            name="force_reindex",
            description="ê°•ì œ ì¬ì¸ë±ì‹± - ê¸°ì¡´ ì¸ë±ìŠ¤ ë¬´íš¨í™” í›„ ì „ì²´ ì¬ì¸ë±ì‹± (BALANCED ëª¨ë“œ, ê° stageë³„ ì ì§„ì  ì‚¬ìš© ê°€ëŠ¥) [Tier 2 - Requires Approval]",
            inputSchema={
                "type": "object",
                "properties": {
                    "repo_id": {
                        "type": "string",
                        "default": "default",
                        "description": "Repository ID",
                    },
                    "reason": {
                        "type": "string",
                        "description": "ì¬ì¸ë±ì‹± ì´ìœ  (loggingìš©)",
                    },
                    "invalidate_cache": {
                        "type": "boolean",
                        "default": True,
                        "description": "ìºì‹œ ë¬´íš¨í™” ì—¬ë¶€",
                    },
                    "mode": {
                        "type": "string",
                        "enum": ["fast", "balanced", "deep"],
                        "default": "balanced",
                        "description": "ì¸ë±ì‹± ëª¨ë“œ (fast=5s, balanced=2min, deep=30min)",
                    },
                },
                "required": [],
            },
        ),
        # ============================================================
        # Graph Semantics Tools (RFC-052 SOTA)
        # ============================================================
        Tool(
            name="graph_slice",
            description="Semantic Slicing - ë²„ê·¸/ì´ìŠˆì˜ Root Causeë§Œ ìµœì†Œ ë‹¨ìœ„ë¡œ ì¶”ì¶œ [Tier 0]",
            inputSchema={
                "type": "object",
                "properties": {
                    "anchor": {"type": "string", "description": "ì•µì»¤ ì‹¬ë³¼ (ë³€ìˆ˜/í•¨ìˆ˜/í´ë˜ìŠ¤)"},
                    "direction": {
                        "type": "string",
                        "enum": ["backward", "forward", "both"],
                        "default": "backward",
                        "description": "ìŠ¬ë¼ì´ìŠ¤ ë°©í–¥",
                    },
                    "max_depth": {"type": "integer", "default": 5, "description": "ìµœëŒ€ íƒìƒ‰ ê¹Šì´"},
                    "max_lines": {"type": "integer", "default": 100, "description": "ìµœëŒ€ ë¼ì¸ ìˆ˜"},
                    "session_id": {"type": "string", "description": "ì„¸ì…˜ ID (optional)"},
                    "repo_id": {"type": "string", "default": "default", "description": "ë¦¬í¬ì§€í† ë¦¬ ID"},
                    "file_scope": {"type": "string", "description": "íŒŒì¼ ì œí•œ (optional)"},
                },
                "required": ["anchor"],
            },
        ),
        Tool(
            name="graph_dataflow",
            description="Dataflow Analysis - source â†’ sink ë„ë‹¬ ê°€ëŠ¥ì„± ì¦ëª… (RFC-052)",
            inputSchema={
                "type": "object",
                "properties": {
                    "source": {"type": "string", "description": "ì†ŒìŠ¤ ì‹¬ë³¼"},
                    "sink": {"type": "string", "description": "ì‹±í¬ ì‹¬ë³¼"},
                    "policy": {"type": "string", "description": "ì •ì±… (sql_injection, xss ë“±)"},
                    "file_path": {"type": "string", "description": "ë¶„ì„í•  íŒŒì¼ (optional)"},
                    "max_depth": {"type": "integer", "default": 10},
                    "session_id": {"type": "string"},
                    "repo_id": {"type": "string", "default": "default"},
                },
                "required": ["source", "sink"],
            },
        ),
        # ============================================================
        # Context Tools
        # ============================================================
        Tool(
            name="get_context",
            description="í†µí•© ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ (definition, usages, callers ë“±) [Tier 0]",
            inputSchema={
                "type": "object",
                "properties": {
                    "target": {"type": "string", "description": "symbol_id | fqn | file:line"},
                    "facets": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "enum": [
                                "definition",
                                "usages",
                                "references",
                                "docstring",
                                "skeleton",
                                "tests",
                                "callers",
                                "callees",
                            ],
                        },
                        "default": ["definition", "usages"],
                    },
                    "budget": {
                        "type": "object",
                        "properties": {
                            "max_chars": {"type": "integer", "default": 8000},
                            "max_items": {"type": "integer", "default": 20},
                        },
                    },
                },
                "required": ["target"],
            },
        ),
        Tool(
            name="get_definition",
            description="ì‹¬ë³¼ ì •ì˜ ì¡°íšŒ",
            inputSchema={
                "type": "object",
                "properties": {
                    "symbol": {"type": "string", "description": "ì‹¬ë³¼ ì´ë¦„ ë˜ëŠ” FQN"},
                    "repo_id": {"type": "string"},
                    "snapshot_id": {"type": "string", "default": "main"},
                    "include_body": {"type": "boolean", "default": True},
                },
                "required": ["symbol"],
            },
        ),
        Tool(
            name="get_references",
            description="ì°¸ì¡° ì¡°íšŒ (with pagination)",
            inputSchema={
                "type": "object",
                "properties": {
                    "symbol": {"type": "string", "description": "ì‹¬ë³¼ ì´ë¦„ ë˜ëŠ” FQN"},
                    "repo_id": {"type": "string"},
                    "snapshot_id": {"type": "string", "default": "main"},
                    "limit": {"type": "integer", "default": 50},
                    "cursor": {"type": "string"},
                },
                "required": ["symbol"],
            },
        ),
        # ============================================================
        # Preview Tools (Lightweight)
        # ============================================================
        Tool(
            name="preview_taint_path",
            description="Taint ê²½ë¡œ í”„ë¦¬ë·° (1-2ì´ˆ, ì¡´ì¬ì„± í™•ì¸)",
            inputSchema={
                "type": "object",
                "properties": {
                    "source_pattern": {"type": "string", "description": "Source íŒ¨í„´"},
                    "sink_pattern": {"type": "string", "description": "Sink íŒ¨í„´"},
                    "repo_id": {"type": "string"},
                    "snapshot_id": {"type": "string", "default": "main"},
                    "limit": {"type": "integer", "default": 5},
                },
                "required": ["source_pattern", "sink_pattern"],
            },
        ),
        Tool(
            name="preview_impact",
            description="Impact í”„ë¦¬ë·° (ë³€ê²½ ì˜í–¥ë„ ê·¼ì‚¬)",
            inputSchema={
                "type": "object",
                "properties": {
                    "changed_symbols": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "ë³€ê²½ëœ ì‹¬ë³¼ FQN ëª©ë¡",
                    },
                    "repo_id": {"type": "string"},
                    "snapshot_id": {"type": "string", "default": "main"},
                    "top_k": {"type": "integer", "default": 20},
                },
                "required": ["changed_symbols"],
            },
        ),
        Tool(
            name="preview_callers",
            description="í˜¸ì¶œì í”„ë¦¬ë·° (ìƒìœ„ í˜¸ì¶œìë§Œ)",
            inputSchema={
                "type": "object",
                "properties": {
                    "symbol": {"type": "string", "description": "ì‹¬ë³¼ FQN"},
                    "depth": {"type": "integer", "default": 2},
                    "top_k": {"type": "integer", "default": 50},
                    "repo_id": {"type": "string"},
                    "snapshot_id": {"type": "string", "default": "main"},
                },
                "required": ["symbol"],
            },
        ),
        # ============================================================
        # Verify Tools
        # ============================================================
        Tool(
            name="verify_patch_compile",
            description="íŒ¨ì¹˜ ë¬¸ë²•/íƒ€ì…/ë¹Œë“œ ê²€ì¦",
            inputSchema={
                "type": "object",
                "properties": {
                    "file_path": {"type": "string", "description": "ìˆ˜ì •ëœ íŒŒì¼ ê²½ë¡œ"},
                    "patch": {"type": "string", "description": "ì ìš©í•  íŒ¨ì¹˜ ë˜ëŠ” ìƒˆ ë‚´ìš©"},
                    "language": {"type": "string", "enum": ["python", "typescript", "javascript"], "default": "python"},
                    "check_types": {"type": "boolean", "default": True},
                },
                "required": ["file_path"],
            },
        ),
        Tool(
            name="verify_finding_resolved",
            description="Finding í•´ê²° í™•ì¸ (ë¶„ì„â†’ìˆ˜ì •â†’ê²€ì¦ ë£¨í”„)",
            inputSchema={
                "type": "object",
                "properties": {
                    "finding_id": {"type": "string", "description": "ì›ë˜ finding ID"},
                    "finding_type": {"type": "string", "description": "Finding ìœ í˜• (taint, null_deref, etc.)"},
                    "original_location": {
                        "type": "object",
                        "properties": {
                            "file": {"type": "string"},
                            "line": {"type": "integer"},
                            "column": {"type": "integer"},
                        },
                        "required": ["file", "line"],
                    },
                    "patch": {"type": "string", "description": "ì ìš©ëœ íŒ¨ì¹˜"},
                    "repo_id": {"type": "string"},
                    "snapshot_id": {"type": "string", "default": "main"},
                },
                "required": ["finding_type", "original_location"],
            },
        ),
    ]


@server.call_tool()
async def call_tool(name: str, arguments: dict) -> list:
    """
    ë„êµ¬ í˜¸ì¶œ.

    Big Tech L11: ì²« ìš”ì²­ ì‹œ ìë™ ì¸ë±ìŠ¤ ì²´í¬ (ìºì‹œ í™œìš©).
    """
    import json
    from mcp.types import TextContent

    # ì²« ìš”ì²­ ì‹œ ìë™ ì¸ë±ìŠ¤ ì²´í¬
    await ensure_indexed()

    # RFC-053 Tier 0
    if name == "search":
        result = await search(search_service, arguments)
        return [TextContent(type="text", text=result)]
    # RFC-052: Graph Semantics Tools
    elif name == "graph_slice":
        result = await graph_slice(arguments)
        return [TextContent(type="text", text=result)]
    elif name == "graph_dataflow":
        result = await graph_dataflow(arguments)
        return [TextContent(type="text", text=result)]
    # Legacy tools (backward compatibility via service layer)
    elif name == "search_chunks":
        # Redirect to search with types=["chunks"]
        arguments["types"] = ["chunks"]
        result = await search(search_service, arguments)
        return [TextContent(type="text", text=result)]
    elif name == "search_symbols":
        # Redirect to search with types=["symbols"]
        arguments["types"] = ["symbols"]
        result = await search(search_service, arguments)
        return [TextContent(type="text", text=result)]
    elif name == "get_chunk":
        # Direct service call
        chunk_id = arguments.get("chunk_id", "")
        chunk_result = await search_service.get_chunk(chunk_id)
        result = json.dumps(chunk_result.to_dict() if chunk_result else {"error": "Not found"})
        return [TextContent(type="text", text=result)]
    elif name == "get_symbol":
        # Direct service call
        symbol_id = arguments.get("symbol_id", "")
        symbol_result = await search_service.get_symbol(symbol_id)
        result = json.dumps(symbol_result.to_dict() if symbol_result else {"error": "Not found"})
        return [TextContent(type="text", text=result)]
    elif name == "get_callers":
        # Use ContextAdapter.get_call_graph
        symbol = arguments.get("symbol_id", "")
        depth = arguments.get("depth", 1)
        repo_id = arguments.get("repo_id", "default")
        snapshot_id = arguments.get("snapshot_id", "main")

        graph_result = await context_adapter.get_call_graph(
            function_name=symbol,
            repo_id=repo_id,
            snapshot_id=snapshot_id,
            depth=depth,
        )
        # Extract only callers
        callers = graph_result.get("callers", [])
        result = json.dumps(callers)
        return [TextContent(type="text", text=result)]
    elif name == "get_callees":
        # Use ContextAdapter.get_call_graph
        symbol = arguments.get("symbol_id", "")
        depth = arguments.get("depth", 1)
        repo_id = arguments.get("repo_id", "default")
        snapshot_id = arguments.get("snapshot_id", "main")

        graph_result = await context_adapter.get_call_graph(
            function_name=symbol,
            repo_id=repo_id,
            snapshot_id=snapshot_id,
            depth=depth,
        )
        # Extract only callees
        callees = graph_result.get("callees", [])
        result = json.dumps(callees)
        return [TextContent(type="text", text=result)]
    # Analysis tools
    elif name == "analyze_cost":
        result = await analyze_cost(None, arguments)
        return [TextContent(type="text", text=result)]
    elif name == "analyze_race":
        result = await analyze_race(None, arguments)
        return [TextContent(type="text", text=result)]
    # Job tools (Async)
    elif name == "job_submit":
        result = await job_submit(arguments)
        return [TextContent(type="text", text=result)]
    elif name == "job_status":
        result = await job_status(arguments)
        return [TextContent(type="text", text=result)]
    elif name == "job_result":
        result = await job_result(arguments)
        return [TextContent(type="text", text=result)]
    elif name == "job_cancel":
        result = await job_cancel(arguments)
        return [TextContent(type="text", text=result)]
    # Admin tools (Tier 2)
    elif name == "force_reindex":
        result = await force_reindex(arguments)
        return [TextContent(type="text", text=result)]
    # Context tools
    elif name == "get_context":
        result = await get_context(arguments)
        return [TextContent(type="text", text=result)]
    elif name == "get_definition":
        result = await get_definition(arguments)
        return [TextContent(type="text", text=result)]
    elif name == "get_references":
        result = await get_references(arguments)
        return [TextContent(type="text", text=result)]
    # Preview tools
    elif name == "preview_taint_path":
        result = await preview_taint_path(arguments)
        return [TextContent(type="text", text=result)]
    elif name == "preview_impact":
        result = await preview_impact(arguments)
        return [TextContent(type="text", text=result)]
    elif name == "preview_callers":
        result = await preview_callers(arguments)
        return [TextContent(type="text", text=result)]
    # Verify tools
    elif name == "verify_patch_compile":
        result = await verify_patch_compile(arguments)
        return [TextContent(type="text", text=result)]
    elif name == "verify_finding_resolved":
        result = await verify_finding_resolved(arguments)
        return [TextContent(type="text", text=result)]
    else:
        raise ValueError(f"Unknown tool: {name}")


# ============================================================
# MCP Resources (RFC-SEM-022 SOTA)
# ============================================================


@server.list_resources()
async def list_resources():
    """
    MCP Resources ëª©ë¡ (RFC-SEM-022).

    Streaming Resources:
    - semantica://jobs/{job_id}/events
    - semantica://jobs/{job_id}/log
    - semantica://jobs/{job_id}/artifacts
    - semantica://executions/{execution_id}/findings
    - semantica://repo/{repo_id}/info
    """
    from mcp.types import Resource

    return [
        Resource(
            uri="semantica://repo/info",
            name="Repository Info",
            description=f"í˜„ì¬ ë¶„ì„ ì¤‘ì¸ ë ˆí¬ì§€í† ë¦¬ ì •ë³´ (Path: {TARGET_REPO_PATH})",
            mimeType="application/json",
        ),
        Resource(
            uri="semantica://jobs/{job_id}/events",
            name="Job Events Stream",
            description="ì‹¤ì‹œê°„ Job ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ (SSE)",
            mimeType="text/event-stream",
        ),
        Resource(
            uri="semantica://jobs/{job_id}/log",
            name="Job Log Stream",
            description="ì‹¤ì‹œê°„ Job ë¡œê·¸",
            mimeType="text/plain",
        ),
        Resource(
            uri="semantica://jobs/{job_id}/artifacts",
            name="Job Artifacts",
            description="Job ì‹¤í–‰ ê²°ê³¼ë¬¼",
            mimeType="application/json",
        ),
        Resource(
            uri="semantica://executions/{execution_id}/findings",
            name="Execution Findings",
            description="ì‹¤í–‰ì—ì„œ ë°œê²¬ëœ ì·¨ì•½ì  ëª©ë¡",
            mimeType="application/json",
        ),
    ]


@server.list_prompts()
async def list_prompts():
    """
    MCP Prompts ëª©ë¡ (RFC-SEM-022 SOTA).

    LLM Agent ìê¸°ë¹„íŒ ë° ì¶”ë¡  ê°€ì´ë“œ.
    """
    from apps.mcp.mcp.prompts import get_prompts

    return get_prompts()


@server.get_prompt()
async def get_prompt(name: str, arguments: dict | None = None):
    """
    Prompt ì¡°íšŒ ë° í…œí”Œë¦¿ ìƒì„±.

    Args:
        name: Prompt ì´ë¦„
        arguments: Prompt ì¸ì

    Returns:
        Prompt í…ìŠ¤íŠ¸ (êµ¬ì¡°í™”ëœ ì¶”ë¡  ê°€ì´ë“œ)
    """
    from apps.mcp.mcp.prompts import get_prompt_template

    if arguments is None:
        arguments = {}

    try:
        template = get_prompt_template(name, arguments)
        return {
            "description": f"Prompt: {name}",
            "messages": [{"role": "user", "content": {"type": "text", "text": template}}],
        }
    except Exception as e:
        raise ValueError(f"Failed to generate prompt: {e}")


@server.read_resource()
async def read_resource(uri: str) -> list:
    """
    MCP Resource ì¡°íšŒ (RFC-SEM-022).

    URI Format:
    - semantica://repo/info
    - semantica://jobs/{job_id}/events
    - semantica://jobs/{job_id}/log
    - semantica://jobs/{job_id}/artifacts
    - semantica://executions/{execution_id}/findings
    """
    import json
    import re
    from mcp.types import TextContent

    # Repo info
    if uri == "semantica://repo/info":
        result = json.dumps(
            {
                "uri": uri,
                "repo_path": TARGET_REPO_PATH,
                "repo_id": "default",
                "snapshot_id": "main",
                "indexed": False,  # TODO: Check actual index status
                "message": "Use CODEGRAPH_REPO_PATH env var to specify target repository",
            }
        )
        return [TextContent(type="text", text=result)]

    # Parse URI
    if match := re.match(r"semantica://jobs/([^/]+)/events", uri):
        job_id = match.group(1)
        # TODO: ì‹¤ì œ SSE ìŠ¤íŠ¸ë¦¼ êµ¬í˜„
        result = json.dumps(
            {
                "uri": uri,
                "job_id": job_id,
                "events": [],
                "message": "Streaming not yet implemented",
            }
        )
        return [TextContent(type="text", text=result)]

    elif match := re.match(r"semantica://jobs/([^/]+)/log", uri):
        job_id = match.group(1)
        result = json.dumps(
            {
                "uri": uri,
                "job_id": job_id,
                "log": [],
            }
        )
        return [TextContent(type="text", text=result)]

    elif match := re.match(r"semantica://jobs/([^/]+)/artifacts", uri):
        job_id = match.group(1)
        result = json.dumps(
            {
                "uri": uri,
                "job_id": job_id,
                "artifacts": {},
            }
        )
        return [TextContent(type="text", text=result)]

    elif match := re.match(r"semantica://executions/([^/]+)/findings", uri):
        execution_id = match.group(1)

        # ExecutionRepositoryì—ì„œ findings ì¡°íšŒ
        try:
            from codegraph_shared.kernel.infrastructure.execution_repository import (
                get_execution_repository,
            )

            repo = get_execution_repository()
            findings = await repo.get_findings(execution_id)

            result = json.dumps(
                {
                    "uri": uri,
                    "execution_id": execution_id,
                    "findings": findings,
                    "count": len(findings),
                }
            )
            return [TextContent(type="text", text=result)]
        except Exception as e:
            result = json.dumps(
                {
                    "uri": uri,
                    "error": str(e),
                }
            )
            return [TextContent(type="text", text=result)]

    else:
        raise ValueError(f"Unknown resource URI: {uri}")


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # Start file watcher (if enabled)
        if _file_watcher_service and ENABLE_FILE_WATCHING:
            await _file_watcher_service.start()

            # Watch target repository
            from pathlib import Path

            await _file_watcher_service.watch_repo(
                Path(TARGET_REPO_PATH),
                repo_id="default",
            )

            _logger.info(f"Watching repository: {TARGET_REPO_PATH}")

        # Run MCP server
        async with stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="codegraph",
                    server_version="0.1.0",
                    capabilities={
                        "tools": {},  # Enable tools support
                        "resources": {},  # Enable resources support
                    },
                ),
            )
    finally:
        # Cleanup: Stop file watcher
        if _file_watcher_service:
            await _file_watcher_service.stop()
            _logger.info("File watcher stopped")


if __name__ == "__main__":
    asyncio.run(main())
