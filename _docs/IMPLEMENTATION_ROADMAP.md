# Semantica CodeGraph v4 - êµ¬í˜„ ë¡œë“œë§µ

**ì‘ì„±ì¼**: 2024-11-24
**ì „ì²´ ì§„í–‰ë„**: 97%

---

## ğŸ“Š ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Agent Layer (ê³„íš)                       â”‚
â”‚           LangGraph ê¸°ë°˜ ì½”ë“œ ì—ì´ì „íŠ¸                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Application Layers                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Server Layer (âœ…)   â”‚  API Server, MCP Server               â”‚
â”‚  Retriever Layer (âœ…) â”‚  Multi-index Search, Fusion           â”‚
â”‚  Index Layer (âœ… 98%) â”‚  Lexical, Vector, Symbol, Fuzzy      â”‚
â”‚  RepoMap Layer (âœ…)   â”‚  PageRank, Summary, Storage           â”‚
â”‚  Chunk Layer (âœ…)     â”‚  6-tier Hierarchy, Incremental        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Foundation Layer (âœ…)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Graph Layer        â”‚  Kuzu-based GraphDocument              â”‚
â”‚  Semantic IR        â”‚  CFG, DFG, Type/Signature              â”‚
â”‚  IR Layer           â”‚  Language-neutral IR v4                â”‚
â”‚  Parsing Layer      â”‚  Tree-sitter based AST                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Infrastructure Layer (âœ…)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Storage    â”‚ PostgreSQL, Redis, Kuzu                        â”‚
â”‚  Search     â”‚ Zoekt, Qdrant                                  â”‚
â”‚  LLM        â”‚ OpenAI, LiteLLM                                â”‚
â”‚  Git        â”‚ GitPython                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… í˜„ì¬ êµ¬í˜„ ì™„ë£Œ (97%)

### 1. Foundation Layer (100%)

#### 1.1 Parsing Layer
**ìœ„ì¹˜**: `src/foundation/parsing/`

**êµ¬í˜„ ì™„ë£Œ**:
- âœ… `parser_registry.py` - Tree-sitter íŒŒì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬
  - ì§€ì› ì–¸ì–´: Python, TypeScript, JavaScript, Go, Java, Rust, C, C++
  - íŒŒì¼ í™•ì¥ì ê¸°ë°˜ ìë™ ì–¸ì–´ ê°ì§€
  - ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ íŒŒì„œ ìºì‹±

- âœ… `source_file.py` - ì†ŒìŠ¤ íŒŒì¼ ì¶”ìƒí™”
  - íŒŒì¼ ë¡œë”© (ë””ìŠ¤í¬/ë¬¸ìì—´)
  - ë¼ì¸ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
  - ì¢Œí‘œ ê¸°ë°˜ ë²”ìœ„ ì¶”ì¶œ (`get_text(start_line, start_col, end_line, end_col)`)

- âœ… `ast_tree.py` - AST íŠ¸ë¦¬ ë˜í¼
  - Tree-sitter ë…¸ë“œ ìˆœíšŒ (`walk()`, `find_by_type()`)
  - ì—ëŸ¬ ë…¸ë“œ ê°ì§€
  - ì¦ë¶„ íŒŒì‹± ì§€ì› (`parse_incremental()`)

**í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**:
```python
# íŒŒì„œ ì„ íƒ ì•Œê³ ë¦¬ì¦˜
def detect_language(file_path: str) -> str:
    ext = Path(file_path).suffix.lower()
    return EXTENSION_MAP.get(ext, "unknown")

# AST ìˆœíšŒ ì•Œê³ ë¦¬ì¦˜ (DFS)
def walk(node: Node) -> Iterator[Node]:
    yield node
    for child in node.children:
        yield from walk(child)
```

---

#### 1.2 IR Layer (Intermediate Representation)
**ìœ„ì¹˜**: `src/foundation/ir/`

**êµ¬í˜„ ì™„ë£Œ**:
- âœ… `models/core.py` - IR í•µì‹¬ ëª¨ë¸
  - `IRNode`: File, Module, Class, Function, Variable, Import
  - `IRDocument`: íŒŒì¼ ë‹¨ìœ„ IR ì»¨í…Œì´ë„ˆ
  - ì•ˆì •ì ì¸ ID ìƒì„±: `{lang}:{repo}:{kind}:{span_hash}`

- âœ… `generators/python_generator.py` - Python IR ìƒì„±ê¸°
  - AST â†’ IRNode ë³€í™˜
  - FQN (Fully Qualified Name) ìƒì„±
  - ìŠ¤ì½”í”„ ì¶”ì  (module â†’ class â†’ function)
  - Import resolution
  - Call site ì¶”ì¶œ

- âœ… `generators/scope_stack.py` - ìŠ¤ì½”í”„ ìŠ¤íƒ
  - ì¤‘ì²© ìŠ¤ì½”í”„ ê´€ë¦¬
  - ì‹¬ë³¼ í…Œì´ë¸” (per-scope symbol registry)
  - ì„€ë„ì‰ ì²˜ë¦¬

**í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**:
```python
# FQN ìƒì„± ì•Œê³ ë¦¬ì¦˜
def build_fqn(scope_stack: ScopeStack, name: str) -> str:
    parts = []
    for frame in scope_stack.frames:
        if frame.kind in ("module", "class"):
            parts.append(frame.name)
    parts.append(name)
    return ".".join(parts)

# Call site ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜
def extract_calls(node: Node) -> list[CallSite]:
    calls = []
    for child in walk(node):
        if child.type == "call":
            callee = extract_callee_name(child)
            calls.append(CallSite(callee=callee, line=child.start_point[0]))
    return calls
```

**ë°ì´í„° íë¦„**:
```
SourceFile â†’ AstTree â†’ PythonGenerator â†’ IRDocument
                â†“
          ScopeStack (ì‹¬ë³¼ ì¶”ì )
                â†“
          IRNode (FQN, Signature, Type)
```

---

#### 1.3 Semantic IR Layer (CFG, DFG, Type/Signature)
**ìœ„ì¹˜**: `src/foundation/semantic_ir/`

**êµ¬í˜„ ì™„ë£Œ**:
- âœ… `cfg/builder.py` - Control Flow Graph ìƒì„±
  - Basic block ì¶”ì¶œ
  - ì¡°ê±´ ë¶„ê¸° (if/elif/else, match)
  - ë£¨í”„ (for/while), ì˜ˆì™¸ ì²˜ë¦¬ (try/except)
  - CFGBlock ë…¸ë“œ, CFGEdge (NORMAL, BRANCH, LOOP_BACK, EXCEPTION)

- âœ… `dfg/builder.py` - Data Flow Graph ìƒì„±
  - ë³€ìˆ˜ ì½ê¸°/ì“°ê¸° ì¶”ì 
  - Assignment, Use site ì¶”ì¶œ
  - SSA ì¤€ë¹„ (í–¥í›„ í™•ì¥ ê°€ëŠ¥)

- âœ… `typing/builder.py` - íƒ€ì… ì‹œìŠ¤í…œ
  - `TypeEntity`: primitive, builtin, user-defined, generic
  - íƒ€ì… í•´ê²° ë‹¨ê³„: raw â†’ builtin â†’ local â†’ module â†’ external
  - Generic íƒ€ì… ë¶„í•´ (List[T], Dict[K, V])

- âœ… `signature/builder.py` - í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜
  - `SignatureEntity`: parameters, return_type, decorators
  - ê°€ì‹œì„± (public, protected, private)
  - Async/static/classmethod í”Œë˜ê·¸

**í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**:
```python
# CFG Basic Block ì¶”ì¶œ
def extract_basic_blocks(func_node: Node) -> list[CFGBlock]:
    blocks = []
    current_block = CFGBlock(kind=CFGBlockKind.ENTRY)

    for stmt in func_node.body:
        if is_branch(stmt):  # if, elif, else
            blocks.append(current_block)
            current_block = CFGBlock(kind=CFGBlockKind.CONDITION)
        elif is_loop(stmt):  # for, while
            blocks.append(current_block)
            current_block = CFGBlock(kind=CFGBlockKind.LOOP_HEADER)
        else:
            current_block.statements.append(stmt)

    blocks.append(current_block)
    return blocks

# DFG ë³€ìˆ˜ ì¶”ì 
def extract_variable_usage(stmt: Node) -> tuple[set[str], set[str]]:
    reads = set()  # ì½ê¸°
    writes = set()  # ì“°ê¸°

    if stmt.type == "assignment":
        writes.add(extract_target(stmt.left))
        reads.update(extract_names(stmt.right))
    elif stmt.type == "expression":
        reads.update(extract_names(stmt))

    return reads, writes
```

---

#### 1.4 Graph Layer (Kuzu-based)
**ìœ„ì¹˜**: `src/foundation/graph/`

**êµ¬í˜„ ì™„ë£Œ**:
- âœ… `models.py` - GraphDocument ëª¨ë¸
  - `GraphNode`: Symbol, Type, Signature ë…¸ë“œ
  - `GraphEdge`: CONTAINS, CALLS, IMPORTS, READS, WRITES, REFERENCES_TYPE

- âœ… `builder.py` - Graph ìƒì„±ê¸°
  - IRDocument â†’ GraphDocument ë³€í™˜
  - External node ìƒì„± (`external::{lang}::{symbol}`)
  - Edge ì¤‘ë³µ ì œê±°, Normalization

- âœ… `src/foundation/storage/kuzu/store.py` - Kuzu ìŠ¤í† ë¦¬ì§€
  - Embedded Kuzu DB
  - Node/Edge ì¼ê´„ UPSERT
  - Cypher-style ì¿¼ë¦¬ ì§€ì›

**í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**:
```python
# IRNode â†’ GraphNode ìŠ¹ê²©
def promote_to_graph_node(ir_node: IRNode) -> GraphNode:
    return GraphNode(
        node_id=ir_node.node_id,
        kind="Symbol",
        properties={
            "name": ir_node.name,
            "fqn": ir_node.fqn,
            "kind": ir_node.kind,
            "visibility": ir_node.visibility,
        }
    )

# Call edge ìƒì„±
def create_call_edges(ir_doc: IRDocument) -> list[GraphEdge]:
    edges = []
    for func in ir_doc.functions:
        for call_site in func.calls:
            target_id = resolve_callee_id(call_site.callee, ir_doc)
            if target_id:
                edges.append(GraphEdge(
                    source=func.node_id,
                    target=target_id,
                    kind="CALLS",
                    properties={"line": call_site.line}
                ))
    return edges
```

**Kuzu ìŠ¤í‚¤ë§ˆ**:
```cypher
// Nodes
CREATE NODE TABLE Symbol (
    node_id STRING PRIMARY KEY,
    name STRING,
    fqn STRING,
    kind STRING,
    visibility STRING
)

// Edges
CREATE REL TABLE CALLS (FROM Symbol TO Symbol)
CREATE REL TABLE IMPORTS (FROM Symbol TO Symbol)
CREATE REL TABLE CONTAINS (FROM Symbol TO Symbol)
```

---

### 2. Chunk Layer (100%)

**ìœ„ì¹˜**: `src/foundation/chunk/`

**êµ¬í˜„ ì™„ë£Œ**:
- âœ… `models.py` - Chunk ëª¨ë¸
  - 6ë‹¨ê³„ ê³„ì¸µ: Repo â†’ Project â†’ Module â†’ File â†’ Class â†’ Function
  - í™•ì¥ íƒ€ì…: route, service, repository, config, job, middleware
  - Span tracking (original_start_line í¬í•¨)
  - ë²„ì „ ê´€ë¦¬ (version, is_deleted, last_indexed_commit)

- âœ… `id_generator.py` - Stable ID ìƒì„±
  - FQN ê¸°ë°˜: `chunk:{repo_id}:{kind}:{fqn}`
  - ì¶©ëŒ ì‹œ content_hash suffix

- âœ… `builder.py` - ChunkBuilder (ê³„ì¸µ ìƒì„±)
  - `_build_repo_chunk()` - Repository ìµœìƒìœ„
  - `_build_project_chunks()` - í”„ë¡œì íŠ¸ ë‹¨ìœ„
  - `_build_module_chunks()` - ëª¨ë“ˆ ë‹¨ìœ„
  - `_build_file_chunks()` - íŒŒì¼ ë‹¨ìœ„
  - `_build_class_chunks()` - í´ë˜ìŠ¤ ë‹¨ìœ„
  - `_build_function_chunks()` - í•¨ìˆ˜ ë‹¨ìœ„ (Leaf)
  - IR/Graph ë§¤í•‘ ìë™ ì—°ê²°

- âœ… `boundary.py` - Boundary ê²€ì¦
  - Sibling gap/overlap ê²€ì¶œ
  - Invalid range ê²€ì¶œ
  - Large chunk ê²½ê³  (í† í° ê¸°ì¤€)

- âœ… `mapping.py` - IR/Graph ë§¤í•‘
  - `ChunkMapper`: Chunk â†’ IRNode (ë¼ì¸ ë²”ìœ„ ê¸°ë°˜)
  - `ChunkGraphMapper`: Chunk â†’ GraphNode (1:1 ë˜ëŠ” ì§‘ê³„)
  - GraphNodeFilter (function/class/methodë§Œ í¬í•¨)

- âœ… `incremental.py` - ì¦ë¶„ ì—…ë°ì´íŠ¸
  - **Phase A**: íŒŒì¼ ì¶”ê°€/ì‚­ì œ/ìˆ˜ì • ì²˜ë¦¬
  - **Phase B**: Span drift, Rename ê°ì§€
  - **Phase C**: Diff-based partial updates
    - `DiffParser`: unified diff íŒŒì‹±
    - `_identify_affected_chunks()`: Hunk overlap ê²€ì‚¬
    - ì˜í–¥ë°›ì§€ ì•Šì€ chunkëŠ” ì¬ì‚¬ìš© (ì„±ëŠ¥ ìµœì í™”!)
  - Hook: `ChunkUpdateHook` (on_modified, on_drifted, on_renamed)

- âœ… `store.py` - ChunkStore êµ¬í˜„
  - **InMemoryChunkStore**: ê°œë°œ/í…ŒìŠ¤íŠ¸ìš© (O(1) file index)
  - **PostgresChunkStore**: Production êµ¬í˜„
    - asyncpg ê¸°ë°˜
    - ë°°ì¹˜ UPSERT (500ê°œì”©)
    - `find_chunk_by_file_and_line()` - Zoekt ë§¤í•‘ í•µì‹¬ ì¿¼ë¦¬
    - Soft delete ì§€ì›

**í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**:
```python
# 6ë‹¨ê³„ ê³„ì¸µ ìƒì„± ì•Œê³ ë¦¬ì¦˜
def build_chunk_hierarchy(ir_doc: IRDocument, graph_doc: GraphDocument):
    # 1. Repo chunk (root)
    repo_chunk = create_repo_chunk(repo_id)

    # 2. Project chunks (grouping by directory structure)
    project_chunks = group_by_project(ir_doc.files)
    for proj in project_chunks:
        proj.parent_id = repo_chunk.chunk_id

    # 3. Module chunks (Python packages)
    module_chunks = group_by_module(ir_doc.modules)

    # 4. File chunks (1:1 with files)
    file_chunks = [create_file_chunk(f) for f in ir_doc.files]

    # 5. Class chunks (from IR classes)
    class_chunks = [create_class_chunk(c) for c in ir_doc.classes]

    # 6. Function chunks (Leaf, from IR functions)
    func_chunks = [create_function_chunk(f) for f in ir_doc.functions]

    return all_chunks

# Diff-based partial update ì•Œê³ ë¦¬ì¦˜
def handle_modified_file_partial(file_path, diff_text):
    # 1. Diff íŒŒì‹±
    hunks = parse_diff(diff_text)

    # 2. ì˜í–¥ë°›ì€ chunk ì‹ë³„
    affected_chunks = []
    for chunk in old_chunks:
        for hunk in hunks:
            if overlaps(chunk.span, hunk.new_range):
                affected_chunks.append(chunk)

    # 3. ì˜í–¥ë°›ì€ chunkë§Œ ì¬ìƒì„±
    new_ir = generate_ir(file_path)
    new_chunks = build_chunks(new_ir)

    # 4. ì˜í–¥ë°›ì§€ ì•Šì€ chunkëŠ” ê¸°ì¡´ ê²ƒ ì¬ì‚¬ìš©
    final_chunks = merge(affected_new_chunks, unaffected_old_chunks)

    return final_chunks

# Zoekt file+line â†’ Chunk ë§¤í•‘ ì•Œê³ ë¦¬ì¦˜
def find_chunk_by_file_and_line(repo_id, file_path, line):
    candidates = get_chunks_for_file(repo_id, file_path)

    # 1. Line ë²”ìœ„ í•„í„°ë§
    matching = [c for c in candidates
                if c.start_line <= line <= c.end_line]

    # 2. ìš°ì„ ìˆœìœ„ ì •ë ¬
    priority = {
        "function": 1,
        "method": 1,
        "class": 2,
        "file": 3
    }
    matching.sort(key=lambda c: (
        priority.get(c.kind, 4),
        c.end_line - c.start_line  # ì‘ì€ chunk ìš°ì„ 
    ))

    return matching[0] if matching else None
```

**DB ìŠ¤í‚¤ë§ˆ** (`infra/db/migrations/001_create_chunk_tables.sql`):
```sql
CREATE TABLE chunks (
    chunk_id TEXT PRIMARY KEY,
    repo_id TEXT NOT NULL,
    snapshot_id TEXT NOT NULL,

    -- Hierarchy
    project_id TEXT,
    module_path TEXT,
    file_path TEXT,
    parent_id TEXT,

    -- Metadata
    kind TEXT NOT NULL,
    fqn TEXT NOT NULL,
    language TEXT,

    -- Source location (Zoekt ë§¤í•‘ìš©)
    start_line INTEGER,
    end_line INTEGER,
    original_start_line INTEGER,  -- Span drift tracking
    original_end_line INTEGER,

    -- Incremental
    content_hash TEXT,
    version INTEGER DEFAULT 1,
    is_deleted BOOLEAN DEFAULT FALSE,

    -- LLM
    summary TEXT,
    importance REAL DEFAULT 0.0,

    attrs JSONB DEFAULT '{}'
);

-- í•µì‹¬ ì¸ë±ìŠ¤: Zoekt ë§¤í•‘ìš©
CREATE INDEX idx_chunks_file_span
ON chunks (repo_id, file_path, start_line, end_line);
```

---

### 3. RepoMap Layer (100%)

**ìœ„ì¹˜**: `src/repomap/`

**êµ¬í˜„ ì™„ë£Œ**:
- âœ… `models.py` - RepoMap ëª¨ë¸
  - `RepoMapNode`: chunk ê¸°ë°˜ íŠ¸ë¦¬ ë…¸ë“œ
  - `PageRankResult`: ì¤‘ìš”ë„ ì ìˆ˜
  - `RepoMapDocument`: ì „ì²´ ë§µ ì»¨í…Œì´ë„ˆ

- âœ… `builder/orchestrator.py` - ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
  - Chunk hierarchy â†’ RepoMap tree ë³€í™˜
  - LLM summary í†µí•©
  - PageRank ì ìˆ˜ ë³‘í•©

- âœ… `pagerank/engine.py` - PageRank ê³„ì‚°
  - Graph-based importance ê³„ì‚°
  - CALLS, IMPORTS edge ê°€ì¤‘ì¹˜
  - Damping factor: 0.85

- âœ… `summarizer/llm_summarizer.py` - LLM ìš”ì•½
  - OpenAI/LiteLLM ê¸°ë°˜
  - Chunk ë‹¨ìœ„ summary ìƒì„±
  - Cost control (token budget)
  - Redis ìºì‹±

- âœ… `storage_postgres.py` - PostgreSQL ì €ì¥ì†Œ
  - RepoMap ì˜ì†í™”
  - ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›

- âœ… `incremental.py` - ì¦ë¶„ ì—…ë°ì´íŠ¸
  - Chunk ë³€ê²½ ê°ì§€ â†’ RepoMap ë¶€ë¶„ ì¬ìƒì„±
  - PageRank ì¬ê³„ì‚° (ì˜í–¥ë°›ì€ ì„œë¸Œê·¸ë˜í”„ë§Œ)

**í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**:
```python
# PageRank ì•Œê³ ë¦¬ì¦˜ (Graph-based)
def calculate_pagerank(graph: GraphDocument, damping=0.85, max_iter=100):
    nodes = set(graph.nodes.keys())
    n = len(nodes)

    # ì´ˆê¸°ê°’: ê· ë“± ë¶„í¬
    ranks = {node: 1.0 / n for node in nodes}

    for _ in range(max_iter):
        new_ranks = {}
        for node in nodes:
            # ë“¤ì–´ì˜¤ëŠ” ë§í¬ì˜ rank í•©ì‚°
            rank_sum = sum(
                ranks[src] / out_degree(src)
                for src in incoming_links(node)
            )
            new_ranks[node] = (1 - damping) / n + damping * rank_sum

        # ìˆ˜ë ´ ì²´í¬
        if converged(ranks, new_ranks):
            break

        ranks = new_ranks

    return ranks

# RepoMap íŠ¸ë¦¬ ìƒì„±
def build_repomap_tree(chunks: list[Chunk], pagerank: dict):
    # 1. Chunk hierarchy â†’ Tree ë³€í™˜
    tree = build_tree_from_chunks(chunks)

    # 2. PageRank ì ìˆ˜ ë³‘í•©
    for node in tree.nodes:
        node.importance = pagerank.get(node.chunk_id, 0.0)

    # 3. LLM summary ìƒì„± (leaf chunksë§Œ)
    for leaf in tree.leaves():
        leaf.summary = llm_summarize(leaf.content)

    return tree
```

---

### 4. Index Layer (98%)

**ìœ„ì¹˜**: `src/index/`

**êµ¬í˜„ ì™„ë£Œ**:
- âœ… `common/documents.py` - ê³µí†µ ëª¨ë¸
  - `IndexDocument`: Chunk â†’ Index ì…ë ¥
  - `SearchHit`: í†µí•© ê²€ìƒ‰ ê²°ê³¼ (source, score, metadata)

- âœ… `common/transformer.py` - ë³€í™˜ê¸°
  - Chunk â†’ IndexDocument ë³€í™˜
  - `search_text` ìƒì„±: summary + code + identifiers

- âœ… `lexical/adapter_zoekt.py` - Lexical Index (Zoekt)
  - **Hybrid ë§¤í•‘ ì „ëµ**: Zoekt file+line â†’ ChunkStore â†’ SearchHit
  - **3ë‹¨ê³„ Fallback**:
    1. Exact function/class chunk â†’ score 1.0
    2. File chunk fallback â†’ score 0.8
    3. Virtual chunk_id â†’ score 0.5 (warning)
  - Zoekt DSL ì¿¼ë¦¬ ë¹Œë” (`repo:`, `lang:`, `file:`)
  - ë§¤í•‘ í†µê³„ ë¡œê¹…

- âœ… `vector/adapter_qdrant.py` - Vector Index (Qdrant)
  - `EmbeddingProvider` ì¶”ìƒí™” (Protocol)
  - `OpenAIEmbeddingProvider` êµ¬í˜„ (text-embedding-3-small)
  - Batch embedding (ìµœëŒ€ 2048 texts)
  - Collection ì „ëµ: `code_embeddings_{repo_id}_{snapshot_id_short}`
  - Batch upsert (256 points per batch)
  - Async/await ì „ì²´ ì ìš©

- âœ… `service.py` - IndexingService
  - 5ê°œ index ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (Lexical, Vector, Symbol, Fuzzy, Domain)
  - `search()`: Weighted fusion (RRF, score normalization)
  - Partial failure ì²˜ë¦¬ (ì¼ë¶€ index ì‹¤íŒ¨í•´ë„ ê³„ì†)
  - ì—ëŸ¬ ë¡œê¹… ë° ìˆ˜ì§‘

- âœ… `factory.py` - Factory Pattern
  - `create_indexing_service()`: ì „ì²´ êµ¬ì„±
  - `create_indexing_service_minimal()`: MVP (Lexical + Vectorë§Œ)
  - `IndexingConfig`: í™˜ê²½ë³„ í”„ë¦¬ì…‹ (DEV, PROD, TEST)
  - DI í†µí•© ì§€ì›

**í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**:
```python
# Zoekt Hybrid ë§¤í•‘ ì•Œê³ ë¦¬ì¦˜
async def search_with_chunk_mapping(query: str, repo_id: str):
    # 1. Zoekt ê²€ìƒ‰ (file+line ê²°ê³¼)
    zoekt_results = await zoekt.search(query, limit=100)

    search_hits = []
    stats = {"exact": 0, "file_fallback": 0, "virtual": 0}

    for match in zoekt_results:
        file_path = match.FileName
        line = match.LineNumber

        # 2. ChunkStore ì¡°íšŒ (ìš°ì„ ìˆœìœ„ ì •ë ¬)
        chunk = await chunk_store.find_chunk_by_file_and_line(
            repo_id, file_path, line
        )

        if chunk and chunk.kind in ("function", "class"):
            # 2-1. Exact mapping
            search_hits.append(SearchHit(
                chunk_id=chunk.chunk_id,
                score=1.0,
                source="lexical",
                metadata={"match_type": "exact"}
            ))
            stats["exact"] += 1

        elif file_chunk := await chunk_store.find_file_chunk(repo_id, file_path):
            # 2-2. File fallback
            search_hits.append(SearchHit(
                chunk_id=file_chunk.chunk_id,
                score=0.8,
                source="lexical",
                metadata={"match_type": "file_fallback"}
            ))
            stats["file_fallback"] += 1

        else:
            # 2-3. Virtual chunk_id
            virtual_id = f"virtual:{repo_id}:{file_path}:{line}"
            search_hits.append(SearchHit(
                chunk_id=virtual_id,
                score=0.5,
                source="lexical",
                metadata={"match_type": "virtual", "warning": "no_chunk_found"}
            ))
            stats["virtual"] += 1

    logger.info(f"Zoekt mapping stats: {stats}")
    return search_hits

# Vector Index ë°°ì¹˜ ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜
async def index_documents_batch(docs: list[IndexDocument]):
    # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
    texts = [doc.search_text for doc in docs]

    # 2. Batch embedding (OpenAI API)
    embeddings = await embedding_provider.embed_batch(
        texts, batch_size=2048
    )

    # 3. Qdrant points ìƒì„±
    points = [
        PointStruct(
            id=hash_id(doc.chunk_id),
            vector=embedding,
            payload={
                "chunk_id": doc.chunk_id,
                "repo_id": doc.repo_id,
                "file_path": doc.file_path,
                "kind": doc.kind,
            }
        )
        for doc, embedding in zip(docs, embeddings)
    ]

    # 4. ë°°ì¹˜ upsert (256ê°œì”©)
    for batch in chunked(points, 256):
        await qdrant_client.upsert(
            collection_name=collection_name,
            points=batch
        )

# Weighted Fusion ì•Œê³ ë¦¬ì¦˜ (RRF)
def fuse_search_results(
    results: list[SearchHit],
    weights: dict[str, float]
) -> list[SearchHit]:
    # 1. Sourceë³„ ê·¸ë£¹í™”
    by_source = defaultdict(list)
    for hit in results:
        by_source[hit.source].append(hit)

    # 2. RRF (Reciprocal Rank Fusion)
    fused_scores = defaultdict(float)
    k = 60  # RRF constant

    for source, hits in by_source.items():
        weight = weights.get(source, 1.0)
        for rank, hit in enumerate(hits, start=1):
            rrf_score = weight / (k + rank)
            fused_scores[hit.chunk_id] += rrf_score

    # 3. ì •ê·œí™” ë° ì •ë ¬
    max_score = max(fused_scores.values()) if fused_scores else 1.0
    final_hits = [
        SearchHit(
            chunk_id=chunk_id,
            score=score / max_score,
            source="fusion",
            metadata={"sources": list(by_source.keys())}
        )
        for chunk_id, score in fused_scores.items()
    ]

    final_hits.sort(key=lambda h: h.score, reverse=True)
    return final_hits
```

**ë¯¸ì™„ë£Œ (Phase 3)**:
- â³ Symbol Index (Kuzu Graph) - 2%
- â³ Fuzzy Index (PostgreSQL pg_trgm)
- â³ Domain Index (ë¬¸ì„œ ì „ìš© ê²€ìƒ‰)

---

### 5. Retriever Layer (100%)

**ìœ„ì¹˜**: `src/retriever/`

**êµ¬í˜„ ì™„ë£Œ**:
- âœ… **Phase 1: MVP**
  - `intent/rule_classifier.py` - Intent ë¶„ë¥˜ (find_definition, find_usage, etc.)
  - `multi_index/orchestrator.py` - ë‹¤ì¤‘ ì¸ë±ìŠ¤ ë³‘ë ¬ ì¿¼ë¦¬
  - `fusion/engine.py` - Weighted fusion, RRF
  - `context_builder/builder.py` - Context íŒ¨í‚¤ì§•

- âœ… **Phase 2: Enhanced SOTA**
  - Late Interaction Search (ColBERT-style MaxSim)
  - Cross-encoder Reranking
  - Correlation-aware Fusion
  - Hard Negative Mining

- âœ… **Phase 3: Advanced SOTA**
  - `query/decomposer.py` - Multi-hop query ë¶„í•´
  - `reasoning/test_time_reasoner.py` - o1-style reasoning
  - `observability/explainer.py` - ê²€ìƒ‰ ê²°ê³¼ ì„¤ëª…
  - `code_reranking/structural_reranker.py` - AST ê¸°ë°˜ ì¬ìˆœìœ„
  - `code_reranking/callgraph_reranker.py` - Call graph proximity
  - `adaptive_embeddings/lora_trainer.py` - Repo-adaptive embeddings

**í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**:
```python
# Multi-hop Retrieval ì•Œê³ ë¦¬ì¦˜
async def retrieve_multi_hop(decomposed_query: DecomposedQuery):
    results = []
    context = {}

    for step in decomposed_query.steps:
        # 1. í˜„ì¬ step ê²€ìƒ‰ (ì´ì „ context í™œìš©)
        step_results = await retriever.search(
            query=step.query,
            context=context,
            intent=step.intent
        )

        # 2. ê²°ê³¼ë¥¼ contextì— ì¶”ê°€
        context[step.step_id] = step_results

        # 3. Graph expansion (í•„ìš” ì‹œ)
        if step.expand_graph:
            expanded = await graph_expander.expand(
                step_results,
                direction=step.direction,
                depth=step.depth
            )
            context[step.step_id].extend(expanded)

        results.extend(step_results)

    # 4. ìµœì¢… fusion
    final = fusion_engine.fuse(results)
    return final

# Call Graph Proximity Reranking
def rerank_by_call_graph(
    candidates: list[SearchHit],
    reference_functions: list[str],
    call_graph: KuzuCallGraphAdapter
) -> list[SearchHit]:
    reranked = []

    for hit in candidates:
        # 1. Reference í•¨ìˆ˜ì™€ì˜ call graph ê±°ë¦¬ ê³„ì‚°
        distances = []
        for ref_func in reference_functions:
            # BFS shortest path
            dist = call_graph.shortest_path(
                ref_func, hit.symbol_id
            )
            distances.append(dist if dist else float('inf'))

        # 2. ìµœì†Œ ê±°ë¦¬ ê¸°ì¤€ boost
        min_dist = min(distances)
        if min_dist < float('inf'):
            boost = 1.0 / (1 + min_dist)  # ê±°ë¦¬ 1 â†’ boost 0.5
            hit.score *= (1 + 0.2 * boost)  # ìµœëŒ€ 20% boost
            hit.metadata["call_graph_boost"] = boost

        reranked.append(hit)

    reranked.sort(key=lambda h: h.score, reverse=True)
    return reranked
```

---

### 6. Server Layer (100%)

**ìœ„ì¹˜**: `server/`

**êµ¬í˜„ ì™„ë£Œ**:
- âœ… `api_server/main.py` - FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜
  - Lifespan ê´€ë¦¬ (startup/shutdown)
  - Container ê¸°ë°˜ DI
  - CORS ë¯¸ë“¤ì›¨ì–´

- âœ… `api_server/routes/search.py` - ê²€ìƒ‰ API
  - `GET /search` - í†µí•© í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
  - `GET /search/lexical` - Lexical ì „ìš©
  - `GET /search/vector` - Vector ì „ìš©
  - `GET /search/symbol` - Symbol ì „ìš© (stub)
  - `GET /search/fuzzy` - Fuzzy ì „ìš© (stub)
  - `GET /search/domain` - Domain ì „ìš© (stub)

- âœ… `api_server/routes/indexing.py` - ì¸ë±ì‹± API (stub)
  - `POST /index/repo` - Full indexing
  - `POST /index/incremental` - Incremental indexing
  - `DELETE /index/repo` - Delete index
  - `GET /index/status/{repo_id}` - Status check

- âœ… `mcp_server/main.py` - MCP Server
  - Claude í†µí•© íˆ´
  - `search_code`, `get_chunk`, `get_symbol` ë“±

---

### 7. Infrastructure Layer (100%)

**ìœ„ì¹˜**: `src/infra/`

**êµ¬í˜„ ì™„ë£Œ**:
- âœ… `storage/postgres.py` - PostgreSQL (asyncpg pool)
- âœ… `cache/redis.py` - Redis cache
- âœ… `vector/qdrant.py` - Qdrant async client
- âœ… `graph/kuzu.py` - Kuzu embedded DB
- âœ… `search/zoekt.py` - Zoekt HTTP client
- âœ… `llm/openai.py` - OpenAI/LiteLLM
- âœ… `git/git_cli.py` - GitPython wrapper

**í…ŒìŠ¤íŠ¸**: 426 tests (100% pass)

---

## ğŸš§ êµ¬í˜„ ì˜ˆì • (3%)

### 1. Agent Layer (0%)

**ëª©í‘œ**: Cursorê¸‰ ì½”ë“œ ì—ì´ì „íŠ¸ êµ¬í˜„

#### Phase 1: Tool Layer (1ì£¼)

**ìœ„ì¹˜**: `src/agent/tools/` (ì‹ ê·œ)

**êµ¬í˜„ ì˜ˆì •**:
```python
# src/agent/tools/code_tools.py
async def code_search(query: str, scope: str = "repo") -> SearchResult:
    """
    Semantica Codegraph ê¸°ë°˜ ì½”ë“œ ê²€ìƒ‰.

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬ (ìì—°ì–´ or í‚¤ì›Œë“œ)
        scope: repo | module | file

    Returns:
        SearchResult with ranked chunks
    """
    pass

async def symbol_search(name: str) -> Symbol:
    """ì‹¬ë³¼ ê²€ìƒ‰ (ì •í™•í•œ ì´ë¦„ ë§¤ì¹­)"""
    pass

async def graph_neighbors(
    symbol_id: str,
    direction: str = "both",
    depth: int = 1
) -> list[Symbol]:
    """
    Call graph íƒìƒ‰.

    Args:
        symbol_id: Symbol ID
        direction: callers | callees | both
        depth: íƒìƒ‰ ê¹Šì´
    """
    pass

# src/agent/tools/file_tools.py
async def open_file(path: str, span: Span | None = None) -> str:
    """
    íŒŒì¼ ì—´ê¸° (ì „ì²´ ë˜ëŠ” ì¼ë¶€).

    Args:
        path: íŒŒì¼ ê²½ë¡œ
        span: íŠ¹ì • ë²”ìœ„ (start_line, end_line)
    """
    pass

async def get_span(path: str, start_line: int, end_line: int) -> str:
    """íŒŒì¼ì˜ íŠ¹ì • ë²”ìœ„ ì¶”ì¶œ"""
    pass

# src/agent/tools/patch_tools.py
async def propose_patch(
    path: str,
    span: Span,
    new_code: str,
    reason: str
) -> Patch:
    """
    íŒ¨ì¹˜ ì œì•ˆ (dry-run, ì‹¤í–‰ ì•ˆ í•¨).

    Args:
        path: íŒŒì¼ ê²½ë¡œ
        span: ìˆ˜ì • ë²”ìœ„
        new_code: ìƒˆ ì½”ë“œ
        reason: ìˆ˜ì • ì´ìœ 

    Returns:
        Patch with validation result
    """
    pass

async def apply_patch(patch: Patch) -> ApplyResult:
    """
    íŒ¨ì¹˜ ì ìš© (ì‹¤ì œ íŒŒì¼ ìˆ˜ì •).

    ì£¼ì˜: Reviewer ìŠ¹ì¸ í›„ì—ë§Œ í˜¸ì¶œ!
    """
    pass

# src/agent/tools/test_tools.py
async def run_tests(scope: str = "all") -> TestResult:
    """
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰.

    Args:
        scope: all | module | file | function
    """
    pass

async def run_lint(path: str) -> LintResult:
    """Linter ì‹¤í–‰"""
    pass

# src/agent/tools/git_tools.py
async def git_diff(path: str | None = None) -> str:
    """Git diff ì¡°íšŒ"""
    pass
```

**í•µì‹¬ ì„¤ê³„**:
- ëª¨ë“  íˆ´ì€ JSON ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ (LLM í˜¸ì¶œ ê°€ëŠ¥)
- Async/await ì „ì²´ ì ìš©
- Dry-run ê¸°ë³¸ (side-effectëŠ” ëª…ì‹œì  ìŠ¹ì¸ í•„ìš”)
- ì—ëŸ¬ í•¸ë“¤ë§ + ì¬ì‹œë„ ë¡œì§

---

#### Phase 2: Agent Orchestration (LangGraph) (1-2ì£¼)

**ìœ„ì¹˜**: `src/agent/orchestration/` (ì‹ ê·œ)

**êµ¬í˜„ ì˜ˆì •**:
```python
# src/agent/orchestration/state.py
class AgentState(TypedDict):
    """LangGraph State"""
    messages: list[BaseMessage]
    plan: Plan  # Step-by-step plan
    current_step: int
    tool_results: dict[str, Any]
    context: ContextBundle  # Semantica context
    done: bool
    error: str | None

# src/agent/orchestration/nodes.py
async def planner_node(state: AgentState) -> AgentState:
    """
    ê³„íš ìƒì„± ë…¸ë“œ.

    ì…ë ¥: User request, context
    ì¶œë ¥: Step-by-step plan

    ì•Œê³ ë¦¬ì¦˜:
    1. Intent ë¶„ì„ (find_bug, refactor, add_feature, etc.)
    2. Context ìˆ˜ì§‘ (ê´€ë ¨ íŒŒì¼, ì‹¬ë³¼, call chain)
    3. Plan ìƒì„± (STEP 1: analyze, STEP 2: propose_patch, ...)
    """
    pass

async def tool_router_node(state: AgentState) -> str:
    """
    ë‹¤ìŒ íˆ´ ì„ íƒ ë…¸ë“œ.

    ì…ë ¥: Current plan, current_step
    ì¶œë ¥: Tool name (code_search, open_file, propose_patch, etc.)

    ì•Œê³ ë¦¬ì¦˜:
    1. Planì˜ current_step íŒŒì‹±
    2. Stepì— í•„ìš”í•œ íˆ´ ì‹ë³„
    3. íˆ´ íŒŒë¼ë¯¸í„° ì¤€ë¹„
    """
    pass

async def tool_node(state: AgentState, tool_name: str) -> AgentState:
    """
    íˆ´ ì‹¤í–‰ ë…¸ë“œ.

    ì…ë ¥: Tool name, parameters
    ì¶œë ¥: Tool result (added to state.tool_results)
    """
    pass

async def reviewer_node(state: AgentState) -> AgentState:
    """
    ê²€í†  ë…¸ë“œ.

    ì…ë ¥: Tool results, plan
    ì¶œë ¥: ìŠ¹ì¸ or ì¬ê³„íš ìš”ì²­

    ì•Œê³ ë¦¬ì¦˜:
    1. Proposal ê²€ì¦ (syntax check, test pass, etc.)
    2. íŒŒì¼ ì¼ì¹˜ì„± í™•ì¸
    3. LLM hallucination ê°ì§€
    4. OK â†’ answer_node, NG â†’ planner_node (revise)
    """
    pass

async def answer_node(state: AgentState) -> AgentState:
    """ìµœì¢… ë‹µë³€ ìƒì„±"""
    pass

# src/agent/orchestration/graph.py
def create_agent_graph() -> StateGraph:
    """
    LangGraph ì •ì˜.

    êµ¬ì¡°:
        START â†’ planner â†’ tool_router â†’ tool â†’ reviewer
                  â†‘                              â†“
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€(revise)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â†“
                                               answer â†’ END
    """
    graph = StateGraph(AgentState)

    # Nodes
    graph.add_node("planner", planner_node)
    graph.add_node("tool_router", tool_router_node)
    graph.add_node("tool", tool_node)
    graph.add_node("reviewer", reviewer_node)
    graph.add_node("answer", answer_node)

    # Edges
    graph.add_edge(START, "planner")
    graph.add_edge("planner", "tool_router")
    graph.add_edge("tool_router", "tool")
    graph.add_edge("tool", "reviewer")

    # Conditional edges
    graph.add_conditional_edges(
        "reviewer",
        should_continue,
        {
            "revise": "planner",  # ì¬ê³„íš
            "answer": "answer",   # ì™„ë£Œ
        }
    )

    graph.add_edge("answer", END)

    return graph.compile()
```

**í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**:
```python
# Plan ìƒì„± ì•Œê³ ë¦¬ì¦˜
async def generate_plan(user_request: str, context: ContextBundle) -> Plan:
    # 1. Intent ë¶„ì„
    intent = classify_intent(user_request)

    # 2. Intentë³„ í…œí”Œë¦¿ ì„ íƒ
    if intent == "fix_bug":
        template = BUG_FIX_TEMPLATE
    elif intent == "refactor":
        template = REFACTOR_TEMPLATE
    elif intent == "add_feature":
        template = ADD_FEATURE_TEMPLATE

    # 3. Context ê¸°ë°˜ ê³„íš ì»¤ìŠ¤í„°ë§ˆì´ì§•
    plan = template.customize(context)

    # 4. LLMìœ¼ë¡œ plan ê²€ì¦/ê°œì„ 
    refined_plan = await llm.refine_plan(plan, user_request)

    return refined_plan

# Workflow í…œí”Œë¦¿
BUG_FIX_TEMPLATE = [
    {"step": 1, "action": "symbol_search", "param": "{error_symbol}"},
    {"step": 2, "action": "open_file", "param": "{file_path}"},
    {"step": 3, "action": "graph_neighbors", "param": "{symbol_id}", "direction": "callers"},
    {"step": 4, "action": "propose_patch", "param": "{fix_location}"},
    {"step": 5, "action": "run_tests", "param": "related"},
    {"step": 6, "action": "finalize"},
]
```

---

#### Phase 3: Context Builder (1ì£¼)

**ìœ„ì¹˜**: `src/agent/context/` (ì‹ ê·œ)

**êµ¬í˜„ ì˜ˆì •**:
```python
# src/agent/context/builder.py
class ContextBuilder:
    """
    Semantica Codegraph ê¸°ë°˜ Context íŒ¨í‚¤ì§•.

    ëª©ì : Agentê°€ "ì–´ë””ë¥¼ ê³ ì³ì•¼ í•˜ëŠ”ì§€" ì •í™•íˆ ì°¾ë„ë¡ ì§€ì›
    """

    def __init__(
        self,
        search_service: IndexingService,
        graph_store: KuzuGraphStore,
        chunk_store: ChunkStore,
    ):
        self.search = search_service
        self.graph = graph_store
        self.chunks = chunk_store

    async def build_bug_context(
        self,
        error_message: str,
        stack_trace: str | None = None
    ) -> ContextBundle:
        """
        ë²„ê·¸ ìˆ˜ì • Context ìƒì„±.

        ì•Œê³ ë¦¬ì¦˜:
        1. Stack trace íŒŒì‹± â†’ symbol ì¶”ì¶œ
        2. Symbol search â†’ ê´€ë ¨ íŒŒì¼ ì‹ë³„
        3. Graph traversal â†’ callers/callees í™•ì¥
        4. ê´€ë ¨ í…ŒìŠ¤íŠ¸ ì½”ë“œ ê²€ìƒ‰
        """
        # 1. Symbol ì¶”ì¶œ
        symbols = parse_stack_trace(stack_trace)

        # 2. ê´€ë ¨ íŒŒì¼ ìˆ˜ì§‘
        files = []
        for sym in symbols:
            chunk = await self.chunks.get_chunk(sym.chunk_id)
            files.append(chunk.file_path)

        # 3. Call graph í™•ì¥
        callers = []
        callees = []
        for sym in symbols:
            callers.extend(await self.graph.query_called_by(sym.symbol_id))
            callees.extend(await self.graph.query_calls(sym.symbol_id))

        # 4. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        tests = await self.search.search(
            query=f"test {symbols[0].name}",
            filters={"kind": "function", "path": "*test*"}
        )

        return ContextBundle(
            files=files,
            symbols=symbols + callers + callees,
            call_chains=self._build_call_chains(symbols, callers, callees),
            tests=tests,
            error_message=error_message,
        )

    async def build_refactor_context(
        self,
        target: str,  # file or module
        intent: str   # simplify, extract, rename
    ) -> ContextBundle:
        """ë¦¬íŒ©í† ë§ Context ìƒì„±"""
        pass

    async def build_feature_context(
        self,
        feature_description: str,
        reference_files: list[str]
    ) -> ContextBundle:
        """ìƒˆ ê¸°ëŠ¥ ì¶”ê°€ Context ìƒì„±"""
        pass

# src/agent/context/models.py
class ContextBundle:
    """Context íŒ¨í‚¤ì§€ (Agentì— ì „ë‹¬)"""
    files: list[str]
    symbols: list[Symbol]
    call_chains: list[CallChain]
    tests: list[Chunk]
    metadata: dict[str, Any]
```

**í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**:
```python
# Call Chain ìƒì„± ì•Œê³ ë¦¬ì¦˜
def build_call_chains(
    symbols: list[Symbol],
    callers: list[Symbol],
    callees: list[Symbol]
) -> list[CallChain]:
    """
    Symbol â†’ Callers/Callees ê´€ê³„ë¥¼ Chainìœ¼ë¡œ ì‹œê°í™”.

    ì˜ˆì‹œ:
    main() â†’ authenticate() â†’ check_password() â†’ ERROR
    """
    chains = []

    for sym in symbols:
        # ì—­ë°©í–¥ chain (callers)
        caller_chain = []
        current = sym
        while current:
            caller_chain.insert(0, current)
            parents = [c for c in callers if c.calls(current)]
            current = parents[0] if parents else None

        # ì •ë°©í–¥ chain (callees)
        callee_chain = [sym]
        current = sym
        while current:
            children = [c for c in callees if current.calls(c)]
            current = children[0] if children else None
            if current:
                callee_chain.append(current)

        chains.append(CallChain(
            root=sym,
            callers=caller_chain,
            callees=callee_chain
        ))

    return chains
```

---

#### Phase 4: Multi-step Patch Engine (1-2ì£¼)

**êµ¬í˜„ ì˜ˆì •**:
```python
# src/agent/patch/engine.py
class PatchEngine:
    """
    Multi-step patch ìƒì„± ì—”ì§„.

    ì›Œí¬í”Œë¡œìš°:
    1. propose_patch (dry-run)
    2. Syntax validation
    3. run_tests
    4. Review ìŠ¹ì¸
    5. apply_patch (ì‹¤ì œ ì ìš©)
    6. ì‹¤íŒ¨ ì‹œ rollback â†’ revise
    """

    async def execute_patch_workflow(
        self,
        patches: list[Patch],
        test_scope: str = "affected"
    ) -> WorkflowResult:
        """
        íŒ¨ì¹˜ ì ìš© ì›Œí¬í”Œë¡œìš° ì‹¤í–‰.

        ì•Œê³ ë¦¬ì¦˜:
        1. ê° patchë¥¼ dry-runìœ¼ë¡œ ê²€ì¦
        2. ëª¨ë“  patchê°€ validí•˜ë©´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        3. í…ŒìŠ¤íŠ¸ í†µê³¼ ì‹œ ìˆœì°¨ ì ìš©
        4. ì‹¤íŒ¨ ì‹œ rollback + ì¬ì‹œë„
        """
        # 1. Validation
        for patch in patches:
            valid = await self._validate_patch(patch)
            if not valid:
                return WorkflowResult(
                    status="failed",
                    reason="syntax_error",
                    failed_patch=patch
                )

        # 2. Apply patches (transaction)
        try:
            for patch in patches:
                await apply_patch(patch)

            # 3. Run tests
            test_result = await run_tests(scope=test_scope)

            if test_result.passed:
                return WorkflowResult(status="success")
            else:
                # Rollback
                await self._rollback_patches(patches)
                return WorkflowResult(
                    status="test_failed",
                    test_failures=test_result.failures
                )

        except Exception as e:
            await self._rollback_patches(patches)
            return WorkflowResult(status="error", error=str(e))
```

---

#### Phase 5: ì•ˆì „ì„± + ì—£ì§€ ì¼€ì´ìŠ¤ (ì§€ì†ì )

**êµ¬í˜„ ì˜ˆì •**:
```python
# src/agent/safety/validators.py
class SafetyValidator:
    """Agent ì•ˆì „ì„± ê²€ì¦"""

    async def validate_tool_call(
        self,
        tool_name: str,
        params: dict
    ) -> ValidationResult:
        """
        íˆ´ í˜¸ì¶œ ê²€ì¦.

        ì²´í¬:
        1. JSON ìŠ¤í‚¤ë§ˆ ì¼ì¹˜
        2. íŒŒë¼ë¯¸í„° íƒ€ì… ê²€ì¦
        3. íŒŒì¼ ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€
        4. ê¶Œí•œ ì²´í¬ (side-effect íˆ´)
        """
        pass

    def detect_loop(self, state: AgentState) -> bool:
        """
        ë£¨í”„ ê°ì§€.

        ì•Œê³ ë¦¬ì¦˜:
        1. Plan hash ê³„ì‚°
        2. ìµœê·¼ 5ê°œ plan hash ë¹„êµ
        3. ë™ì¼ plan ë°˜ë³µ ì‹œ ê²½ê³ 
        """
        pass

    def check_max_calls(self, state: AgentState) -> bool:
        """
        ë™ì¼ íˆ´ ë°˜ë³µ í˜¸ì¶œ ì²´í¬.

        ì •ì±…: ë™ì¼ íˆ´ ìµœëŒ€ 3ë²ˆ
        """
        tool_counts = defaultdict(int)
        for result in state.tool_results.values():
            tool_counts[result["tool"]] += 1

        return max(tool_counts.values()) <= 3

# src/agent/safety/hallucination_detector.py
class HallucinationDetector:
    """LLM Hallucination ê°ì§€"""

    async def detect_in_patch(self, patch: Patch) -> bool:
        """
        Patchì—ì„œ hallucination ê°ì§€.

        ì²´í¬:
        1. ë³€ê²½ë˜ì§€ ì•Šì€ ë¼ì¸ í¬í•¨ ì—¬ë¶€
        2. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í•¨ìˆ˜ í˜¸ì¶œ
        3. Contextì™€ ë¬´ê´€í•œ ë³€ê²½
        """
        pass
```

---

### 2. Index Layer Phase 3 (2%)

#### Symbol Index (Kuzu Graph)

**ìœ„ì¹˜**: `src/index/symbol/adapter_kuzu.py` (ë¯¸ì™„ì„±)

**êµ¬í˜„ ì˜ˆì •**:
```python
class KuzuSymbolIndex:
    """
    Kuzu Graph ê¸°ë°˜ Symbol Index.

    ê¸°ëŠ¥:
    - Go-to-definition
    - Find-references
    - Call graph íƒìƒ‰
    """

    async def index_graph(self, graph_doc: GraphDocument) -> None:
        """GraphDocumentë¥¼ Kuzuì— ì €ì¥"""
        pass

    async def search(
        self,
        query: str,
        kind: str | None = None
    ) -> list[SearchHit]:
        """
        Symbol ì´ë¦„ ê²€ìƒ‰.

        ì¿¼ë¦¬ ì˜ˆì‹œ:
        MATCH (s:Symbol {name: $query})
        WHERE s.kind = $kind
        RETURN s
        """
        pass

    async def go_to_definition(self, symbol_id: str) -> SearchHit:
        """Definition ì¡°íšŒ"""
        pass

    async def find_references(self, symbol_id: str) -> list[SearchHit]:
        """
        References ê²€ìƒ‰.

        ì¿¼ë¦¬ ì˜ˆì‹œ:
        MATCH (caller)-[:CALLS]->(target:Symbol {id: $symbol_id})
        RETURN caller
        """
        pass
```

---

#### Fuzzy Index (PostgreSQL pg_trgm)

**ìœ„ì¹˜**: `src/index/fuzzy/adapter_postgres.py` (ë¯¸ì™„ì„±)

**êµ¬í˜„ ì˜ˆì •**:
```python
class PostgresFuzzyIndex:
    """
    PostgreSQL pg_trgm ê¸°ë°˜ Fuzzy Index.

    ê¸°ëŠ¥:
    - ì˜¤íƒ€ í—ˆìš© identifier ê²€ìƒ‰
    - Trigram similarity ë§¤ì¹­
    """

    async def index(self, docs: list[IndexDocument]) -> None:
        """
        Identifier ì¶”ì¶œ ë° ì¸ë±ì‹±.

        SQL:
        INSERT INTO fuzzy_identifiers (chunk_id, identifier)
        VALUES ...
        ON CONFLICT DO NOTHING
        """
        pass

    async def search(
        self,
        query: str,
        threshold: float = 0.3
    ) -> list[SearchHit]:
        """
        Fuzzy ê²€ìƒ‰.

        SQL:
        SELECT chunk_id, identifier,
               similarity(identifier, $query) AS score
        FROM fuzzy_identifiers
        WHERE identifier % $query  -- Trigram match
        ORDER BY score DESC
        """
        pass
```

---

#### Domain Index (PostgreSQL Full-text)

**ìœ„ì¹˜**: `src/index/domain_meta/adapter.py` (ë¯¸ì™„ì„±)

**êµ¬í˜„ ì˜ˆì •**:
```python
class DomainMetaIndex:
    """
    ë¬¸ì„œ ì „ìš© ê²€ìƒ‰ (README, ADR, API docs).

    ê¸°ëŠ¥:
    - Full-text search (tsvector/tsquery)
    - ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜
    """

    async def index(self, docs: list[IndexDocument]) -> None:
        """
        ë¬¸ì„œ ì¸ë±ì‹±.

        SQL:
        INSERT INTO domain_documents (chunk_id, title, content, doc_type)
        VALUES ...
        """
        pass

    async def search(
        self,
        query: str,
        doc_type: str | None = None
    ) -> list[SearchHit]:
        """
        Full-text ê²€ìƒ‰.

        SQL:
        SELECT chunk_id,
               ts_rank(search_vector, plainto_tsquery($query)) AS score
        FROM domain_documents
        WHERE search_vector @@ plainto_tsquery($query)
        ORDER BY score DESC
        """
        pass
```

---

## ğŸ“‹ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Critical (ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥)
1. **Agent Tool Layer** (1ì£¼)
   - Semantica ê¸°ë°˜ tool êµ¬í˜„
   - ê°€ì¥ ë¨¼ì € ì™„ì„±í•´ì•¼ LLMì— ë…¸ì¶œ ê°€ëŠ¥

### High (Tool Layer ì™„ì„± í›„)
2. **Agent Orchestration** (1-2ì£¼)
   - LangGraph ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°
   - Toolê³¼ ì—°ê²°ë˜ì–´ì•¼ ë™ì‘

3. **Context Builder** (1ì£¼)
   - Toolê³¼ ë™ì‹œ ì§„í–‰ ê°€ëŠ¥
   - Agentì˜ "ë„¤ë¹„ê²Œì´ì…˜" ì—­í• 

### Medium (Agent ì™„ì„± í›„)
4. **Symbol Index** (1ì£¼)
   - Kuzu Graph í™œìš©
   - go-to-def, find-refs í•µì‹¬ ê¸°ëŠ¥

5. **Fuzzy/Domain Index** (1ì£¼)
   - ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
   - ì„ íƒì  êµ¬í˜„

---

## ğŸ¯ ìµœì¢… ëª©í‘œ

### 3ê°œì›” ì™„ì„± ê³„íš
- **Month 1**: Agent Tool Layer + Orchestration (Phase 1-2)
- **Month 2**: Context Builder + Multi-step Patch (Phase 3-4)
- **Month 3**: ì•ˆì „ì„± + Index Phase 3 (Phase 5)

### ì˜ˆìƒ ìµœì¢… êµ¬ì„±
```
ì „ì²´ ì§„í–‰ë„: 100%
â”œâ”€â”€ Foundation Layer (âœ… 100%)
â”œâ”€â”€ Chunk Layer (âœ… 100%)
â”œâ”€â”€ RepoMap Layer (âœ… 100%)
â”œâ”€â”€ Index Layer (âœ… 100%)
â”‚   â”œâ”€â”€ Lexical (âœ…)
â”‚   â”œâ”€â”€ Vector (âœ…)
â”‚   â”œâ”€â”€ Symbol (ğŸ”œ)
â”‚   â”œâ”€â”€ Fuzzy (ğŸ”œ)
â”‚   â””â”€â”€ Domain (ğŸ”œ)
â”œâ”€â”€ Retriever Layer (âœ… 100%)
â”œâ”€â”€ Server Layer (âœ… 100%)
â””â”€â”€ Agent Layer (ğŸ”œ 0%)
    â”œâ”€â”€ Tool Layer (ğŸ”œ)
    â”œâ”€â”€ Orchestration (ğŸ”œ)
    â”œâ”€â”€ Context Builder (ğŸ”œ)
    â”œâ”€â”€ Patch Engine (ğŸ”œ)
    â””â”€â”€ Safety (ğŸ”œ)
```

**ìµœì¢… ëª©í‘œ**: Cursorë¥¼ ëŠ¥ê°€í•˜ëŠ” Graph-based ì½”ë“œ ì—ì´ì „íŠ¸! ğŸš€

---

**ì‘ì„± ì™„ë£Œì¼**: 2024-11-24
**ë‹¤ìŒ ì—…ë°ì´íŠ¸**: Agent Layer Phase 1 ì™„ë£Œ í›„
