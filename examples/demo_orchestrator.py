"""Orchestrator ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸

ì‹¤ì œ Agentë¥¼ ì‹¤í–‰í•´ë³´ëŠ” ê°„ë‹¨í•œ ì˜ˆì œ
"""

import asyncio
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.agent.adapters.context_adapter import ContextAdapter
from src.agent.orchestrator import AgentOrchestrator
from src.agent.router.intent_classifier import IntentClassifier
from src.agent.router.router import Router
from src.agent.task_graph.planner import TaskGraphPlanner
from src.agent.workflow.state_machine import WorkflowStateMachine
from src.infra.config.settings import Settings
from src.infra.llm.litellm_adapter import LiteLLMAdapter


async def demo_basic():
    """ê¸°ë³¸ ë°ëª¨: Mock LLM ì‚¬ìš©"""
    print("=" * 70)
    print("ğŸ¯ Orchestrator ê¸°ë³¸ ë°ëª¨ (Mock LLM)")
    print("=" * 70)
    print()

    # Mock LLM for demo
    class MockLLM:
        async def complete(self, prompt: str, **kwargs) -> str:
            if "calculate_total" in prompt.lower():
                return '{"intent": "fix_bug", "reasoning": "Bug fix needed", "confidence": 0.95}'
            return '{"intent": "unknown", "reasoning": "Unclear", "confidence": 0.3}'

    # Setup components
    llm = MockLLM()
    classifier = IntentClassifier(llm)
    router = Router(classifier)
    planner = TaskGraphPlanner()
    workflow = WorkflowStateMachine(max_iterations=1)
    context_adapter = ContextAdapter()

    # Create orchestrator
    orchestrator = AgentOrchestrator(
        router=router,
        task_planner=planner,
        workflow=workflow,
        context_adapter=context_adapter,
    )

    # Execute
    print("ğŸ“¥ User Request: fix bug in calculate_total function")
    print()

    result = await orchestrator.execute(
        user_request="fix bug in calculate_total function", context={"repo_id": "demo-repo"}
    )

    # Display results
    print("ğŸ“¤ Results:")
    print(f"  Intent:       {result.intent.value}")
    print(f"  Confidence:   {result.confidence:.2f}")
    print(f"  Status:       {result.status.value}")
    print(f"  Tasks:        {len(result.tasks_completed)} completed")
    print(f"  - {', '.join(result.tasks_completed)}")
    print(f"  Time:         {result.execution_time_ms:.0f}ms")
    print(f"  Success:      {'âœ…' if result.is_success() else 'âŒ'}")
    print()

    if result.result:
        print("  Result Preview:")
        result_str = str(result.result)[:200]
        print(f"  {result_str}...")
    print()


async def demo_multiple_requests():
    """ì—¬ëŸ¬ ìš”ì²­ ë°ëª¨"""
    print("=" * 70)
    print("ğŸ¯ ì—¬ëŸ¬ ìš”ì²­ ì²˜ë¦¬ ë°ëª¨")
    print("=" * 70)
    print()

    class MockLLM:
        async def complete(self, prompt: str, **kwargs) -> str:
            prompt_lower = prompt.lower()
            if "authentication" in prompt_lower or "add" in prompt_lower:
                return '{"intent": "add_feature", "reasoning": "Feature request", "confidence": 0.90}'
            elif "refactor" in prompt_lower:
                return '{"intent": "refactor", "reasoning": "Refactor request", "confidence": 0.88}'
            elif "bug" in prompt_lower or "fix" in prompt_lower:
                return '{"intent": "fix_bug", "reasoning": "Bug fix", "confidence": 0.95}'
            return '{"intent": "unknown", "reasoning": "Unclear", "confidence": 0.3}'

    llm = MockLLM()
    classifier = IntentClassifier(llm)
    router = Router(classifier)
    planner = TaskGraphPlanner()
    workflow = WorkflowStateMachine(max_iterations=1)
    context_adapter = ContextAdapter()

    orchestrator = AgentOrchestrator(
        router=router,
        task_planner=planner,
        workflow=workflow,
        context_adapter=context_adapter,
    )

    requests = [
        "fix bug in payment processing",
        "add new feature for user authentication",
        "refactor database connection code",
    ]

    results = []
    for i, request in enumerate(requests, 1):
        print(f"{i}. Request: {request}")
        result = await orchestrator.execute(request, {"repo_id": f"repo-{i}"})
        results.append(result)
        print(f"   â†’ {result.intent.value} ({result.confidence:.2f}) - {result.status.value}")
        print()

    print("ğŸ“Š Summary:")
    print(f"  Total requests:  {len(results)}")
    print(f"  Successful:      {sum(1 for r in results if r.is_success())}")
    print(f"  Avg confidence:  {sum(r.confidence for r in results) / len(results):.2f}")
    print(f"  Avg time:        {sum(r.execution_time_ms for r in results) / len(results):.0f}ms")
    print()


async def demo_with_real_llm():
    """ì‹¤ì œ LLM ë°ëª¨ (ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´)"""
    print("=" * 70)
    print("ğŸ¯ ì‹¤ì œ LLM ë°ëª¨ (Optional)")
    print("=" * 70)
    print()

    try:
        settings = Settings()

        # Check if LLM is configured
        if not settings.llm.local_llm_base_url:
            print("âš ï¸  Local LLM not configured. Skipping real LLM demo.")
            print("   Set LOCAL_LLM_BASE_URL in .env to enable.")
            print()
            return

        print(f"ğŸ“¡ Connecting to LLM: {settings.llm.local_llm_base_url}")

        # Try to create LiteLLMAdapter
        llm = LiteLLMAdapter(
            model=settings.llm.local_llm_model or "qwen2.5-coder:32b",
            api_base=settings.llm.local_llm_base_url,
        )

        classifier = IntentClassifier(llm)
        router = Router(classifier)
        planner = TaskGraphPlanner()
        workflow = WorkflowStateMachine(max_iterations=1)
        context_adapter = ContextAdapter()

        orchestrator = AgentOrchestrator(
            router=router,
            task_planner=planner,
            workflow=workflow,
            context_adapter=context_adapter,
        )

        print("ğŸ“¥ Request: fix null pointer exception in getUserData")
        print()

        result = await orchestrator.execute(
            user_request="fix null pointer exception in getUserData method", context={"repo_id": "production-app"}
        )

        print("ğŸ“¤ Results (Real LLM):")
        print(f"  Intent:       {result.intent.value}")
        print(f"  Confidence:   {result.confidence:.2f}")
        print(f"  Status:       {result.status.value}")
        print(f"  Time:         {result.execution_time_ms:.0f}ms")
        print()

    except Exception as e:
        print(f"âš ï¸  Real LLM demo failed: {e}")
        print("   This is OK - using Mock LLM for demos")
        print()


async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                  Orchestrator ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸                       â•‘")
    print("â•‘                                                                  â•‘")
    print("â•‘  Phase 0 ì™„ë£Œ! ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” Agentë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.              â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    try:
        # Demo 1: ê¸°ë³¸
        await demo_basic()

        # Demo 2: ì—¬ëŸ¬ ìš”ì²­
        await demo_multiple_requests()

        # Demo 3: ì‹¤ì œ LLM (optional)
        await demo_with_real_llm()

        print("=" * 70)
        print("âœ… ëª¨ë“  ë°ëª¨ ì™„ë£Œ!")
        print("=" * 70)
        print()
        print("ğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
        print("  1. ì‹¤ì œ LLM ì—°ê²° (.env ì„¤ì •)")
        print("  2. ì‹¤ì œ ì½”ë“œë² ì´ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸")
        print("  3. CLI ì¸í„°í˜ì´ìŠ¤ êµ¬ì¶•")
        print("  4. API ì„œë²„ ë°°í¬")
        print()

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì¤‘ë‹¨ë¨")
    except Exception as e:
        print(f"\n\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
