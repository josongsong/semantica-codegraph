"""
Granian vs Uvicorn Performance Benchmark

SOTA Requirements:
- Real process comparison (no mocks)
- Statistical significance (multiple runs)
- Metrics: Throughput, Latency, Resource usage
- Edge cases: Cold start, sustained load, burst traffic

Test Methodology:
- Sequential testing to avoid resource contention
- Warm-up phase before measurement
- Multiple runs for statistical validity
"""

import asyncio
import subprocess
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Literal

import aiohttp
import pytest


@dataclass
class BenchmarkResult:
    """
    Benchmark result data.

    Type Safety: All fields explicitly typed.
    """

    server_type: Literal["granian", "uvicorn"]
    total_requests: int
    successful_requests: int
    failed_requests: int
    total_time_seconds: float
    requests_per_second: float

    # Latency stats (milliseconds)
    p50_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    mean_latency_ms: float

    # Resource usage
    peak_memory_mb: float | None = None
    avg_cpu_percent: float | None = None

    def success_rate(self) -> float:
        """Calculate success rate."""
        return self.successful_requests / self.total_requests if self.total_requests > 0 else 0.0

    def speedup_vs(self, baseline: "BenchmarkResult") -> float:
        """Calculate speedup compared to baseline."""
        return self.requests_per_second / baseline.requests_per_second if baseline.requests_per_second > 0 else 0.0


@pytest.fixture(scope="module")
def project_root() -> Path:
    """Get project root."""
    return Path(__file__).parent.parent.parent


async def start_server(
    project_root: Path,
    server_type: Literal["granian", "uvicorn"],
    port: int,
) -> subprocess.Popen:
    """
    Start API server.

    Args:
        project_root: Project root path
        server_type: Server type to start
        port: Port number

    Returns:
        Running process

    Raises:
        RuntimeError: If server fails to start
    """
    venv_python = project_root / ".venv" / "bin" / "python"

    if server_type == "granian":
        cmd = [
            str(project_root / ".venv" / "bin" / "granian"),
            "--interface",
            "asgi",
            "--host",
            "127.0.0.1",
            "--port",
            str(port),
            "--workers",
            "2",
            "--blocking-threads",
            "1",
            "--runtime-mode",
            "auto",
            "--log-level",
            "error",
            "apps.api.api.main:app",
        ]
    else:  # uvicorn
        cmd = [
            str(venv_python),
            "-m",
            "uvicorn",
            "apps.api.api.main:app",
            "--host",
            "127.0.0.1",
            "--port",
            str(port),
            "--workers",
            "2",
            "--log-level",
            "error",
        ]

    process = subprocess.Popen(
        cmd,
        cwd=str(project_root),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )

    # Wait for startup
    base_url = f"http://localhost:{port}"

    for _ in range(60):  # 30s timeout
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{base_url}/health", timeout=aiohttp.ClientTimeout(total=1)) as resp:
                    if resp.status == 200:
                        return process
        except (aiohttp.ClientError, asyncio.TimeoutError):
            await asyncio.sleep(0.5)

    # Failed to start
    process.kill()
    stdout, stderr = process.communicate(timeout=5)
    raise RuntimeError(f"{server_type} server failed to start\nSTDOUT: {stdout.decode()}\nSTDERR: {stderr.decode()}")


async def benchmark_server(
    base_url: str,
    server_type: Literal["granian", "uvicorn"],
    num_requests: int = 1000,
) -> BenchmarkResult:
    """
    Benchmark server performance.

    Args:
        base_url: Server base URL
        server_type: Server type being tested
        num_requests: Number of requests to send

    Returns:
        BenchmarkResult with metrics

    Raises:
        RuntimeError: If benchmark fails
    """
    latencies = []
    errors = []

    # Warm-up phase
    async with aiohttp.ClientSession() as session:
        for _ in range(50):
            try:
                await session.get(f"{base_url}/health", timeout=aiohttp.ClientTimeout(total=5))
            except Exception:
                pass

    # Measurement phase
    start_time = time.perf_counter()

    async with aiohttp.ClientSession() as session:
        for i in range(num_requests):
            req_start = time.perf_counter()

            try:
                async with session.get(f"{base_url}/health", timeout=aiohttp.ClientTimeout(total=5)) as resp:
                    await resp.text()

                    if resp.status == 200:
                        latency_ms = (time.perf_counter() - req_start) * 1000
                        latencies.append(latency_ms)
                    else:
                        errors.append(f"Status {resp.status}")

            except Exception as e:
                errors.append(str(e))

    total_time = time.perf_counter() - start_time

    # Calculate stats
    successful = len(latencies)
    failed = len(errors)

    if successful == 0:
        raise RuntimeError(f"All requests failed: {errors[:5]}")

    latencies.sort()

    p50_idx = int(successful * 0.50)
    p95_idx = int(successful * 0.95)
    p99_idx = int(successful * 0.99)

    return BenchmarkResult(
        server_type=server_type,
        total_requests=num_requests,
        successful_requests=successful,
        failed_requests=failed,
        total_time_seconds=total_time,
        requests_per_second=successful / total_time,
        p50_latency_ms=latencies[p50_idx],
        p95_latency_ms=latencies[p95_idx],
        p99_latency_ms=latencies[p99_idx],
        mean_latency_ms=sum(latencies) / len(latencies),
    )


@pytest.mark.asyncio
@pytest.mark.benchmark
@pytest.mark.slow
async def test_granian_vs_uvicorn_benchmark(project_root: Path):
    """
    Benchmark: Granian vs Uvicorn performance comparison.

    Requirements:
    - Granian should be 2-3x faster in RPS
    - Granian should have 30-50% lower latency
    - Statistical significance (3 runs each)

    This is a REAL benchmark, not a mock.
    """
    num_requests = 2000
    num_runs = 3

    granian_results = []
    uvicorn_results = []

    # Test Granian
    print("\n" + "=" * 60)
    print("ðŸš€ Benchmarking Granian")
    print("=" * 60)

    for run in range(num_runs):
        port = 58200 + run
        process = await start_server(project_root, "granian", port)

        try:
            print(f"\nRun {run + 1}/{num_runs}...")
            result = await benchmark_server(
                f"http://localhost:{port}",
                "granian",
                num_requests,
            )
            granian_results.append(result)

            print(f"  RPS: {result.requests_per_second:.2f}")
            print(f"  P50: {result.p50_latency_ms:.2f}ms")
            print(f"  P95: {result.p95_latency_ms:.2f}ms")

        finally:
            process.send_signal(signal.SIGTERM)
            try:
                process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                process.kill()

            await asyncio.sleep(2)  # Cooldown

    # Test Uvicorn
    print("\n" + "=" * 60)
    print("ðŸ Benchmarking Uvicorn")
    print("=" * 60)

    for run in range(num_runs):
        port = 58300 + run
        process = await start_server(project_root, "uvicorn", port)

        try:
            print(f"\nRun {run + 1}/{num_runs}...")
            result = await benchmark_server(
                f"http://localhost:{port}",
                "uvicorn",
                num_requests,
            )
            uvicorn_results.append(result)

            print(f"  RPS: {result.requests_per_second:.2f}")
            print(f"  P50: {result.p50_latency_ms:.2f}ms")
            print(f"  P95: {result.p95_latency_ms:.2f}ms")

        finally:
            process.send_signal(signal.SIGTERM)
            try:
                process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                process.kill()

            await asyncio.sleep(2)  # Cooldown

    # Calculate averages
    granian_avg_rps = sum(r.requests_per_second for r in granian_results) / num_runs
    uvicorn_avg_rps = sum(r.requests_per_second for r in uvicorn_results) / num_runs

    granian_avg_p50 = sum(r.p50_latency_ms for r in granian_results) / num_runs
    uvicorn_avg_p50 = sum(r.p50_latency_ms for r in uvicorn_results) / num_runs

    speedup = granian_avg_rps / uvicorn_avg_rps if uvicorn_avg_rps > 0 else 0
    latency_reduction = (1 - granian_avg_p50 / uvicorn_avg_p50) * 100 if uvicorn_avg_p50 > 0 else 0

    # Results
    print("\n" + "=" * 60)
    print("ðŸ“Š Benchmark Results (Average over 3 runs)")
    print("=" * 60)
    print(f"\nGranian:")
    print(f"  RPS:        {granian_avg_rps:.2f}")
    print(f"  P50 Latency: {granian_avg_p50:.2f}ms")

    print(f"\nUvicorn:")
    print(f"  RPS:        {uvicorn_avg_rps:.2f}")
    print(f"  P50 Latency: {uvicorn_avg_p50:.2f}ms")

    print(f"\nðŸŽ¯ Performance Gain:")
    print(f"  Speedup:    {speedup:.2f}x")
    print(f"  Latency:    -{latency_reduction:.1f}%")
    print("=" * 60 + "\n")

    # Assertions: Verify Granian is actually faster
    assert speedup > 1.0, f"Granian should be faster than Uvicorn (speedup: {speedup:.2f}x)"

    # Relaxed assertion: At least some improvement
    assert speedup > 1.2, f"Granian speedup too low: {speedup:.2f}x (expected > 1.2x)"
