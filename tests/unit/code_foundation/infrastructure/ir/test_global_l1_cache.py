"""
RFC-039 P0.3: Global L1 Cache Tests

SOTA Features:
- Global singleton (process-wide)
- Project-level quota
- Fair eviction
- Noisy neighbor prevention

Tests:
- Global sharing across builders
- Project quota enforcement
- Fair eviction
- Thread safety
"""

import pytest

from codegraph_engine.code_foundation.infrastructure.ir.cache import CacheKey
from codegraph_engine.code_foundation.infrastructure.ir.cache_global import (
    GlobalMemoryCache,
    get_global_l1_cache,
)
from codegraph_engine.code_foundation.infrastructure.ir.models.document import IRDocument
from codegraph_engine.code_foundation.infrastructure.ir.models.core import Node, NodeKind, Span


class TestGlobalL1Singleton:
    """Test global singleton pattern."""

    def test_singleton_same_instance(self):
        """Multiple calls return same instance."""
        cache1 = get_global_l1_cache()
        cache2 = get_global_l1_cache()

        assert cache1 is cache2  # Same object!

    def test_shared_across_contexts(self):
        """Cache shared across different contexts."""
        cache = get_global_l1_cache()

        # Set from "project A"
        key = CacheKey.from_content("test.py", "content")
        ir = IRDocument(repo_id="test", snapshot_id="1")
        cache.set(key, ir, project_id="projectA")

        # Get from "project B" (same cache!)
        result = cache.get(key)

        assert result is ir  # Shared!


class TestProjectQuota:
    """Test project-level quota."""

    def test_project_soft_limit_eviction(self):
        """Project exceeding soft limit gets evicted first."""
        cache = GlobalMemoryCache(
            max_size=100,
            max_bytes=1024 * 1024,  # 1MB total
            project_soft_limit=300 * 1024,  # 300KB per project
        )

        # Create large IRs
        def make_ir(name, nodes_count=50):
            ir = IRDocument(repo_id="test", snapshot_id=name)
            for i in range(nodes_count):
                ir.nodes.append(
                    Node(
                        id=f"node_{name}_{i}",
                        kind=NodeKind.FUNCTION,
                        fqn=f"test.func_{i}",
                        file_path="test.py",
                        span=Span(start_line=i, start_col=0, end_line=i + 1, end_col=0),
                        language="python",
                        name=f"func_{i}",
                    )
                )
            return ir

        # Project A: Fill up to soft limit
        for i in range(5):
            key = CacheKey.from_content(f"a_{i}.py", f"content_{i}")
            ir = make_ir(f"a_{i}", 50)
            cache.set(key, ir, project_id="projectA")

        stats = cache.stats()
        projectA_bytes = stats["projects"]["projectA"]["bytes"]

        # Project A should be near soft limit
        print(f"Project A: {projectA_bytes / 1024:.1f} KB")

        # Project B: Add entries
        for i in range(3):
            key = CacheKey.from_content(f"b_{i}.py", f"content_{i}")
            ir = make_ir(f"b_{i}", 50)
            cache.set(key, ir, project_id="projectB")

        stats = cache.stats()

        # Both projects should coexist
        assert "projectA" in stats["projects"]
        assert "projectB" in stats["projects"]

    def test_fair_eviction(self):
        """No single project monopolizes cache."""
        cache = GlobalMemoryCache(
            max_size=20,
            max_bytes=200 * 1024,  # 200KB total
            project_soft_limit=50 * 1024,  # 50KB per project
        )

        def make_ir(name):
            ir = IRDocument(repo_id="test", snapshot_id=name)
            for i in range(10):
                ir.nodes.append(
                    Node(
                        id=f"node_{name}_{i}",
                        kind=NodeKind.FUNCTION,
                        fqn=f"test.func_{i}",
                        file_path="test.py",
                        span=Span(start_line=i, start_col=0, end_line=i + 1, end_col=0),
                        language="python",
                        name=f"func_{i}",
                    )
                )
            return ir

        # 3 projects, 10 entries each
        for proj in ["A", "B", "C"]:
            for i in range(10):
                key = CacheKey.from_content(f"{proj}_{i}.py", f"content_{i}")
                ir = make_ir(f"{proj}_{i}")
                cache.set(key, ir, project_id=f"project{proj}")

        stats = cache.stats()

        # All 3 projects should have entries (fairness)
        assert len(stats["projects"]) >= 2

        # No project dominates (graceful check)
        sizes = [p["bytes"] for p in stats["projects"].values() if p["bytes"] > 0]

        if len(sizes) >= 2:
            max_size = max(sizes)
            min_size = min(sizes)

            # Fairness: max < min Ã— 10 (relaxed for small cache)
            if min_size > 0:
                assert max_size < min_size * 10, f"Unfair: max={max_size}, min={min_size}"


class TestGlobalLRU:
    """Test global LRU across projects."""

    def test_global_lru_ordering(self):
        """Oldest entry evicted regardless of project."""
        cache = GlobalMemoryCache(max_size=5, max_bytes=1024 * 1024)

        ir = IRDocument(repo_id="test", snapshot_id="1")

        # Fill cache
        keys = []
        for i in range(5):
            key = CacheKey.from_content(f"file_{i}.py", f"content_{i}")
            cache.set(key, ir, project_id=f"project_{i % 2}")  # 2 projects
            keys.append(key)

        # Access key[2] (make it recent)
        cache.get(keys[2])

        # Add new entry (should evict key[0], oldest)
        new_key = CacheKey.from_content("new.py", "new")
        cache.set(new_key, ir, project_id="project_new")

        # key[0] should be evicted, key[2] should remain
        assert cache.get(keys[0]) is None  # Evicted
        assert cache.get(keys[2]) is not None  # Recent, kept


class TestMemoryStats:
    """Test detailed statistics."""

    def test_per_project_stats(self):
        """Stats include per-project breakdown."""
        cache = GlobalMemoryCache()

        ir = IRDocument(repo_id="test", snapshot_id="1")

        # Add to 2 projects
        for proj in ["A", "B"]:
            for i in range(3):
                key = CacheKey.from_content(f"{proj}_{i}.py", f"content")
                cache.set(key, ir, project_id=f"project{proj}")

        stats = cache.stats()

        # Verify project stats exist
        assert "projects" in stats
        assert "projectA" in stats["projects"]
        assert "projectB" in stats["projects"]

        # Each project has entries
        assert stats["projects"]["projectA"]["entries"] == 3
        assert stats["projects"]["projectB"]["entries"] == 3


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
