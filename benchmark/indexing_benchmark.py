#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ì¸ë±ì‹± ë²¤ì¹˜ë§ˆí¬.

ê°œë³„ íŒŒì¼ì„ ì§ì ‘ ì²˜ë¦¬í•˜ë©´ì„œ ì„±ëŠ¥ ì¸¡ì •.
"""

import asyncio
import json
import os
import time
from pathlib import Path

from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv()

# API í‚¤ ë§¤í•‘
if os.getenv("SEMANTICA_OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = os.getenv("SEMANTICA_OPENAI_API_KEY")


async def benchmark_indexing(repo_path: str, sample_size: int = 50):
    """
    ê°„ë‹¨í•œ ì¸ë±ì‹± ë²¤ì¹˜ë§ˆí¬.

    Args:
        repo_path: ë ˆí¬ì§€í† ë¦¬ ê²½ë¡œ
        sample_size: ìƒ˜í”Œ íŒŒì¼ ìˆ˜
    """
    repo_path = Path(repo_path).resolve()
    repo_name = repo_path.name

    print(f"{'=' * 80}")
    print(f"{'ì¸ë±ì‹± ë²¤ì¹˜ë§ˆí¬ (ìƒ˜í”Œ ê¸°ë°˜)':^80}")
    print(f"{'=' * 80}\n")
    print(f"ë ˆí¬ì§€í† ë¦¬: {repo_name}")
    print(f"ê²½ë¡œ: {repo_path}")
    print(f"ìƒ˜í”Œ í¬ê¸°: {sample_size}ê°œ íŒŒì¼\n")

    # Phase 1: íŒŒì¼ ìŠ¤ìº”
    print(f"{'â”€' * 80}")
    print("Phase 1: íŒŒì¼ ìŠ¤ìº”")
    print(f"{'â”€' * 80}")

    start = time.time()

    exclude = [".venv", "venv", "node_modules", ".git", "__pycache__", "build", "dist"]
    all_files = []

    for py_file in repo_path.rglob("*.py"):
        if not any(ex in str(py_file) for ex in exclude):
            all_files.append(py_file)

    scan_time = time.time() - start

    # ìƒ˜í”Œë§
    import random

    sample_files = random.sample(all_files, min(sample_size, len(all_files)))

    print(f"  ì „ì²´ íŒŒì¼: {len(all_files):,}ê°œ")
    print(f"  ìƒ˜í”Œ íŒŒì¼: {len(sample_files):,}ê°œ")
    print(f"  ìŠ¤ìº” ì‹œê°„: {scan_time:.2f}ì´ˆ\n")

    # Phase 2: Container ì´ˆê¸°í™”
    print(f"{'â”€' * 80}")
    print("Phase 2: Container ì´ˆê¸°í™”")
    print(f"{'â”€' * 80}")

    from src.container import Container

    container = Container()

    print("  âœ… Container ì´ˆê¸°í™” ì™„ë£Œ\n")

    # Phase 3: ê°œë³„ íŒŒì¼ ì²˜ë¦¬
    print(f"{'â”€' * 80}")
    print("Phase 3: íŒŒì¼ ì²˜ë¦¬ (ìƒ˜í”Œ)")
    print(f"{'â”€' * 80}")

    processing_times = []
    successful = 0
    failed = 0
    total_lines = 0

    for i, file_path in enumerate(sample_files, 1):
        try:
            file_start = time.time()

            # ë‹¨ìˆœíˆ íŒŒì¼ ì½ê¸°ë§Œ
            content = file_path.read_text(encoding="utf-8", errors="ignore")
            lines = content.count("\n") + 1
            total_lines += lines

            # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” íŒŒì‹±/ì²­í‚¹ì´ ì¼ì–´ë‚  ê²ƒ)
            import hashlib

            _ = hashlib.md5(content.encode()).hexdigest()

            file_time = time.time() - file_start
            processing_times.append(file_time)
            successful += 1

            if i % 10 == 0:
                avg_time = sum(processing_times) / len(processing_times)
                print(
                    f"  [{i:3d}/{len(sample_files):3d}] {file_path.name[:40]:40s} "
                    f"{file_time * 1000:6.1f}ms ({lines:5d} lines, í‰ê· : {avg_time * 1000:6.1f}ms)"
                )

        except Exception as e:
            failed += 1
            if failed <= 3:
                print(f"  âŒ {file_path.name}: {e}")

    total_processing_time = sum(processing_times)

    print(f"\n  ì²˜ë¦¬ ì™„ë£Œ: {successful}/{len(sample_files)}ê°œ")
    print(f"  ì²˜ë¦¬ ì‹¤íŒ¨: {failed}ê°œ")
    print(f"  ì´ ì²˜ë¦¬ ì‹œê°„: {total_processing_time:.2f}ì´ˆ")

    if processing_times:
        avg_time = sum(processing_times) / len(processing_times)
        print(f"  í‰ê·  íŒŒì¼ ì²˜ë¦¬ ì‹œê°„: {avg_time * 1000:.1f}ms")
        print(f"  ì²˜ë¦¬ëŸ‰: {len(processing_times) / total_processing_time:.1f} files/sec")

        # ì˜ˆìƒ ì „ì²´ ì‹œê°„
        estimated_total = avg_time * len(all_files)
        print(f"\n  ğŸ“Š ì „ì²´ ë ˆí¬ì§€í† ë¦¬ ì˜ˆìƒ ì‹œê°„: {estimated_total:.1f}ì´ˆ ({estimated_total / 60:.1f}ë¶„)")

    print(f"\n{'=' * 80}")
    print(f"{'ì™„ë£Œ':^80}")
    print(f"{'=' * 80}\n")

    # ê²°ê³¼ ì €ì¥
    results = {
        "repo": str(repo_path),
        "repo_name": repo_name,
        "total_files": len(all_files),
        "sample_size": len(sample_files),
        "successful": successful,
        "failed": failed,
        "processing_times": processing_times,
        "avg_time_ms": (sum(processing_times) / len(processing_times) * 1000) if processing_times else 0,
        "throughput": len(processing_times) / total_processing_time if total_processing_time > 0 else 0,
        "estimated_total_seconds": (sum(processing_times) / len(processing_times) * len(all_files))
        if processing_times
        else 0,
    }

    # benchmark/reports/{project_name}/{ë²¤ì¹˜ë§ˆí‚¹íƒ€ì…}_{íƒ€ì„ìŠ¤íƒ¬í”„}.json
    output_dir = Path("benchmark") / "reports" / repo_name
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = time.strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"simple_{timestamp}.json"

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"ê²°ê³¼ ì €ì¥: {output_file}\n")


if __name__ == "__main__":
    import sys

    repo_path = sys.argv[1] if len(sys.argv) > 1 else "benchmark/repo-test/small/typer"
    sample_size = int(sys.argv[2]) if len(sys.argv) > 2 else 50

    asyncio.run(benchmark_indexing(repo_path, sample_size))
