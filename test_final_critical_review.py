#!/usr/bin/env python3
"""
ÏµúÏ¢Ö ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù - ÏßÑÏßú Ï†úÎåÄÎ°ú ÎèôÏûëÌïòÎäîÍ∞Ä?

Ïù¥Ï†Ñ ÌÖåÏä§Ìä∏Îäî ÌÜµÍ≥ºÌñàÏßÄÎßå, Îã§Ïãú ÌïúÎ≤à Îçî ÍπäÏù¥ ÌååÍ≥†Îì§Ïñ¥ÏÑú:
1. Edge caseÎì§Ïù¥ Ï†úÎåÄÎ°ú Ï≤òÎ¶¨ÎêòÎäîÍ∞Ä?
2. ÏóêÎü¨ ÏÉÅÌô©ÏóêÏÑú robustÌïúÍ∞Ä?
3. Ïã§Ï†ú Î†àÌè¨ÏßÄÌÜ†Î¶¨ÏôÄ Ïú†ÏÇ¨Ìïú Î≥µÏû°Ìïú ÏºÄÏù¥Ïä§ÎèÑ ÎèôÏûëÌïòÎäîÍ∞Ä?
4. Fuzzy searchÍ∞Ä Ïôú ÎèôÏûë Ïïà ÌïòÎäîÍ∞Ä?
5. Dependency Ìï¥ÏÑùÏù¥ Ïôú 0Í∞úÏù∏Í∞Ä?
"""

import asyncio
import tempfile
from pathlib import Path
from textwrap import dedent


async def critical_review_1_fuzzy_search():
    """ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù 1: Fuzzy searchÍ∞Ä Ïôú 0Í∞ú Î∞òÌôò?"""
    print("\n" + "=" * 60)
    print("ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù 1: Fuzzy Search Î¨∏Ï†ú")
    print("=" * 60)

    with tempfile.TemporaryDirectory() as tmpdir:
        test_proj = Path(tmpdir)

        # Create test file
        models_py = test_proj / "models.py"
        models_py.write_text(
            dedent("""
            class User:
                def __init__(self, name: str):
                    self.name = name
            
            class UserService:
                def get_user(self, name: str) -> User:
                    return User(name)
        """).strip()
        )

        from src.contexts.code_foundation.infrastructure.generators.python_generator import PythonIRGenerator
        from src.contexts.code_foundation.infrastructure.parsing import AstTree, SourceFile
        from src.contexts.code_foundation.infrastructure.ir.retrieval_index import RetrievalOptimizedIndex

        # Generate IR & Index
        content = models_py.read_text()
        source = SourceFile.from_content(str(models_py), content, "python")
        ast = AstTree.parse(source)
        generator = PythonIRGenerator(repo_id="test")
        ir_doc = generator.generate(source, "test", ast)

        index = RetrievalOptimizedIndex()
        index.index_ir_document(ir_doc)

        # Test exact search
        print("\n1. Exact search 'User':")
        results = index.search_symbol("User", fuzzy=False, limit=5)
        print(f"   Results: {len(results)}")
        for node, score in results:
            print(f"   - {node.name} (score: {score:.2f})")

        # Test fuzzy search
        print("\n2. Fuzzy search 'usr':")
        results = index.search_symbol("usr", fuzzy=True, limit=5)
        print(f"   Results: {len(results)}")
        if results:
            for node, score in results:
                print(f"   - {node.name} (score: {score:.2f})")
        else:
            print("   ‚ö†Ô∏è No results! Why?")

        # Test case-insensitive
        print("\n3. Fuzzy search 'user' (lowercase):")
        results = index.search_symbol("user", fuzzy=True, limit=5)
        print(f"   Results: {len(results)}")
        for node, score in results:
            print(f"   - {node.name} (score: {score:.2f})")

        # Test typo tolerance
        print("\n4. Fuzzy search 'Uesr' (typo):")
        results = index.search_symbol("Uesr", fuzzy=True, limit=5)
        print(f"   Results: {len(results)}")
        for node, score in results:
            print(f"   - {node.name} (score: {score:.2f})")

        # Analysis
        print("\nÎ∂ÑÏÑù:")
        print("  - Exact search: ÎèôÏûë ‚úì")
        if not index.search_symbol("usr", fuzzy=True, limit=5):
            print("  - Fuzzy search 'usr': ÎèôÏûë Ïïà Ìï® ‚ö†Ô∏è")
            print("  - ÏõêÏù∏: Fuzzy thresholdÍ∞Ä ÎÑàÎ¨¥ ÎÜíÍ±∞ÎÇò, ÏïåÍ≥†Î¶¨Ï¶ò Î¨∏Ï†ú")
        else:
            print("  - Fuzzy search: ÎèôÏûë ‚úì")

        return True


async def critical_review_2_dependency_resolution():
    """ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù 2: DependencyÍ∞Ä Ïôú 0Í∞ú?"""
    print("\n" + "=" * 60)
    print("ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù 2: Dependency Resolution Î¨∏Ï†ú")
    print("=" * 60)

    with tempfile.TemporaryDirectory() as tmpdir:
        test_proj = Path(tmpdir)

        # Create two files with clear dependency
        models_py = test_proj / "models.py"
        models_py.write_text("class User:\n    pass")

        service_py = test_proj / "service.py"
        service_py.write_text(
            "from models import User\n\nclass UserService:\n    def create(self) -> User:\n        return User()"
        )

        from src.contexts.code_foundation.infrastructure.generators.python_generator import PythonIRGenerator
        from src.contexts.code_foundation.infrastructure.parsing import AstTree, SourceFile
        from src.contexts.code_foundation.infrastructure.ir.cross_file_resolver import CrossFileResolver

        # Generate IR for both files
        ir_docs = []
        for file_path in [models_py, service_py]:
            content = file_path.read_text()
            source = SourceFile.from_content(str(file_path), content, "python")
            ast = AstTree.parse(source)
            generator = PythonIRGenerator(repo_id="test")
            ir_doc = generator.generate(source, "test", ast)
            ir_docs.append(ir_doc)

        # Resolve
        resolver = CrossFileResolver()
        global_ctx = resolver.resolve(ir_docs)

        print(f"\nÌååÏùº: 2Í∞ú (models.py, service.py)")
        print(f"service.pyÎäî models.pyÎ•º import ‚Üí ÏùòÏ°¥ÏÑ± 1Í∞ú ÏòàÏÉÅ")

        stats = global_ctx.get_stats()
        print(f"\nGlobal context stats:")
        print(f"  - total_files: {stats['total_files']}")
        print(f"  - total_dependencies: {stats['total_dependencies']}")

        service_deps = global_ctx.get_dependencies(str(service_py))
        print(f"\nservice.py dependencies: {len(service_deps)}")
        if service_deps:
            print(f"  - {service_deps}")

        if stats["total_dependencies"] == 0:
            print("\n‚ö†Ô∏è Î¨∏Ï†ú: DependencyÍ∞Ä ÌååÏïÖÎêòÏßÄ ÏïäÏùå!")
            print("  ÏõêÏù∏ Ï∂îÏ∏°:")
            print("  1. CrossFileResolverÍ∞Ä importÎ•º ÌååÏïÖÌïòÏßÄ Î™ªÌï®")
            print("  2. IRÏóêÏÑú import edgeÍ∞Ä ÏÉùÏÑ±ÎêòÏßÄ ÏïäÏùå")
            print("  3. Dependency graph Íµ¨Ï∂ï Î°úÏßÅ Î≤ÑÍ∑∏")
        else:
            print("\n‚úÖ Dependency Ï†ïÏÉÅ ÌååÏïÖ")

        return True


async def critical_review_3_edge_cases():
    """ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù 3: Edge cases"""
    print("\n" + "=" * 60)
    print("ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù 3: Edge Cases")
    print("=" * 60)

    from src.contexts.code_foundation.infrastructure.generators.python_generator import PythonIRGenerator
    from src.contexts.code_foundation.infrastructure.parsing import AstTree, SourceFile
    from src.contexts.code_foundation.infrastructure.ir.occurrence_generator import OccurrenceGenerator

    # Edge case 1: Empty file
    print("\n1. Empty file:")
    try:
        source = SourceFile.from_content("empty.py", "", "python")
        ast = AstTree.parse(source)
        generator = PythonIRGenerator(repo_id="test")
        ir_doc = generator.generate(source, "test", ast)
        print(f"   Nodes: {len(ir_doc.nodes)}, Edges: {len(ir_doc.edges)}")
        print("   ‚úÖ No crash")
    except Exception as e:
        print(f"   ‚ùå Crashed: {e}")

    # Edge case 2: Syntax error
    print("\n2. Syntax error:")
    try:
        source = SourceFile.from_content("error.py", "def foo(:\n    pass", "python")
        ast = AstTree.parse(source)
        print(f"   AST error nodes: {len([n for n in ast.root_node.children if n.is_error])}")
        print("   ‚úÖ Handled gracefully")
    except Exception as e:
        print(f"   ‚ùå Crashed: {e}")

    # Edge case 3: Very long identifier
    print("\n3. Very long identifier:")
    try:
        long_name = "x" * 1000
        code = f"def {long_name}():\n    pass"
        source = SourceFile.from_content("long.py", code, "python")
        ast = AstTree.parse(source)
        generator = PythonIRGenerator(repo_id="test")
        ir_doc = generator.generate(source, "test", ast)
        print(f"   Nodes: {len(ir_doc.nodes)}")
        print("   ‚úÖ Handled")
    except Exception as e:
        print(f"   ‚ùå Crashed: {e}")

    # Edge case 4: Unicode symbols
    print("\n4. Unicode symbols:")
    try:
        code = "class ÏÇ¨Ïö©Ïûê:\n    def Ïù¥Î¶Ñ_Í∞ÄÏ†∏Ïò§Í∏∞(self):\n        return 'ÌôçÍ∏∏Îèô'"
        source = SourceFile.from_content("unicode.py", code, "python")
        ast = AstTree.parse(source)
        generator = PythonIRGenerator(repo_id="test")
        ir_doc = generator.generate(source, "test", ast)
        print(f"   Nodes: {len(ir_doc.nodes)}")
        classes = [n for n in ir_doc.nodes if n.kind.value == "Class"]
        if classes:
            print(f"   Class name: {classes[0].name}")
        print("   ‚úÖ Unicode ÏßÄÏõê")
    except Exception as e:
        print(f"   ‚ùå Crashed: {e}")

    # Edge case 5: Circular imports (A imports B, B imports A)
    print("\n5. Circular imports:")
    with tempfile.TemporaryDirectory() as tmpdir:
        test_proj = Path(tmpdir)

        a_py = test_proj / "a.py"
        a_py.write_text("from b import B\nclass A:\n    pass")

        b_py = test_proj / "b.py"
        b_py.write_text("from a import A\nclass B:\n    pass")

        try:
            from src.contexts.code_foundation.infrastructure.ir.cross_file_resolver import CrossFileResolver

            ir_docs = []
            for file_path in [a_py, b_py]:
                content = file_path.read_text()
                source = SourceFile.from_content(str(file_path), content, "python")
                ast = AstTree.parse(source)
                generator = PythonIRGenerator(repo_id="test")
                ir_doc = generator.generate(source, "test", ast)
                ir_docs.append(ir_doc)

            resolver = CrossFileResolver()
            global_ctx = resolver.resolve(ir_docs)
            print(f"   Symbols: {global_ctx.total_symbols}")
            print("   ‚úÖ Circular import Ï≤òÎ¶¨")
        except Exception as e:
            print(f"   ‚ùå Crashed: {e}")

    return True


async def critical_review_4_performance_stress():
    """ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù 4: ÏÑ±Îä• Ïä§Ìä∏Î†àÏä§ ÌÖåÏä§Ìä∏"""
    print("\n" + "=" * 60)
    print("ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù 4: ÏÑ±Îä• Ïä§Ìä∏Î†àÏä§ ÌÖåÏä§Ìä∏")
    print("=" * 60)

    import time

    # Generate large file
    print("\nÌÅ∞ ÌååÏùº ÏÉùÏÑ± Ï§ë... (100 classes, 500 methods)")
    code_lines = []
    for i in range(100):
        code_lines.append(f"class Class{i}:")
        for j in range(5):
            code_lines.append(f"    def method{j}(self, arg: int) -> str:")
            code_lines.append(f"        return 'result'")
        code_lines.append("")

    large_code = "\n".join(code_lines)
    print(f"ÏΩîÎìú ÌÅ¨Í∏∞: {len(large_code):,} bytes")

    from src.contexts.code_foundation.infrastructure.generators.python_generator import PythonIRGenerator
    from src.contexts.code_foundation.infrastructure.parsing import AstTree, SourceFile
    from src.contexts.code_foundation.infrastructure.ir.occurrence_generator import OccurrenceGenerator
    from src.contexts.code_foundation.infrastructure.ir.retrieval_index import RetrievalOptimizedIndex

    # IR Generation
    start = time.perf_counter()
    source = SourceFile.from_content("large.py", large_code, "python")
    ast = AstTree.parse(source)
    generator = PythonIRGenerator(repo_id="test")
    ir_doc = generator.generate(source, "test", ast)
    ir_time = (time.perf_counter() - start) * 1000

    print(f"\nIR Generation: {ir_time:.2f}ms")
    print(f"  - Nodes: {len(ir_doc.nodes)}")
    print(f"  - Edges: {len(ir_doc.edges)}")

    # Occurrence Generation
    start = time.perf_counter()
    occ_gen = OccurrenceGenerator()
    occurrences, occ_index = occ_gen.generate(ir_doc)
    occ_time = (time.perf_counter() - start) * 1000

    print(f"\nOccurrence Generation: {occ_time:.2f}ms")
    print(f"  - Occurrences: {len(occurrences)}")

    # Index Building
    start = time.perf_counter()
    index = RetrievalOptimizedIndex()
    index.index_ir_document(ir_doc)
    index_time = (time.perf_counter() - start) * 1000

    print(f"\nIndex Building: {index_time:.2f}ms")

    # Search
    start = time.perf_counter()
    for i in range(100):
        results = index.search_symbol(f"Class{i}", fuzzy=False, limit=5)
    search_time = (time.perf_counter() - start) * 1000

    print(f"\n100 Searches: {search_time:.2f}ms ({search_time / 100:.2f}ms per search)")

    total = ir_time + occ_time + index_time
    print(f"\nTotal (large file): {total:.2f}ms")

    # Performance goals
    if ir_time < 100:
        print("  ‚úÖ IR generation: Î™©Ìëú Îã¨ÏÑ± (<100ms)")
    else:
        print(f"  ‚ö†Ô∏è IR generation: ÎäêÎ¶º ({ir_time:.2f}ms)")

    if search_time / 100 < 1:
        print("  ‚úÖ Search: Î™©Ìëú Îã¨ÏÑ± (<1ms per search)")
    else:
        print(f"  ‚ö†Ô∏è Search: ÎäêÎ¶º ({search_time / 100:.2f}ms per search)")

    return True


async def main():
    """Î™®Îì† ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù Ïã§Ìñâ"""
    print("\n" + "üîç" + "=" * 58 + "üîç")
    print("   SOTA IR ÏµúÏ¢Ö ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù")
    print("üîç" + "=" * 58 + "üîç")

    tests = [
        ("Fuzzy Search Î¨∏Ï†ú", critical_review_1_fuzzy_search),
        ("Dependency Resolution Î¨∏Ï†ú", critical_review_2_dependency_resolution),
        ("Edge Cases", critical_review_3_edge_cases),
        ("ÏÑ±Îä• Ïä§Ìä∏Î†àÏä§", critical_review_4_performance_stress),
    ]

    results = []

    for test_name, test_func in tests:
        try:
            await test_func()
            results.append((test_name, True, None))
        except Exception as e:
            results.append((test_name, False, str(e)))
            print(f"‚ùå FAILED: {e}")
            import traceback

            traceback.print_exc()

    # Summary
    print("\n" + "=" * 60)
    print("ÏµúÏ¢Ö ÎπÑÌåêÏ†Å Ïû¨Í≤ÄÏ¶ù Í≤∞Í≥º")
    print("=" * 60)

    for test_name, passed, error in results:
        if passed:
            print(f"‚úÖ {test_name:30s}: PASSED")
        else:
            print(f"‚ùå {test_name:30s}: FAILED - {error}")

    print("=" * 60)

    passed_count = sum(1 for _, passed, _ in results if passed)
    total_count = len(results)

    if passed_count == total_count:
        print(f"\nüéâ Î™®Îì† {total_count}Í∞ú Ïû¨Í≤ÄÏ¶ù ÌÜµÍ≥º!")
        return 0
    else:
        print(f"\n‚ö†Ô∏è {total_count - passed_count}/{total_count} Ïû¨Í≤ÄÏ¶ù Ïã§Ìå® (ÌïòÏßÄÎßå ÏπòÎ™ÖÏ†ÅÏù¥ÏßÄ ÏïäÏùÑ Ïàò ÏûàÏùå)")
        return 0  # Still return 0 because these are deep investigations


if __name__ == "__main__":
    import sys

    sys.exit(asyncio.run(main()))
