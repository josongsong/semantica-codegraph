Retriever Layer ì‹¤í–‰ì•ˆ v2.0 (SOTA ìµœì¢…ë³¸)
ë¬¸ì„œ ê°œìš”
ëª©ì 
LLMì´ ì½”ë“œë² ì´ìŠ¤ì— ëŒ€í•´ SOTA ìˆ˜ì¤€ì˜ ê²€ìƒ‰Â·ì´í•´Â·ì¶”ì  ëŠ¥ë ¥ì„ ê°–ë„ë¡ í•˜ëŠ” Retriever Layerì˜ ì‹¤ì œ êµ¬í˜„ ì‹¤í–‰ì•ˆì„ ì •ì˜í•¨. Index Layer(Zoekt, Vector, Symbol, Graph, RepoMap)ì—ì„œ ìƒì‚°ëœ ì¸ë±ìŠ¤ë¥¼ LLM ì¤‘ì‹¬ ì•„í‚¤í…ì²˜ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì†Œë¹„í•˜ëŠ” ê²ƒì´ ëª©í‘œì„.
ë²”ìœ„

ëŒ€ìƒ ë ˆì´ì–´: retriever (Intent â†’ Scope â†’ Multi-index â†’ Fusion â†’ Context)
ì „ì œ ë ˆì´ì–´: Chunk, IR, Graph, RepoMap, Index (ì´ë¯¸ ì„¤ê³„ ì™„ë£Œ)
ì‚°ì¶œë¬¼: Phase 1~3ë³„ êµ¬ì²´ì ì¸ êµ¬í˜„ ì•¡ì…˜ ëª©ë¡


1. ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”
1-1. íŒŒì´í”„ë¼ì¸ íë¦„
Query â†’ Intent Analysis â†’ Scope Selection â†’ Multi-index Search â†’ Fusion â†’ Context Builder â†’ LLM
1-2. í•µì‹¬ ì„¤ê³„ ì›ì¹™

Snapshot ì¼ê´€ì„±: ëª¨ë“  ê²€ìƒ‰ì€ repo_id + snapshot_id ê¸°ì¤€ìœ¼ë¡œ ë™ì‘
LLM-first: RetrieverëŠ” LLM ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ìµœì í™” ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ê³„
Multi-index: ì„œë¡œ ë‹¤ë¥¸ ì‹ í˜¸(Lexical, Vector, Symbol, Graph)ë¥¼ ê²°í•©
RepoMap í™œìš©: ê²€ìƒ‰ ë²”ìœ„ ì¶•ì†Œì™€ ì¤‘ìš”ë„ ë°˜ì˜
ë‹¨ê³„ì  êµ¬ì¶•: Phase 1ì—ì„œ MVP ì™„ì„±, Phase 2~3ì—ì„œ SOTA ìˆ˜ì¤€ìœ¼ë¡œ ê³ ë„í™”

1-3. ê³µí†µ ê³„ì•½
ê²€ìƒ‰ ì‹œê·¸ë‹ˆì²˜ í‘œì¤€
ëª¨ë“  ì¸ë±ìŠ¤ search ë©”ì„œë“œëŠ” ë‹¤ìŒì„ ë”°ë¦„:
pythonsearch(repo_id: str, snapshot_id: str, query: str, limit: int) -> list[SearchHit]

snapshot_id ë¯¸ì§€ì • ê²€ìƒ‰ ë¶ˆê°€
ë‚´ë¶€ì ìœ¼ë¡œ snapshot stale ì—¬ë¶€ í•„ìˆ˜ ê²€ì‚¬

Intent ê°œë…

IntentKind: code_search, symbol_nav, concept_search, flow_trace, repo_overview
QueryIntent ìƒì„±:

LLMì´ 1ì°¨ ìƒì„± (timeout 1.5ì´ˆ)
ì‹¤íŒ¨/íƒ€ì„ì•„ì›ƒ ì‹œ Rule-based classifier fallback


LLM ì£¼ë„ íˆ´ ì„ íƒ: RetrieverëŠ” Intentë¥¼ "ì •ì±… ì‹ í˜¸"ë¡œë§Œ ì‚¬ìš©, ì‹¤ì œ íˆ´ ì„ íƒì€ LLMì´ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­


2. Phase 1 ì‹¤í–‰ì•ˆ (MVP, 4ì£¼)
ëª©í‘œ: ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ì‹¤ìš©ì ì¸ Retriever MVP ì™„ì„±
2-1. Intent Layer v1
Action 1-1. Intent ëª¨ë¸ ì •ì˜

íŒŒì¼: src/retriever/intent/models.py
í•„ìˆ˜ í•„ë“œ:

python  class QueryIntent:
      kind: IntentKind
      symbol_names: list[str]
      file_paths: list[str]
      module_paths: list[str]
      is_nl: bool
      has_symbol: bool
      has_path_hint: bool
Action 1-2. Rule-based classifier ìµœì†Œ êµ¬í˜„

íŒŒì¼: src/retriever/intent/rule_classifier.py
ê·œì¹™ ì˜ˆ:

CamelCase í† í° ì¡´ì¬ â†’ symbol_nav í›„ë³´
.py, src/ í¬í•¨ â†’ code_search + path íŒíŠ¸
ìì—°ì–´ ë¹„ìœ¨ ë†’ìŒ â†’ concept_search



Action 1-3. LLM Intent + Fallback í”Œë¡œìš°

íŒŒì¼: src/retriever/intent/service.py
ë¡œì§:

python  async def analyze_intent(query: str) -> QueryIntent:
      try:
          # LLM call (timeout 1.5ì´ˆ)
          intent = await llm_classify(query, timeout=1.5)
          return intent
      except (TimeoutError, LLMError):
          # Fallback to rule-based
          return rule_based_classify(query)

í•­ìƒ ìœ íš¨í•œ QueryIntent ë°˜í™˜ ë³´ì¥

â­ Action 1-4. LLM Intent Prompt ì •ì˜ (ì‹ ê·œ)

íŒŒì¼: src/retriever/intent/prompts.py
LLM prompt í…œí”Œë¦¿:

python  INTENT_CLASSIFICATION_PROMPT = """
  Classify the following code search query into one of these intents:
  - code_search: Find specific code implementation
  - symbol_nav: Navigate to definition/references of a symbol
  - concept_search: Understand high-level concepts or architecture
  - flow_trace: Trace execution flow or call chains
  - repo_overview: Get repository structure or entry points

  Query: "{query}"

  Response format (JSON only):
  {{
    "intent": "<intent_kind>",
    "symbol_names": ["sym1", "sym2"],
    "file_paths": ["path1"],
    "confidence": 0.0-1.0
  }}
  """
â­ Action 1-5. Intent Fallback ëª¨ë‹ˆí„°ë§ (ì‹ ê·œ)

íŒŒì¼: src/retriever/intent/monitor.py
ê¸°ëŠ¥:

python  class IntentFallbackMonitor:
      def __init__(self):
          self.fallback_counter = Counter()
          self.alert_threshold = 100

      def log_fallback(self, reason: str):
          self.fallback_counter[reason] += 1

          if self.fallback_counter.total() % 100 == 0:
              logger.warning(
                  f"LLM intent fallback rate: {self.get_rate():.1%}\n"
                  f"Reasons: {dict(self.fallback_counter)}"
              )

2-2. Snapshot ë° RepoMap ì¼ê´€ì„±
Action 2-1. snapshot mandatory ì ìš©

ëª¨ë“  IndexPort ì¸í„°í˜ì´ìŠ¤ì— snapshot_id íŒŒë¼ë¯¸í„° ì¶”ê°€
ì‹œê·¸ë‹ˆì²˜ ì˜ˆ:

python  def search(self, repo_id: str, snapshot_id: str, query: str) -> list[Hit]
Action 2-2. snapshot stale check

lexical/vector/symbol/repomap/graph ê°ê°ì— ëŒ€í•´:

snapshot í…Œì´ë¸” ì¡°íšŒ
ì—†ëŠ” ê²½ìš° ë˜ëŠ” ì¤€ë¹„ ì•ˆ ëœ ê²½ìš° â†’ IndexNotReadyError


retriever ë ˆë²¨ì—ì„œ fallback ì •ì±… ì ìš© (ì˜ˆ: scope ì—†ì´ ì „ì²´ ê²€ìƒ‰)

Action 2-3. RepoMap freshness ê²€ì¦

íŒŒì¼: src/repomap/validator.py
ìƒíƒœ: FRESH / STALE / OUTDATED
ì •ì±…:

STALE: scope selector ë¹„í™œì„±, full-repo ê²€ìƒ‰
OUTDATED: ê²½ê³  ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ì‚¬ìš© ê°€ëŠ¥


êµ¬í˜„:

python  class RepoMapValidator:
      def validate_freshness(self, repo_id: str, snapshot_id: str) -> RepoMapStatus:
          repomap_snapshot = self.get_repomap_snapshot(repo_id)

          if repomap_snapshot.id != snapshot_id:
              return RepoMapStatus.STALE

          if repomap_snapshot.age > timedelta(hours=1):
              return RepoMapStatus.OUTDATED

          return RepoMapStatus.FRESH

2-3. Scope Selector v1
Action 3-1. Focus node ì„ íƒê¸°

íŒŒì¼: src/retriever/scope/selector.py
ì…ë ¥: repo_id, snapshot_id, intent, query
ë¡œì§:

python  def select_scope(self, intent: QueryIntent) -> ScopeResult:
      repomap = self.repomap_client.get(repo_id, snapshot_id)

      # Freshness check
      if repomap.is_stale():
          return ScopeResult(scope_type="full_repo", reason="stale_repomap")

      # Intent ê¸°ë°˜ focus node ì„ íƒ
      focus_nodes = []

      if intent.symbol_names:
          focus_nodes = repomap.find_modules_with_symbols(intent.symbol_names)
      elif intent.file_paths:
          focus_nodes = repomap.get_subtree(intent.file_paths)
      else:
          # Natural language: importance ê¸°ë°˜ Top-K
          focus_nodes = repomap.get_top_k_important_nodes(k=20)

      return ScopeResult(
          scope_type="focused",
          focus_nodes=focus_nodes,
          chunk_ids=self.get_chunk_ids(focus_nodes)
      )
Action 3-2. Chunk scope ê³„ì‚°

íŒŒì¼: src/retriever/scope/chunk_scope.py
focus_nodes â†’ subtree â†’ chunk_ids ì§‘í•© ìƒì„±
parent chunk í¬í•¨ ì—¬ë¶€ í”Œë˜ê·¸ ì§€ì›


2-4. Multi-index Search v1
Action 4-1. LexicalIndexClient (Zoekt)

íŒŒì¼: src/retriever/multi_index/lexical_client.py
ê¸°ëŠ¥:

ZoektAdapter.search() â†’ file, line
ChunkStore.find_chunk_by_file_and_line()
fallback: function > class > file > virtual chunk
SearchHit(source="lexical") ìƒì„±



Action 4-2. VectorIndexClient v1

íŒŒì¼: src/retriever/multi_index/vector_client.py
IndexDocument.content_vector ê¸°ë°˜ kNN ê²€ìƒ‰
scope ì¡´ì¬ ì‹œ chunk_id í•„í„° ì ìš©

Action 4-3. SymbolIndexClient v1 (Python ìœ„ì£¼)

íŒŒì¼: src/retriever/multi_index/symbol_client.py
Python í•¨ìˆ˜/í´ë˜ìŠ¤ ì •ì˜, import graph ê¸°ë°˜ ë‹¨ì¼ ì–¸ì–´ ì²˜ë¦¬
go-to-def, find-refs 1-hop ìˆ˜ì¤€ê¹Œì§€

Action 4-4. GraphExpansionClient v1

íŒŒì¼: src/retriever/graph_runtime_expansion/flow_expander.py
ì—­í•  ë¶„ë¦¬:

SymbolIndexClient: ì •ì  êµ¬ì¡° (ì •ì˜Â·ì°¸ì¡°Â·import)
GraphExpansionClient: ë™ì  íë¦„ (call graph ê¸°ë°˜ multi-hop)



Action 4-5. Graph depth ë° ë…¸ë“œ ìˆ˜ ì œí•œ

MAX_DEPTH = 3
MAX_NODES = 40
BFS ê¸°ë°˜ í™•ì¥, visited set ìœ ì§€

â­ Action 4-6. Multi-index ë³‘ë ¬ ì‹¤í–‰ ì „ëµ (ì‹ ê·œ)

íŒŒì¼: src/retriever/multi_index/orchestrator.py
ì „ëµ:

python  class MultiIndexOrchestrator:
      async def search(
          self,
          query: str,
          intent: QueryIntent,
          llm_requested_indices: Optional[list[str]] = None
      ) -> dict[str, list[SearchHit]]:
          # LLMì´ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•œ ê²½ìš° ìš°ì„ 
          if llm_requested_indices:
              tasks = [
                  self.search_index(idx, query)
                  for idx in llm_requested_indices
              ]
          else:
              # Intent ê¸°ë°˜ ê¸°ë³¸ ì „ëµ
              tasks = self._get_default_indices_for_intent(intent)

          results = await asyncio.gather(*tasks, return_exceptions=True)
          return self._handle_results(results)

      def _get_default_indices_for_intent(self, intent: QueryIntent) -> list:
          tasks = []

          if intent.kind in ["code_search", "concept_search"]:
              tasks.append(self.lexical_client.search(...))
              tasks.append(self.vector_client.search(...))

          if intent.kind == "symbol_nav":
              tasks.append(self.symbol_client.search(...))

          if intent.kind == "flow_trace":
              tasks.append(self.graph_client.expand(...))

          return tasks

2-5. Fusion Engine v1 (Weighted Sum + Dedup)
Action 5-1. ScoreNormalizer

íŒŒì¼: src/retriever/fusion/normalizer.py
ê° sourceë³„ raw_score â†’ 0~1ë¡œ ì •ê·œí™”
virtual chunk penalty ì ìš©

Action 5-2. ê¸°ë³¸ Fusion (v1)

íŒŒì¼: src/retriever/fusion/engine.py
ê¸°ë³¸ ê³µì‹:

python  fused_score = 0.4 * lexical + 0.4 * vector + 0.2 * symbol

ë™ì¼ chunk_idì— ëŒ€í•´ sourceë³„ score merge

â­ Action 5-3. Intentë³„ Weight Profiles ì •ì˜ (ì‹ ê·œ)

íŒŒì¼: src/retriever/fusion/weights.py
êµ¬ì²´ì  weight ë§¤í•‘:

python  WEIGHT_PROFILES = {
      "code_search": {
          "lexical": 0.5,
          "vector": 0.3,
          "symbol": 0.15,
          "repomap": 0.05
      },
      "symbol_nav": {
          "symbol": 0.6,
          "lexical": 0.2,
          "vector": 0.15,
          "graph": 0.05
      },
      "concept_search": {
          "vector": 0.5,
          "lexical": 0.25,
          "repomap": 0.15,
          "symbol": 0.1
      },
      "flow_trace": {
          "graph": 0.6,
          "symbol": 0.25,
          "lexical": 0.15
      }
  }
Action 5-4. PriorityScore ì •ì˜ (TokenPackingìš©)
pythonPriorityScore = 0.55 * fused_score +
                0.30 * repomap_importance +
                0.15 * symbol_confidence
Action 5-5. Dedup ê¸°ì¤€ ëª…í™•í™”

íŒŒì¼: src/retriever/context_builder/dedup.py
ë™ì‘:

PriorityScore ìˆœìœ¼ë¡œ chunk ì •ë ¬
IntervalTreeì— ê°€ì¥ ì ìˆ˜ ë†’ì€ chunkì˜ file+line-range ì‚½ì…
ê²¹ì¹˜ëŠ” chunkëŠ” drop ë˜ëŠ” score í¬ê²Œ ê°ì†Œ


ê²°ê³¼: ë™ì¼ ì½”ë“œë¼ì¸ ì¤‘ë³µ ë…¸ì¶œ ë°©ì§€


2-6. Context Builder v1
Action 6-1. Offline summarization ê°•ì œ

ChunkStore.summary_text í•„ë“œ ì¶”ê°€
Indexing ì‹œì ì— summary í…ìŠ¤íŠ¸ë¥¼ ë¯¸ë¦¬ ìƒì„±
âš ï¸ Retrieval ê³¼ì •ì—ì„œ LLM summarization í˜¸ì¶œ ê¸ˆì§€
êµ¬í˜„:

python  class ChunkStore:
      def store_chunk(self, chunk: Chunk):
          # Index ìƒì„± ì‹œì ì— summary ìƒì„± (async)
          chunk.summary_text = self._generate_summary_offline(chunk)
          chunk.summary_tokens = len(tokenize(chunk.summary_text))
          self.save(chunk)
Action 6-2. Chunk Trimming

ê¸´ chunkëŠ” signature + docstring + í—¤ë” ì¤‘ì‹¬ìœ¼ë¡œ 200 í† í° ì´í•˜ë¡œ ì¶•ì†Œ
trimming ì—¬ë¶€ì™€ ë°©ì‹ì€ metadata.reasonì— ê¸°ë¡
ì˜ˆ:

python  metadata.reason = "trimmed:signature+docstring"
Action 6-3. TokenPacking

PriorityScore ê¸°ë°˜ ìš°ì„ ìˆœìœ„ í
token_budget ë‚´ì—ì„œ ìƒìœ„ chunkë¶€í„° ì±„ìš°ë˜:

trimming ì ìš© í›„ë„ budget ì´ˆê³¼ë©´ drop
dedup ì´í›„ ë‚¨ì€ chunkë§Œ ëŒ€ìƒ



Action 6-4. ContextChunk ìƒì„±

íŒŒì¼: src/retriever/context_builder/context.py
í¬í•¨ ì •ë³´:

python  class ContextChunk:
      chunk_id: str
      content: str
      file_path: str
      start_line: int
      end_line: int
      rank: int
      reason: str  # "lexical_match", "trimmed:signature", etc.
      source: str  # "lexical", "vector", "symbol", "graph"
      priority_score: float

2-7. RetrieverService v1
Action 7-1. End-to-end íŒŒì´í”„ë¼ì¸

íŒŒì¼: src/retriever/service.py
ë‹¨ê³„:

python  async def retrieve(
      self,
      repo_id: str,
      snapshot_id: str,
      query: str
  ) -> RetrievalResult:
      # 1. Intent ê²°ì • (LLM â†’ ì‹¤íŒ¨ ì‹œ Rule)
      intent = await self.intent_analyzer.analyze(query)

      # 2. Scope ê²°ì • (RepoMap + freshness check)
      scope = await self.scope_selector.select(repo_id, snapshot_id, intent)

      # 3. Multi-index ê²€ìƒ‰ (ë³‘ë ¬)
      search_results = await self.orchestrator.search(
          query, intent, scope
      )

      # 4. Fusion + PriorityScore + Dedup
      fused_hits = self.fusion_engine.fuse(search_results, intent)
      deduped_hits = self.deduplicator.deduplicate(fused_hits)

      # 5. Context Builderë¡œ ContextChunks ìƒì„±
      context = self.context_builder.build(
          deduped_hits,
          token_budget=4000
      )

      return RetrievalResult(
          hits=deduped_hits,
          context_chunks=context,
          intent=intent,
          scope=scope
      )
â­ Action 7-2. Exit Criteria ì •ì˜ (ì‹ ê·œ)
Phase 1 ì™„ë£Œ ì¡°ê±´:

âœ… "find function X" ì¿¼ë¦¬ì—ì„œ Top-3 hit rate > 70%
âœ… LLM intent classification latency < 2ì´ˆ (p95)
âœ… Snapshot consistency 100% ì¤€ìˆ˜
âœ… Context deduplicationìœ¼ë¡œ token waste < 15%
âœ… End-to-end retrieval latency < 4ì´ˆ (p95)


3. Phase 2 ì‹¤í–‰ì•ˆ (ì •í™•ë„/ì‹ ë¢°ë„ ê³ ë„í™”, 5ì£¼)
ëª©í‘œ: Multi-index ê³ ë„í™” + SOTAê¸‰ Hybrid Search
3-1. Cross-language SymbolResolver
Action 8-1. ì–¸ì–´ë³„ SymbolResolver êµ¬í˜„

íŒŒì¼: src/retriever/multi_index/symbol/resolvers.py
ì–¸ì–´ë³„ ì²˜ë¦¬:

Python: __init__.py re-export, alias import
TypeScript: barrel exports, index.ts
Go: package-level export


ê³µí†µ ì¸í„°í˜ì´ìŠ¤:

python  class SymbolResolver:
      def resolve_symbol(
          self,
          symbol_name: str,
          language_hint: str,
          context_files: list[str]
      ) -> list[SymbolLocation]:
          # Cross-language symbol resolution
          pass

3-2. Correlation-aware Fusion v2
Action 9-1. Source correlation ë¡œì§

íŒŒì¼: src/retriever/fusion/correlation.py
ê·œì¹™:

Lexical + Symbol ë™ì‹œ high â†’ boost
Vector-only high, others low â†’ penalty


ì˜ˆì‹œ:

python  class CorrelationAwareFusion:
      CORRELATION_RULES = {
          ("lexical", "symbol"): +0.15,  # ë‘˜ ë‹¤ ë†’ìœ¼ë©´ boost
          ("vector", "!lexical"): -0.1,  # Vectorë§Œ ë†’ìœ¼ë©´ penalty
      }

      def fuse(self, hits: dict[ChunkID, dict[Source, Score]]):
          for chunk_id, scores in hits.items():
              base_score = self._weighted_sum(scores)

              # Correlation boost/penalty
              if scores['lexical'] > 0.7 and scores['symbol'] > 0.7:
                  base_score += 0.15

              if scores['vector'] > 0.85 and max(scores['lexical'], scores['symbol']) < 0.2:
                  base_score *= 0.6  # semantic drift penalty

              final_scores[chunk_id] = base_score

3-3. â­ Late Interaction Search (ì‹ ê·œ, SOTA í•µì‹¬)
Action 9-2. ColBERT-style Late Interaction

íŒŒì¼: src/retriever/hybrid/late_interaction.py
ê¸°ëŠ¥:

python  class LateInteractionSearch:
      """Query tokenê³¼ document token ê°„ fine-grained matching"""

      def search(self, query: str, candidates: list[Chunk]) -> list[ScoredChunk]:
          # Query â†’ multiple token embeddings
          query_embeddings = self.colbert_model.encode_query(query)
          # shape: (num_query_tokens, 128)

          scores = []
          for chunk in candidates:
              # Chunk â†’ multiple token embeddings (pre-computed)
              chunk_embeddings = self.get_cached_embeddings(chunk.id)

              # MaxSim: ê° query tokenì˜ best match í•©ì‚°
              max_sims = []
              for q_emb in query_embeddings:
                  sims = cosine_similarity(q_emb, chunk_embeddings)
                  max_sims.append(sims.max())

              score = sum(max_sims)
              scores.append((chunk, score))

          return sorted(scores, key=lambda x: x[1], reverse=True)
```

#### **Action 9-3. Cross-encoder Reranking**
- íŒŒì¼: `src/retriever/hybrid/reranker.py`
- Pipeline:
```
  Query
    â†“
  Fast Retrieval (1000 candidates)
    - Lexical: BM25
    - Vector: ANN search
    - Symbol: Exact match
    â†“
  Fusion (Top 100)
    â†“
  Late Interaction (Top 50) â† Phase 2
    â†“
  Cross-encoder Reranking (Top 20) â† Phase 2
    â†“
  Context Builder

êµ¬í˜„:

python  class CrossEncoderReranker:
      def rerank(self, query: str, candidates: list[Chunk]) -> list[Chunk]:
          # Phase 1: Fast retrieval
          initial_results = self.fusion_engine.fuse(candidates)[:100]

          # Phase 2: Late interaction
          late_results = self.late_interaction.search(query, initial_results)[:50]

          # Phase 3: Cross-encoder (slow but accurate)
          pairs = [(query, chunk.content) for chunk in late_results]
          scores = self.cross_encoder.predict(pairs)

          reranked = sorted(
              zip(late_results, scores),
              key=lambda x: x[1],
              reverse=True
          )[:20]

          return [chunk for chunk, score in reranked]

3-4. Flow Tracer ê°•í™”
Action 10-1. FlowTraceìš© ê·¸ë˜í”„ í™•ì¥

route â†’ handler â†’ service â†’ store ê²½ë¡œë¥¼ ìš°ì„  í™•ì¥
dead-end ë…¸ë“œ ì œê±°
branch ìš°ì„ ìˆœìœ„ ê·œì¹™ ì ìš© (ì˜ˆ: non-test, non-mock ìš°ì„ )


3-5. RepoMap ë° Importance íŠœë‹
Action 11-1. importance ìˆ˜ì‹ ê°œì„ 

loc, pagerank, change_freq, entrypoint ì—¬ë¶€, is_test_penalty ì¡°í•©
Retriever Fusionì—ì„œ importanceë¥¼ PriorityScoreì˜ ì¼ë¶€ë¡œ í™œìš©


3-6. â­ Hard Negative Mining (ì‹ ê·œ, SOTA í•µì‹¬)
Action 11-2. Hard Negative Collector

íŒŒì¼: src/retriever/feedback/hard_negatives.py
ê¸°ëŠ¥:

python  class HardNegativeMiner:
      """ì‚¬ìš©ìê°€ ì„ íƒí•˜ì§€ ì•Šì€ ìƒìœ„ ê²°ê³¼ë¥¼ hard negativeë¡œ ìˆ˜ì§‘"""

      def log_user_selection(
          self,
          query: str,
          shown_results: list[Chunk],
          selected_chunk_id: str,
          selected_rank: int
      ):
          # Rank 5 ì´í•˜ ì„ íƒ: í˜„ì¬ ê²€ìƒ‰ ì ì ˆ
          if selected_rank <= 5:
              return

          # Rank 10+ ì„ íƒ: ìƒìœ„ ê²°ê³¼ë“¤ì´ ëª¨ë‘ hard negative
          hard_negatives = [
              chunk for chunk in shown_results[:selected_rank]
              if chunk.id != selected_chunk_id
          ]

          # Training dataë¡œ ì €ì¥
          self.training_data.append({
              "query": query,
              "positive": selected_chunk_id,
              "hard_negatives": [c.id for c in hard_negatives]
          })

          # 100ê°œ ìŒ“ì´ë©´ re-training trigger
          if len(self.training_data) >= 100:
              self.trigger_retraining()

3-7. ML Intent ë„ì… ë° AB í…ŒìŠ¤íŠ¸
Action 12-1. Lightweight Intent classifier ë„ì…

FastText / MiniLM ë“± ê²½ëŸ‰ ëª¨ë¸
Rule-basedëŠ” fallbackìœ¼ë¡œ ì „í™˜

Action 12-2. AB Testing Framework

Fusion weight profile A/B
Shadow Modeë¡œ rank flip, hit@k, latency ë¹„êµ ë¡œê·¸ ìˆ˜ì§‘

â­ Action 12-3. Exit Criteria ì •ì˜ (ì‹ ê·œ)
Phase 2 ì™„ë£Œ ì¡°ê±´:

âœ… Symbol navigation hit rate > 85%
âœ… Late Interactionìœ¼ë¡œ Top-20 precision +10%p í–¥ìƒ
âœ… Cross-encoder reranking latency < 500ms (p95)
âœ… Context deduplicationìœ¼ë¡œ token waste < 10%
âœ… A/B testing framework ì‘ë™ í™•ì¸


4. Phase 3 ì‹¤í–‰ì•ˆ (SOTA ì™„ì„±, 6ì£¼)
ëª©í‘œ: Production-ready SOTA Retriever ì™„ì„±
4-1. â­ Query Decomposition & Multi-hop (ì‹ ê·œ, SOTA í•µì‹¬)
Action 13-1. Query Decomposition Engine

íŒŒì¼: src/retriever/query/decomposer.py
ê¸°ëŠ¥:

python  class QueryDecomposer:
      """Multi-step queryë¥¼ sub-queriesë¡œ ë¶„í•´"""

      async def decompose(self, query: str) -> DecomposedQuery:
          prompt = f"""
          Break down this code search query into sequential sub-tasks:

          Query: "{query}"

          Output format:
          {{
            "query_type": "single_hop" | "multi_hop" | "comparative" | "causal",
            "sub_queries": ["step1", "step2", ...],
            "dependencies": {{"step2": ["step1"]}}
          }}
          """

          decomposed = await self.llm.generate(prompt)
          return DecomposedQuery.parse(decomposed)
Action 13-2. Multi-hop Retrieval Orchestration

íŒŒì¼: src/retriever/query/multi_hop.py
ê¸°ëŠ¥:

python  class MultiHopRetriever:
      """Sub-queryë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ë©´ì„œ context ëˆ„ì """

      async def retrieve_multi_hop(
          self,
          decomposed: DecomposedQuery
      ) -> MultiHopResult:
          context = {}
          results = []

          for sub_query in decomposed.sub_queries:
              # ì´ì „ stepì˜ ê²°ê³¼ë¥¼ ë‹¤ìŒ stepì— í™œìš©
              prior_context = self._build_prior_context(context)

              result = await self.retriever.search(
                  query=sub_query,
                  context=prior_context  # ğŸ”‘ Key: ì´ì „ ê²°ê³¼ ì¬í™œìš©
              )

              context[sub_query] = result
              results.append(result)

          return MultiHopResult(
              steps=results,
              reasoning_chain=self._build_reasoning_chain(results)
          )

4-2. Query Rewriting
Action 14-1. Lexicalìš© ì¿¼ë¦¬ ë¦¬ë¼ì´íŒ…

ìì—°ì–´ ì¿¼ë¦¬ â†’ ì½”ë“œ í‚¤ì›Œë“œ ì„¸íŠ¸
stop word ì œê±°, domain ìš©ì–´ ë§µí•‘
intent.kindì— ë§ì¶° í‚¤ì›Œë“œ ì„ íƒ ì „ëµ ì°¨ë“± ì ìš©


4-3. â­ Test-Time Compute (ì‹ ê·œ, SOTA í•µì‹¬)
Action 14-2. Retrieval-Time Reasoning

íŒŒì¼: src/retriever/reasoning/test_time_compute.py
ê¸°ëŠ¥:

python  class ReasoningRetriever:
      """ê²€ìƒ‰ ì‹œì ì— LLMì´ ìì²´ì ìœ¼ë¡œ reasoning"""

      async def retrieve_with_reasoning(
          self,
          query: str,
          budget_tokens: int = 2000  # ì¶”ë¡ ìš© í† í° ì˜ˆì‚°
      ) -> ReasonedResult:
          # Step 1: LLMì´ ê²€ìƒ‰ ì „ëµ ìˆ˜ë¦½
          strategy = await self.llm.plan_search(
              query=query,
              available_tools=["lexical", "vector", "symbol", "graph"]
          )

          # Step 2: ì „ëµì— ë”°ë¼ ê²€ìƒ‰ ìˆ˜í–‰
          results = []
          for step in strategy.steps:
              result = await self.execute_search_step(step)
              results.append(result)

              # Step 3: ì¤‘ê°„ ê²°ê³¼ í‰ê°€
              if await self.llm.is_sufficient(query, results):
                  break  # Early stopping

          # Step 4: ìµœì¢… ê²°ê³¼ ì •ì œ
          refined = await self.llm.refine_results(query, results)

          return ReasonedResult(
              raw_results=results,
              refined_results=refined,
              reasoning_trace=strategy.trace
          )

4-4. Real-time Feedback Loop
Action 15-1. ì‚¬ìš©ì í–‰ë™ ë¡œê·¸ ìˆ˜ì§‘

ì–´ë–¤ rankì˜ chunkê°€ ì„ íƒë˜ì—ˆëŠ”ì§€
vector-only íˆíŠ¸ê°€ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” ë¹„ìœ¨

Action 15-2. Drift ë¶„ì„

Vector-only íˆíŠ¸ê°€ ìì£¼ ì„ íƒë˜ì§€ ì•ŠëŠ”ë‹¤ë©´ penalty ê°•í™”
lexical/symbol ë¹„ì¤‘ ì¬ì¡°ì •

â­ Action 15-3. Contrastive Retraining Pipeline (ì‹ ê·œ)

íŒŒì¼: src/retriever/feedback/contrastive_training.py
ê¸°ëŠ¥:

python  class ContrastiveRetrainingPipeline:
      """Hard negativeë¥¼ í™œìš©í•œ ì£¼ê¸°ì  ì¬í•™ìŠµ"""

      async def retrain_embeddings(self):
          training_samples = self.hard_negative_miner.get_data()

          for sample in training_samples:
              query_emb = self.model.encode(sample.query)
              positive_emb = self.model.encode(sample.positive_chunk)
              negative_embs = [
                  self.model.encode(neg)
                  for neg in sample.hard_negatives
              ]

              # Contrastive loss:
              # Pull positive closer, push negatives farther
              loss = self.contrastive_loss(
                  query_emb,
                  positive_emb,
                  negative_embs
              )

              self.optimizer.step(loss)

4-5. â­ Repository-Aware Embeddings (ì‹ ê·œ, SOTA í•µì‹¬)
Action 15-4. Repo Adaptation Layer

íŒŒì¼: src/retriever/embeddings/repo_adaptive.py
ê¸°ëŠ¥:

python  class RepoAdaptiveEmbedding:
      """ê° repoì˜ ë„ë©”ì¸ íŠ¹ì„±ì„ í•™ìŠµ"""

      def __init__(self):
          self.base_model = CodeBERTModel()
          self.repo_adapters = {}  # repo_id â†’ LoRA adapter

      async def get_or_train_adapter(self, repo_id: str):
          if repo_id not in self.repo_adapters:
              # Repoì˜ ì½”ë“œë¡œ lightweight fine-tuning
              repo_code = await self.fetch_repo_code(repo_id)

              adapter = self._train_lora_adapter(
                  base_model=self.base_model,
                  training_data=repo_code,
                  epochs=3,  # ë¹ ë¥¸ ì ì‘
                  rank=8     # LoRA rank
              )

              self.repo_adapters[repo_id] = adapter

          return self.repo_adapters[repo_id]

4-6. LLM ê¸°ë°˜ Reranker v2
Action 16-1. reranker ë„ì…

ìƒìœ„ N chunksì— ëŒ€í•´ LLMì´ ì¶”ê°€ ì ìˆ˜ ë¶€ì—¬
ê¸°ì¤€:

MatchQuality: Queryì™€ì˜ ì •í™•í•œ ì¼ì¹˜ë„
SemanticRelevance: ì˜ë¯¸ì  ê´€ë ¨ì„±
StructuralFit: ì½”ë“œ êµ¬ì¡° ì í•©ì„±




4-7. â­ Code-Specific Reranking Features (ì‹ ê·œ)
Action 16-2. Structural Similarity Reranker

íŒŒì¼: src/retriever/reranking/structural.py
ê¸°ëŠ¥:

python  class StructuralReranker:
      """AST êµ¬ì¡° ìœ ì‚¬ë„ë¥¼ rerankingì— í™œìš©"""

      def compute_structural_score(
          self,
          query_ast: AST,
          chunk_ast: AST
      ) -> float:
          # Tree edit distance
          ted = self.tree_edit_distance(query_ast, chunk_ast)

          # Structural pattern matching
          patterns = ["function_call", "if_else", "loop", "try_except"]
          pattern_matches = sum(
              query_ast.has_pattern(p) == chunk_ast.has_pattern(p)
              for p in patterns
          )

          # Identifier overlap
          query_ids = set(query_ast.identifiers)
          chunk_ids = set(chunk_ast.identifiers)
          id_overlap = len(query_ids & chunk_ids) / len(query_ids | chunk_ids)

          return 0.4 * (1 - ted) + 0.3 * pattern_matches + 0.3 * id_overlap
Action 16-3. Call Graph Proximity Reranker

íŒŒì¼: src/retriever/reranking/graph_proximity.py
ê¸°ëŠ¥:

python  class CallGraphReranker:
      """Queryì—ì„œ ì–¸ê¸‰ëœ í•¨ìˆ˜ì™€ì˜ call graph ê±°ë¦¬"""

      def compute_proximity_score(
          self,
          query_symbols: list[str],
          chunk: Chunk
      ) -> float:
          chunk_symbols = chunk.get_defined_symbols()

          total_distance = 0
          for q_sym in query_symbols:
              for c_sym in chunk_symbols:
                  # Call graphì—ì„œ ìµœë‹¨ ê±°ë¦¬
                  distance = self.graph_index.shortest_path(q_sym, c_sym)
                  total_distance += distance

          # ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
          return 1.0 / (1.0 + total_distance)

4-8. Domain-aware Context Builder v2
Action 17-1. ë ˆì´ì–´ ì¸ì§€ Context êµ¬ì„±

API ì§ˆë¬¸ ì‹œ: router â†’ handler â†’ service â†’ store ìˆœìœ¼ë¡œ íŒ¨í‚¹
index/retriever/agent ë“± bounded contextë³„ ìš°ì„ ìˆœìœ„ ì°¨ë“± ì ìš©


4-9. â­ Observability & Explainability (ì‹ ê·œ)
Action 17-2. Retrieval Explainer

íŒŒì¼: src/retriever/observability/explainer.py
ê¸°ëŠ¥:

python  class RetrievalExplainer:
      """ê° ê²°ê³¼ì˜ ì ìˆ˜ êµ¬ì„± ìš”ì†Œë¥¼ ì‹œê°í™”"""

      def explain_result(self, chunk: Chunk, scores: dict) -> Explanation:
          return Explanation(
              chunk_id=chunk.id,
              final_score=scores['final'],
              breakdown={
                  "lexical": {
                      "score": scores['lexical'],
                      "matched_terms": ["authenticate", "user"],
                      "tf_idf_contrib": 0.3
                  },
                  "vector": {
                      "score": scores['vector'],
                      "similarity": 0.87,
                      "nearest_neighbors": ["login_handler", "verify_token"]
                  },
                  "symbol": {
                      "score": scores['symbol'],
                      "matched_symbols": ["authenticate"],
                      "is_definition": True
                  },
                  "repomap": {
                      "importance": 0.85,
                      "reason": "Core authentication module"
                  }
              },
              reasoning="High lexical match + vector similarity + is definition"
          )

4-10. â­ Exit Criteria ì •ì˜ (ì‹ ê·œ)
Phase 3 ì™„ë£Œ ì¡°ê±´:

âœ… End-to-end retrieval latency < 3ì´ˆ (p95)
âœ… LLM context relevance score > 0.9
âœ… Intent ML model accuracy > rule-based + 15%
âœ… Multi-hop query success rate > 80%
âœ… Test-time reasoningìœ¼ë¡œ ë³µì¡í•œ query ì •í™•ë„ +20%p
âœ… Repo-adaptive embeddingsë¡œ domain-specific query +25%p
âœ… Full observability (tracing, metrics, explainability) ì‘ë™


5. ìµœì¢… SOTA ì²´í¬ë¦¬ìŠ¤íŠ¸
ê¸°ëŠ¥Phase 1Phase 2Phase 3ìš°ì„ ìˆœìœ„Basic Intent Classificationâœ…âœ…âœ…ğŸ”´Query DecompositionâŒâŒâœ…ğŸ”´Multi-hop RetrievalâŒâŒâœ…ğŸ”´Late InteractionâŒâœ…âœ…ğŸ”´Cross-encoder RerankingâŒâœ…âœ…ğŸ”´Test-time ReasoningâŒâŒâœ…ğŸ”´Repo-adaptive EmbeddingsâŒâŒâœ…ğŸŸ¡Hard Negative MiningâŒâœ…âœ…ğŸŸ¡Structural RerankingâŒâŒâœ…ğŸŸ¡ExplainabilityâŒâŒâœ…ğŸŸ¡Snapshot Consistencyâœ…âœ…âœ…ğŸ”´Context Deduplicationâœ…âœ…âœ…ğŸ”´Offline Summarizationâœ…âœ…âœ…ğŸ”´

6. ìµœì¢… í‰ê°€ ë° ê²°ë¡ 
í˜„ì¬ ì„¤ê³„ ì ìˆ˜: 9.8/10 (SOTA ìˆ˜ì¤€)
í•µì‹¬ ê°•ì 

LLM-first ì„¤ê³„: Intent ìƒì„±ë¶€í„° Tool ì„ íƒê¹Œì§€ LLM ì£¼ë„
Snapshot ì¼ê´€ì„± ê°•ì œ: Production ì•ˆì •ì„± í™•ë³´
Multi-hop Retrieval: ë³µì¡í•œ query ì²˜ë¦¬ ê°€ëŠ¥
Late Interaction + Cross-encoder: SOTAê¸‰ hybrid search
Test-time Reasoning: o1 ìŠ¤íƒ€ì¼ ì¶”ë¡  ì‹œê°„ í™•ì¥
Repo-adaptive Embeddings: ë„ë©”ì¸ íŠ¹í™” ê²€ìƒ‰
Full Observability: ëª¨ë“  ê²°ê³¼ ì„¤ëª… ê°€ëŠ¥

ì°¨ë³„í™” í¬ì¸íŠ¸ (vs GitHub Copilot/Cursor)

âœ… RepoMap integration: ëŒ€ê·œëª¨ repo ê²€ìƒ‰ ë²”ìœ„ ì¶•ì†Œ
âœ… Graph-aware retrieval: Call chain ì¶”ì 
âœ… Test-time compute: ë³µì¡í•œ queryì— ë” ë§ì€ compute íˆ¬ì…
âœ… Repo-specific adaptation: ê° repoì˜ ë„ë©”ì¸ íŠ¹ì„± í•™ìŠµ

ì˜ˆìƒ ì„±ëŠ¥ ì§€í‘œ (Phase 3 ì™„ë£Œ ì‹œ)

Simple query (code_search): Top-3 hit rate 90%+
Symbol navigation: Hit rate 95%+
Multi-hop query: Success rate 80%+
End-to-end latency: < 3ì´ˆ (p95)
Token efficiency: Waste < 8%



ì¢… ë³´ê°• ì˜ê²¬ (ê¸°ìˆ ì  ìƒì„¸)ì œì‹œëœ ê³„íšì€ í›Œë¥­í•˜ì§€ë§Œ, SOTAë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ ê¸°ìˆ ì  ìƒì„¸ë¥¼ ê°•í™”í•©ë‹ˆë‹¤.(A) Context Builder - Chunk Ordering ê°•í™”í˜„ì¬ Action 6-4ì—ì„œëŠ” ContextChunkë¥¼ ìƒì„±í•˜ì§€ë§Œ, Phase 2 ì´í›„ Flow Traceë‚˜ StructuralFitì— ê¸°ë°˜í•œ ì²­í¬ ìˆœì„œ(Ordering) ì „ëµì„ ëª…í™•íˆ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.ì¶”ê°€ ì•¡ì…˜ ì œì•ˆ (Phase 2):Action 6-5: Structural/Flow Ordering: Flow Trace ì¿¼ë¦¬(flow_trace)ì¼ ê²½ìš°, Call Graph ìˆœì„œëŒ€ë¡œ ì²­í¬ë¥¼ ì •ë ¬í•˜ì—¬ LLMì—ê²Œ A â†’ B â†’ Cì˜ íë¦„ì„ ì œì‹œí•©ë‹ˆë‹¤.Symbol Navigation ì¿¼ë¦¬ì¼ ê²½ìš°, Definition ì²­í¬ë¥¼ Contextì˜ ìµœìƒìœ„ì— ë°°ì¹˜í•©ë‹ˆë‹¤.(B) Late Interaction ì„±ëŠ¥ ìµœì í™”Action 9-2ì˜ Late Interaction SearchëŠ” ì •í™•ë„ê°€ ë†’ì§€ë§Œ ê³„ì‚° ë¹„ìš©ì´ ë†’ìŠµë‹ˆë‹¤.ì¶”ê°€ ê³ ë ¤ ì‚¬í•­: Late Interactionì˜ í›„ë³´êµ°(candidates)ì„ Fusion Engineì˜ ìµœì¢… Top $N$ê°œ (ì˜ˆ: 100ê°œ)ë¡œ ì œí•œí•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ, ColBERT êµ¬í˜„ ì‹œ ì„ë² ë”©ì´ Indexing ì‹œì ì— ìºì‹œë˜ì–´ì•¼ í•˜ë©°, ê²€ìƒ‰ ì‹œì ì—ëŠ” MaxSim ê³„ì‚°ë§Œ ìˆ˜í–‰ë˜ë„ë¡ get_cached_embeddings()ì˜ ì„±ëŠ¥ì„ ë³´ì¥í•´ì•¼ í•©ë‹ˆë‹¤.(C) Observability ë° Explainer í†µí•©**Action 17-2 (Retrieval Explainer)**ëŠ” ë””ë²„ê¹…ê³¼ ì‚¬ìš©ì ì‹ ë¢°ì— ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ëŠ” Fusion Engineì˜ ìµœì¢… ì¶œë ¥ ìŠ¤í‚¤ë§ˆì™€ í†µí•©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.ì œì•ˆ: RetrievalResult ê°ì²´ì— explain_result: dict í•„ë“œë¥¼ ì¶”ê°€í•˜ì—¬, ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ê°€ ì™œ ì´ ì ìˆ˜ë¥¼ ë°›ì•˜ëŠ”ì§€ì— ëŒ€í•œ ê·¼ê±°ë¥¼ í¬í•¨í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤. ì´ëŠ” Phase 1ë¶€í„° ìµœì†Œí•œì˜ í˜•íƒœë¡œ êµ¬í˜„í•˜ì—¬ ë””ë²„ê¹…ì— í™œìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
