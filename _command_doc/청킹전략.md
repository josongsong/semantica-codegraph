# Semantica v2: Chunking Layer Architecture (청킹 레이어 아키텍처)

---

# 0. Semantica v2 전체 **7단계** 아키텍처 개요 (Agent Layer 포함)

Semantica v2 엔진은 총 **7단계 파이프라인**으로 구성되며, Chunking Layer는 그 중 **2단계 핵심 레이어**임.

| 단계 | 레이어명                         | 핵심 역할                                            | 주요 산출물                                                                      |
| -- | ---------------------------- | ------------------------------------------------ | --------------------------------------------------------------------------- |
| 1  | **Parser Layer**             | 코드 → CodeNode 변환                                 | CodeNode, AST span, Node ID                                                 |
| 2  | **Chunking Layer** (본 문서)    | CodeNode → Leaf/Parent Chunk, Summary, Embedding | LeafChunk, ParentChunk, Summary, SearchText, EmbeddingDocument, DeltaChunks |
| 3  | **Graph Construction Layer** | 호출·참조·정의 그래프 생성                                  | GraphEdge (calls, contains, imports, overrides)                             |
| 4  | **Indexing Layer**           | 문서·벡터·그래프 인덱싱 및 증분 업데이트                          | Qdrant, Meilisearch, Kùzu 인덱스                                               |
| 5  | **RepoMap Layer**            | 코드베이스 전체 구조 요약 지도 생성                             | ProjectMap, 중요도(PageRank), 구조 Tree                                          |
| 6  | **Retriever Layer**          | Hybrid 검색 + Rerank + Context Builder             | symbol/semantic/graph 검색 결과, context bundles                                |
| 7  | **Agent Layer**              | 코드 편집·리팩토링·분석을 수행하는 실행계층                         | Tool call, Patch, Action Plan, FixFlow                                      |

Chunking Layer는 Parser에서 CodeNode를 받아, 이후 **Graph → Indexing → RepoMap → Retriever → Agent**에서 모두 사용하는 기반 단위를 생성하는 핵심 레이어임. (옵션 A 반영)

Chunking Layer는 Parser에서 CodeNode를 받아, 이후 **Graph·Indexing·RepoMap·Retriever·Agent**에서 모두 사용하는 핵심 기반 단위를 생성하는 역할을 담당함.

## 0.1 Chunking Layer 입력/출력 (Delta 포함)

* 입력

  * CodeNode 리스트 (파일 단위)
  * RepoConfig / ChunkingConfig
  * ParserDiagnostics (선택)
  * 이전 인덱싱 상태에서의 기존 Chunk 스냅샷(OldChunks) 레퍼런스

* 출력

  * LeafChunk 리스트 (NewLeafChunks)
  * ParentChunk 리스트 (NewParentChunks)
  * EmbeddingDocument 리스트 (NewEmbeddingDocs)
  * DeltaChunks

    * Insert: 새로 생성된 청크
    * Update: 내용이 변경된 청크
    * Delete: 더 이상 존재하지 않는 청크
    * No-Op: 변경 없는 청크 (인덱싱 레이어에는 전달하지 않음)

Chunking Layer는 단순히 "새 청크 생성"에 그치지 않고, 이전 상태와 비교하여 **증분 업데이트(Delta Processing)의 첫 관문** 역할을 수행함.

---

# 1. 개요 및 설계 목표

(본 문단 아래에 Chunking Layer의 역할과 비역할을 명확히 구분해 추가함)

## 1.3 Chunking Layer에서 **하는 것(Responsibilities)**

### 1) CodeNode → Chunk 구조 생성

* 함수·클래스·파일 단위 ParentChunk 생성
* 코드 조각(LeafChunk) 생성
* Chunk 계층 구조(parent-child) 생성

### 2) Summary / SearchText 생성

* ParentChunk 단위 LLM 요약(summary) 생성
* 문서·벡터 검색용 search_text 생성

### 3) EmbeddingDocument 생성

* summary/code/signature 용도별 임베딩 문서 생성
* cross-file 링크 정보 포함

### 4) 증분 업데이트(DeltaChunk) 계산

* OldChunks vs NewChunks 비교해 Insert/Update/Delete/No-Op 계산
* Indexing Layer로 전달되는 변경 최소화

### 5) Stable Chunk ID 생성

* canonical_content_hash + node_id 기반
* 상단 라인 이동에 영향받지 않는 ID 생성

### 6) Chunk 메타데이터 생성

* 중요도 기반 adaptive chunking 기준점
* lineage 정보(source_file_hash 등)

## 1.4 Chunking Layer에서 **하지 않는 것(Non‑Responsibilities)**

Chunking Layer는 아래 작업을 **절대 수행하지 않음**. 이는 후속 레이어의 책임임.

### 1) **청크 계층 구조 확장(File → Module → Repo)**

* Chunking Layer는 "Leaf ↔ Parent" 구조만 생성함
* FileSummary, ModuleSummary, RepoSummary는 **RepoMap Layer**에서 생성됨
* 즉 3·4·5단계 계층(파일·모듈·레포 요약)은 Chunking 레이어의 역할이 아님

### 2) 그래프(edge) 생성

* calls / contains / imports / overrides 등 그래프 생성은 **Graph Construction Layer(3단계)** 책임

### 3) 실제 인덱싱(문서·벡터·그래프 저장)

* Qdrant, Meilisearch, Kùzu 저장은 **Indexing Layer(4단계)** 책임
* Chunking은 단지 "인덱싱용 문서 준비"까지만 수행

### 4) RepoMap 생성

* PageRank/중요도 계산
* 최적화된 구조 트리 생성
* 상위 요약(파일/모듈/레포)
  → 모두 **RepoMap Layer(5단계)** 책임

### 5) 검색 / rerank / context 조립

→ **Retriever Layer(6단계)** 책임

### 6) 코드 수정·추천·자동 패치

→ **Agent Layer(7단계)** 책임

Chunking Layer는 **검색·편집의 재료를 만드는 단계**고, 실제 검색/편집 로직은 다른 레이어에서 수행됨. (청킹 레이어 아키텍처)

---

# 1. 개요 및 설계 목표

Semantica v2 Chunking Layer는 Parser 레이어에서 생성된 Raw CodeNode를 기반으로, **코드 검색(Search)**, **코드 이해(Comprehension)**, **코드 편집(Editing)** 및 **RepoMap 생성**에 최적화된 다중 계층(Hierarchical) Chunk를 생성하는 핵심 레이어임.

Cursor, Sourcegraph, Bloop, GitHub Copilot 등 SOTA 코드 인텔리전스 시스템의 기법 및 RAPTOR·Semantic Chunking·Hierarchical Summaries 등 연구 기반으로 설계되며 Semantica v2 엔진 품질을 결정하는 중심 단계임.

## 1.1 핵심 설계 원칙

### 1) 구조 우선 + 의미 보정

* 함수, 클래스, 파일 등 **코드 구조 단위**를 1차 기준
* 토큰 기반 커팅, 의미 기반(Semantic refinement) 보정 적용

### 2) 다중 계층(Hierarchical) 청킹

* ParentChunk (구조 단위) + LeafChunk (코드 조각)로 상·하위 맥락 제공
* FileSummary → ModuleSummary → RepoSummary 까지 연결되는 RAPTOR-style 구조 지원

### 3) 안정적인 Chunk ID (Stable ID)

* 코드 상단의 변경에도 ID가 바뀌지 않도록 **내용 기반 해싱(canonical hashing)** 중심 설계

### 4) 요약 기반 임베딩 (Optimal Embedding)

* Summary → Signature → Code 순으로 자연어 기반 텍스트를 구성해 semantic 검색 최적화

### 5) 플러그형 전략 (Pluggable Strategy)

* LeafChunker / ParentChunker / SummaryBuilder 등이 전략 객체로 구현
* 언어별·도메인별로 교체 가능

### 6) [SOTA 강화] 동적·적응형 청킹

* ParentChunk 중요도(Importance Score)에 따라 **max_tokens를 자동 조절**하는 adaptive chunking 지원

## 1.2 Chunking의 파이프라인 위치

| 단계 | 레이어                | 입력                      | 출력                                                     |
| -- | ------------------ | ----------------------- | ------------------------------------------------------ |
| 1  | Parser             | Raw Code                | CodeNode 리스트                                           |
| 2  | **Chunking**       | CodeNode + RepoConfig   | Parent/Leaf Chunks, Summary/SearchText, Embedding Docs |
| 3  | Graph Construction | CodeNode · Chunk        | Graph (Call, Contains, Summarizes)                     |
| 4  | Indexing           | Chunks + Embedding Docs | Vector/Document Index                                  |

---

# 2. 도메인 모델 및 ID 안정성

## 2.1 Chunk ID 안정화 전략 (보완 적용)

위치 기반(span) 해싱은 상단 변경으로 인해 **ID 전파 붕괴(ID shift)**가 발생해 증분 인덱싱 효율이 떨어짐.
따라서 **내용 기반(canonical content) 해싱 + 안정 Node ID**를 조합해야 함.

### LeafChunk ID

```
chunk_id = hash(repo_id + file_path + canonical_text + parent_id)
```

### ParentChunk ID

```
chunk_id = hash(repo_id + node_id + signature_text)
```

### 목적

* 내용 변경만 반영, 상단에 주석/import가 추가되어도 ID 유지
* Node 기반으로 구조적 안정성 강화

---

## 2.2 LeafChunk 모델 (최소 단위)

```python
class LeafChunk(BaseModel):
    id: str
    repo_id: str
    file_path: str
    language: str

    kind: Literal["code", "doc", "mixed"]
    parent_id: str | None
    node_id: str | None

    start_line: int
    end_line: int

    text: str
    token_count: int

    attrs: dict[str, Any] = {}
```

---

## 2.3 ParentChunk 모델 (구조 단위)

RepoMap 및 Insights 레이어까지 고려해 확장한 스키마.

```python
class ParentChunk(BaseModel):
    id: str
    repo_id: str
    file_path: str
    language: str

    node_id: str | None
    title: str | None
    kind: str

    span_start_line: int
    span_end_line: int

    summary_text: str | None
    search_text: str | None

    leaf_ids: list[str] = []
    token_count: int | None = None

    # Insights + RepoMap
    importance_score: float = 0.0
    graph_centrality: float = 0.0
    last_commit_date: str | None = None

    # SOTA 강화: 동적 청킹 기준점
    suggested_max_tokens: int | None = None

    attrs: dict[str, Any] = {}
```

---

## 2.4 EmbeddingDocument 모델 (벡터 인덱싱용)

```python
class EmbeddingDocument(BaseModel):
    id: str
    repo_id: str

    ref_type: Literal["leaf", "parent", "node", "file"]
    ref_id: str

    # 다중 임베딩 목적 구분
    embedding_purpose: Literal["summary", "code", "signature"]

    text_for_embedding: str
    metadata: dict[str, Any] = {}

    # Cross-file 문맥을 위한 링크
    cross_file_links: list[str] = []

    # 동적 청킹 설정
    dynamic_max_tokens: int | None = None
```

## 2.5 DeltaChunk 모델 (증분 인덱싱용)

DeltaChunk는 이전 인덱싱 상태의 Chunk와 새로 생성된 Chunk를 비교한 결과를 표현하는 도메인 모델임.

```python
class ChunkDeltaOperation(str, Enum):
    INSERT = "insert"
    UPDATE = "update"
    DELETE = "delete"
    NOOP = "noop"

class ChunkDelta(BaseModel):
    chunk_id: str
    kind: Literal["leaf", "parent"]  # 어떤 타입의 청크인지

    operation: ChunkDeltaOperation

    # 변경 전/후 버전 추적을 위한 해시 (optional)
    old_hash: str | None = None
    new_hash: str | None = None

    # 레퍼런스용 메타데이터
    ref_type: Literal["leaf", "parent"]
    ref_id: str  # LeafChunk.id 또는 ParentChunk.id

    # 삭제의 경우, ref_id는 이전 상태 기준
```

DeltaChunk는 Indexing Layer에서:

* INSERT → 새 문서/벡터/그래프 노드 추가
* UPDATE → 기존 문서/벡터/그래프 노드 업데이트
* DELETE → 기존 문서/벡터/그래프 노드 삭제
* NOOP → 인덱싱 레이어에 전달하지 않음 (Chunking 단계에서 걸러짐)

---

# 3. Chunking Granularity (계층적 구조)

| 유형          | 목적           | 사용처           |
| ----------- | ------------ | ------------- |
| LeafChunk   | LLM 입력 최소 단위 | 코드 편집·직접 인용   |
| ParentChunk | 함수·클래스 중심 맥락 | 검색·요약·RepoMap |
| FileSummary | 파일 전체 요약     | Agent·고수준 탐색  |
| RepoSummary | 프로젝트 개요      | RepoMap       |

LLM Retrieval 품질의 핵심 대상은 ParentChunk.

---

# 4. Chunk Boundary 알고리즘

## 4.1 Token 기반 Baseline

* min_tokens, max_tokens, overlap_lines 기준으로 1차 분할

## 4.2 구조 기반 보정 (Syntax-Aware)

* CodeNode의 구조 경계를 최우선으로 사용
* 가능하면 함수/클래스 단위로 청크 구분

## 4.3 의미 기반 Refinement (SOTA 확장)

* LeafChunk 인접 줄의 embedding 유사도 기반 분할/병합
* RAPTOR 연구 기반 Semantic boundary detection 적용 가능

---

# 5. LeafChunker Architecture

* TokenizerPort 기반 토큰 계산
* SyntaxAware + TokenAware 정책 혼합
* Large file safe mode: 상단 N줄 chunking + 나머지는 요약 전용 처리

## 5.1 TokenizerPort 명세

Chunking Layer는 특정 LLM/임베딩 모델에 종속되지 않도록, 토크나이저를 추상화한 TokenizerPort를 사용함.

```python
class TokenizerPort(ABC):
    @abstractmethod
    def encode(self, text: str) -> list[int]:
        """텍스트를 토큰 ID 시퀀스로 변환"""
        ...

    @abstractmethod
    def count_tokens(self, text: str) -> int:
        """텍스트를 토큰화했을 때 토큰 개수 반환"""
        ...
```

* 구현 예시

  * OpenAITokenizerPort (gpt-4 계열)
  * AnthropicTokenizerPort (claude 계열)
  * LocalTokenizerPort (로컬 LLM)

ChunkingConfig에는 `tokenizer_profile` 또는 `target_model`을 통해 어떤 TokenizerPort를 사용할지 선택할 수 있음.

## 5.2 CanonicalTextNormalizerPort 명세

Stable Chunk ID 및 Delta 판단을 위해 **canonical text**가 필요하며, 이를 담당하는 포트가 CanonicalTextNormalizerPort임.

```python
class CanonicalTextNormalizerPort(ABC):
    @abstractmethod
    def strip_comments(self, text: str, language: str) -> str:
        """주석 제거"""
        ...

    @abstractmethod
    def strip_whitespace(self, text: str) -> str:
        """불필요한 공백/빈 줄 제거"""
        ...

    @abstractmethod
    def normalize_signature(self, node: CodeNode) -> str:
        """함수/메서드 시그니처를 표준화된 문자열로 변환"""
        ...

    @abstractmethod
    def canonicalize(self, text: str, language: str) -> str:
        """청크 해시 생성에 사용할 canonical text 생성"""
        ...
```

* LeafChunk/ParentChunk ID 생성 시

  * canonical_text = canonicalize(node.text, language)
  * signature_text = normalize_signature(node)

이 포트 덕분에, 주석/포맷팅/사소한 공백 변경은 ID에 영향을 주지 않고, **실제 의미 있는 코드 변경만** Delta로 인식할 수 있음.

---

# 6. ParentChunker Architecture

* CodeNode 기반 ParentChunk 생성
* LeafChunk.parent_id 안정적 매핑
* ParentChunk.ordinal_in_file 유지
* File-level ParentChunk는 반드시 1개 생성

---

# 7. Summary / SearchText Builder (LLM 최적 템플릿)

## 7.1 Summary 템플릿

```
You are summarizing a [kind] named [title].
This artifact is defined in [file_path], and is part of [module_summary].
It is called by:
[callers_summary]
Here is the source code:
[leaf_code]
Task: Provide a concise summary following [summary_principle].
```

## 7.2 SearchText 구성 순서 (LLM Alignment)

1. summary_text (자연어 기반)
2. file_path / symbol_name
3. signature / title
4. representative code snippet

→ semantic + lexical 검색 모두 강화됨.

---

# 8. ChunkingConfig

```python
class ChunkingConfig(BaseModel):
    min_tokens: int = 50
    max_tokens: int = 256
    overlap_lines: int = 3

    max_chunks_per_file: int = 200

    language_overrides: dict[str, "ChunkingConfig"] = {}

    enable_semantic_refine: bool = False
    semantic_refine_threshold: float = 0.8

    # Adaptive Chunking (SOTA)
    enable_adaptive_sizing: bool = True
    min_importance_score_for_split: float = 0.7

    # Security
    enable_pii_masking: bool = True
```

---

# 9. SOTA 확장 및 안정성 보강

## 9.1 Cross-file Context Inlining

* Graph 레이어의 호출/참조 정보를 EmbeddingDocument.cross_file_links에 저장
* 리트리버가 ParentChunk 외부의 시그니처·요약을 빠르게 인라인 가능

## 9.2 PII / Secret Filtering

* SummaryBuilderPort 앞단에 **PIIFilterPort** 적용
* LLM 호출 시 민감 정보 제거

## 9.3 Observability (모니터링)

* chunking_time_per_file: 파일당 청킹 처리 시간 (성능 모니터링)
* chunks_per_file: 파일당 생성된 청크 개수 (폭발 여부 확인)
* token_waste_ratio: 사용된 LLM 컨텍스트 토큰 대비 실제 유효 코드 토큰 비율

이 메트릭들은 인덱싱/검색 파이프라인 튜닝 및 병목 분석에 활용됨.

---

# 10. Lineage & Auditing (혈통 및 감사)

Chunking 결과를 안전하게 재현·관리하기 위해, 각 Chunk/EmbeddingDocument에 혈통(Lineage) 정보를 남김.

## 10.1 Lineage 필드 예시

* source_file_hash: 원본 파일 내용 해시 (코드 변경 여부 판단)
* parser_version: CodeNode를 생성한 Parser 버전
* chunking_config_hash: 사용된 ChunkingConfig의 해시
* summary_llm_version: Summary 생성에 사용된 LLM 모델/버전
* generation_run_id: 이 Chunk를 생성한 배치/증분 실행 ID

이 정보를 활용해:

* 파서/청킹/LLM 버전 변경 시, 어떤 청크를 재생성해야 하는지 결정
* 특정 배치 실행 결과를 롤백하거나 재실행 가능

## 10.2 Latency-Aware Consumption

Retriever/Agent 레이어는 Lineage 정보를 활용해 컨텍스트 구성 전략을 조정할 수 있음.

* summary_llm_version이 최신이 아닌 경우

  * ParentChunk.summary_text 대신 LeafChunk.text + title 기반의 빠른 컨텍스트 사용
* generation_run_id 기준으로, 실험용 인덱스와 운영 인덱스를 분리 관리

이로써 Chunking Layer는 단순 전처리 단계를 넘어, **증분 인덱싱·버전 관리·실험 관리의 기반 레이어** 역할까지 수행하게 됨.
