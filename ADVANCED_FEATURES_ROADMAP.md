# ğŸš€ Advanced Features Implementation Roadmap

**ëª©í‘œ**: ì—…ê³„ SOTAë¥¼ ë„˜ì–´ì„œëŠ” ì°¨ì„¸ëŒ€ Code Intelligence Engine êµ¬ì¶•  
**í˜„ì¬ ìƒíƒœ**: SOTA IR ì™„ì„± (17/18, 94%)  
**ë‹¤ìŒ ë‹¨ê³„**: P0 ê¸°ë³¸ SOTA â†’ P1 ì°¨ì„¸ëŒ€ ê¸°ëŠ¥

---

## ğŸ“Š í˜„ì¬ ìƒíƒœ ìš”ì•½

### âœ… ì™„ì„±ëœ ê¸°ëŠ¥ (SOTAê¸‰)
- Symbol Resolution: 100%
- Call Graph: Inter-procedural, 828 edges
- Dataflow: READS/WRITES tracking
- Incremental Update: 192x faster
- Module/Import Graph: Canonical signatures
- Inheritance Graph: 9/9 tracking
- Exception Tracking: raises/catches
- Graph Query: BFS/DFS, Pattern matching

### ğŸš§ ë¶€ë¶„ êµ¬í˜„ (í™•ì¥ í•„ìš”)
- Type Narrowing: ê¸°ë³¸ êµ¬ì¡°ë§Œ ìˆìŒ â†’ Full implementation í•„ìš”
- Context-Insensitive Call Graph â†’ Context-Sensitiveë¡œ ì—…ê·¸ë ˆì´ë“œ

### ğŸ“ ë¯¸êµ¬í˜„ (ì‹ ê·œ ê¸°ëŠ¥)
- Local Overlay (Uncommitted Changes Layer)
- Semantic Region Index (SRI)
- Impact-Based Partial Rebuild
- Speculative Graph Execution
- Semantic Change Detection
- AutoRRF Query Fusion

---

## ğŸ¯ P0: ê¸°ë³¸ SOTA ê¸°ëŠ¥ (ì—…ê³„ í‘œì¤€ì„ í™•ì‹¤íˆ ë„˜ê¹€)

### 1.1 Local Overlay (Uncommitted Changes Layer)
**Impact**: â­â­â­â­â­ (Critical - ì •í™•ë„ 30-50% í–¥ìƒ)  
**Difficulty**: â­â­â­â­ (Hard)  
**Priority**: P0 - ìµœìš°ì„   
**Status**: ğŸš§ TODO (Must-Have 18/18 ë‹¬ì„±)

#### í•µì‹¬ ê°€ì¹˜
- IDE/Agent ì •í™•ë„ë¥¼ **ì¦‰ì‹œ 30-50% í–¥ìƒ**
- ì‚¬ìš©ìê°€ í¸ì§‘ ì¤‘ì¸ ì½”ë“œë¥¼ IR/Graphì— **ì‹¤ì‹œê°„ ë°˜ì˜**
- Sourcegraph: ë§¤ìš° ì œí•œì 
- CodeQL: ê±°ì˜ ì§€ì› ì•ˆ í•¨
- **êµ¬í˜„í•˜ë©´ ì—…ê³„ SOTA í™•ì •**

#### ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Layer (LSP, Agent, Retrieval)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Overlay Merge Layer (NEW!)                     â”‚
â”‚  - Base Snapshot (committed code)               â”‚
â”‚  - Overlay Graph (uncommitted changes)          â”‚
â”‚  - Smart Merge Strategy                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Base IR/Graphâ”‚  â”‚ Overlay IR/Graph â”‚
â”‚ (Committed)  â”‚  â”‚ (Uncommitted)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### êµ¬í˜„ ì „ëµ

**Phase 1: Overlay IR Builder** (1ì£¼)
```python
# src/contexts/analysis_indexing/infrastructure/overlay/overlay_builder.py

class OverlayIRBuilder:
    """Uncommitted changesë¥¼ IRë¡œ ë³€í™˜"""
    
    def build_overlay(
        self,
        base_snapshot_id: str,
        uncommitted_files: Dict[str, str]  # path -> content
    ) -> OverlaySnapshot:
        """
        1. Uncommitted filesë§Œ íŒŒì‹±
        2. Base IRê³¼ì˜ delta ê³„ì‚°
        3. Overlay IR document ìƒì„±
        """
        pass
    
    def invalidate_affected(
        self,
        changed_file: str,
        base_graph: KuzuGraph
    ) -> Set[str]:
        """
        ë³€ê²½ íŒŒì¼ì— ì˜í–¥ë°›ëŠ” symbols ê³„ì‚°
        - Callers of changed functions
        - Importers of changed modules
        - Subtypes of changed classes
        """
        pass
```

**Phase 2: Graph Merge Strategy** (1ì£¼)
```python
# src/contexts/analysis_indexing/infrastructure/overlay/graph_merger.py

class GraphMerger:
    """Base + Overlay graphë¥¼ ë³‘í•©"""
    
    def merge_graphs(
        self,
        base: KuzuGraph,
        overlay: OverlayGraph
    ) -> MergedSnapshot:
        """
        Merge ì „ëµ:
        1. Symbol ì¶©ëŒ í•´ê²° (overlay ìš°ì„ )
        2. Edge ì—…ë°ì´íŠ¸ (call graph, import graph)
        3. Dead symbol ì œê±° (baseì—ë§Œ ìˆëŠ” deleted symbols)
        """
        pass
    
    def resolve_symbol_conflict(
        self,
        base_symbol: Symbol,
        overlay_symbol: Symbol
    ) -> Symbol:
        """
        Overlayê°€ í•­ìƒ ìš°ì„ :
        - Signature ë³€ê²½ â†’ overlay ì‚¬ìš©
        - Body ë³€ê²½ â†’ overlay ì‚¬ìš©
        - ì‚­ì œ â†’ baseì—ì„œ ì œê±°
        """
        pass
```

**Phase 3: Query Layer Integration** (3ì¼)
```python
# src/contexts/retrieval_search/infrastructure/overlay_aware_retriever.py

class OverlayAwareRetriever:
    """Overlayë¥¼ ê³ ë ¤í•œ ê²€ìƒ‰"""
    
    async def search(
        self,
        query: str,
        repo_id: str,
        include_overlay: bool = True  # NEW!
    ) -> List[SearchResult]:
        """
        1. Base snapshotì—ì„œ ê²€ìƒ‰
        2. Overlay snapshot ê³ ë ¤
        3. ê²°ê³¼ merge (overlay ìš°ì„ )
        """
        if include_overlay:
            merged_snapshot = self.get_merged_snapshot(repo_id)
            return self.search_in_snapshot(query, merged_snapshot)
        else:
            return self.search_in_snapshot(query, base_snapshot)
```

**Phase 4: LSP Integration** (3ì¼)
```python
# server/mcp_server/overlay_lsp_handler.py

class OverlayLSPHandler:
    """LSP ìš”ì²­ ì‹œ overlay ë°˜ì˜"""
    
    def handle_definition(
        self,
        file: str,
        position: Position,
        uncommitted_content: str
    ) -> List[Location]:
        """
        1. Fileì˜ uncommitted contentë¡œ overlay ìƒì„±
        2. Merged snapshotì—ì„œ ì •ì˜ ì°¾ê¸°
        3. Overlay symbol ìš°ì„  ë°˜í™˜
        """
        pass
    
    def handle_references(
        self,
        symbol: str,
        uncommitted_files: Dict[str, str]
    ) -> List[Location]:
        """
        ì°¸ì¡° ê²€ìƒ‰:
        1. Base references
        2. Overlayì—ì„œ ì¶”ê°€ëœ references
        3. Merge í›„ ë°˜í™˜
        """
        pass
```

#### êµ¬í˜„ ìœ„ì¹˜
```
src/contexts/analysis_indexing/infrastructure/overlay/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ overlay_builder.py          # Uncommitted IR builder
â”œâ”€â”€ graph_merger.py             # Base + Overlay merge
â”œâ”€â”€ overlay_snapshot.py         # Overlay snapshot model
â””â”€â”€ conflict_resolver.py        # Symbol conflict resolution

src/contexts/retrieval_search/infrastructure/
â””â”€â”€ overlay_aware_retriever.py  # Overlay ê³ ë ¤í•œ ê²€ìƒ‰

server/mcp_server/
â””â”€â”€ overlay_lsp_handler.py      # LSP overlay support
```

#### ì„±ëŠ¥ ëª©í‘œ
- Overlay IR ìƒì„±: < 10ms (single file)
- Graph merge: < 50ms (typical overlay size)
- Query overhead: < 5% (overlay í¬í•¨ ì‹œ)

#### ê²€ì¦ ê¸°ì¤€
```python
# tests/test_overlay_integration.py

def test_overlay_definition():
    """Uncommitted ë³€ê²½ì´ ì •ì˜ ê²€ìƒ‰ì— ë°˜ì˜ë¨"""
    base = index_repo("test_repo")
    
    # íŒŒì¼ ìˆ˜ì • (ë¯¸ì»¤ë°‹)
    uncommitted = {
        "src/main.py": "def foo(): return 42"  # ì‹œê·¸ë‹ˆì²˜ ë³€ê²½
    }
    
    # Overlay ë°˜ì˜ëœ ê²°ê³¼
    result = find_definition("foo", overlay=uncommitted)
    assert result.signature == "() -> int"  # ìƒˆ ì‹œê·¸ë‹ˆì²˜
    
def test_overlay_call_graph():
    """Uncommitted ë³€ê²½ì´ call graphì— ë°˜ì˜ë¨"""
    # foo()ë¥¼ bar()ë¡œ rename (ë¯¸ì»¤ë°‹)
    uncommitted = {
        "src/main.py": "def bar(): pass\n\ndef caller(): bar()"
    }
    
    cg = get_call_graph(overlay=uncommitted)
    assert ("caller", "bar") in cg.edges
    assert ("caller", "foo") not in cg.edges  # ì‚­ì œë¨
```

---

### 1.2 Full Type Narrowing (TS/Python)
**Impact**: â­â­â­â­ (Call Graph precision +30%)  
**Difficulty**: â­â­â­â­ (Hard)  
**Priority**: P0  
**Status**: ğŸš§ ë¶€ë¶„ êµ¬í˜„ (ê¸°ë³¸ êµ¬ì¡°ë§Œ ìˆìŒ)

#### í•µì‹¬ ê°€ì¹˜
- Call Graph precision **30% í–¥ìƒ**
- TS ì–¸ì–´ ì„œë²„ ìˆ˜ì¤€ì˜ narrowing
- Sourcegraph: ì—†ìŒ
- CodeQL: ì¼ë¶€ë§Œ ìˆê³  ì •í™•ë„ ë¶€ì¡±

#### í˜„ì¬ ìƒíƒœ
```python
# src/contexts/code_foundation/infrastructure/analyzers/type_narrowing_full.py
# ê¸°ë³¸ êµ¬ì¡°ë§Œ ìˆìŒ (enum, dataclass ì •ì˜)
# ì‹¤ì œ ë¶„ì„ ë¡œì§ì€ ë¯¸ì™„ì„±
```

#### êµ¬í˜„ ë²”ìœ„

**Python:**
1. **isinstance narrowing**
```python
def process(x: Union[str, int]):
    if isinstance(x, str):
        # xëŠ” strë¡œ narrowing
        print(x.upper())
    else:
        # xëŠ” intë¡œ narrowing
        print(x + 1)
```

2. **None narrowing**
```python
def process(x: Optional[str]):
    if x is not None:
        # xëŠ” strë¡œ narrowing
        print(x.upper())
    
    if x is None:
        return
    # xëŠ” strë¡œ narrowing (early return)
    print(x.lower())
```

3. **Truthiness narrowing**
```python
def process(x: Optional[List[str]]):
    if x:
        # xëŠ” List[str]ë¡œ narrowing (not None, not empty)
        for item in x:
            print(item)
```

4. **Type guard functions**
```python
def is_admin(user: User) -> TypeGuard[AdminUser]:
    return user.role == "admin"

def process(user: User):
    if is_admin(user):
        # userëŠ” AdminUserë¡œ narrowing
        user.admin_action()
```

**TypeScript:**
1. **typeof narrowing**
```typescript
function process(x: string | number) {
    if (typeof x === "string") {
        // xëŠ” stringìœ¼ë¡œ narrowing
        console.log(x.toUpperCase());
    } else {
        // xëŠ” numberë¡œ narrowing
        console.log(x.toFixed(2));
    }
}
```

2. **Discriminated unions**
```typescript
type Result = 
    | { status: "success"; data: string }
    | { status: "error"; error: Error };

function handle(result: Result) {
    if (result.status === "success") {
        // resultëŠ” { status: "success"; data: string }ë¡œ narrowing
        console.log(result.data);
    } else {
        // resultëŠ” { status: "error"; error: Error }ë¡œ narrowing
        console.error(result.error);
    }
}
```

3. **instanceof narrowing**
```typescript
function process(x: Error | CustomError) {
    if (x instanceof CustomError) {
        // xëŠ” CustomErrorë¡œ narrowing
        x.customMethod();
    }
}
```

4. **Custom type guards**
```typescript
function isString(x: unknown): x is string {
    return typeof x === "string";
}

function process(x: unknown) {
    if (isString(x)) {
        // xëŠ” stringìœ¼ë¡œ narrowing
        console.log(x.toUpperCase());
    }
}
```

#### êµ¬í˜„ ì „ëµ

**Phase 1: CFG-based Type State Tracking** (1ì£¼)
```python
# src/contexts/code_foundation/infrastructure/analyzers/type_state_tracker.py

class TypeStateTracker:
    """Control Flow ê¸°ë°˜ íƒ€ì… ìƒíƒœ ì¶”ì """
    
    def analyze_function(self, func_ir: FunctionIR) -> TypeStateMap:
        """
        1. CFG ìƒì„±
        2. ê° basic blockì˜ entry/exit type state ê³„ì‚°
        3. Join pointsì—ì„œ type state merge
        """
        cfg = self.build_cfg(func_ir)
        type_states = {}
        
        for block in cfg.blocks:
            entry_state = self.compute_entry_state(block, type_states)
            exit_state = self.analyze_block(block, entry_state)
            type_states[block.id] = exit_state
        
        return type_states
    
    def narrow_type(
        self,
        var: str,
        condition: ast.expr,
        current_state: TypeState
    ) -> TypeState:
        """
        ì¡°ê±´ì‹ì—ì„œ íƒ€ì… narrowing:
        - isinstance(x, T) â†’ x: T
        - x is None â†’ x: None
        - typeof x === "string" â†’ x: string
        """
        pass
```

**Phase 2: Call Graph Precision Enhancement** (1ì£¼)
```python
# src/contexts/code_foundation/infrastructure/graphs/precise_call_graph.py

class PreciseCallGraphBuilder:
    """Type narrowing ê¸°ë°˜ ì •ë°€ call graph"""
    
    def resolve_call(
        self,
        call: CallIR,
        type_state: TypeState
    ) -> Set[str]:
        """
        íƒ€ì… ì •ë³´ ê¸°ë°˜ call target í•´ì„:
        
        ì˜ˆ:
        def process(handler: Handler):
            if isinstance(handler, FastHandler):
                handler.fast_process()  # FastHandler.fast_processë§Œ
            else:
                handler.slow_process()  # Handler.slow_process (not FastHandler)
        """
        receiver_type = type_state.get_type(call.receiver)
        
        if receiver_type.is_narrowed:
            # Narrowed typeìœ¼ë¡œë§Œ resolve
            return self.resolve_method(receiver_type, call.method_name)
        else:
            # ëª¨ë“  ê°€ëŠ¥í•œ íƒ€ì…ìœ¼ë¡œ resolve (ê¸°ì¡´ ë°©ì‹)
            return self.resolve_method_union(receiver_type, call.method_name)
```

**Phase 3: Integration with IR** (3ì¼)
```python
# src/contexts/code_foundation/infrastructure/ir/enhanced_ir_builder.py

class EnhancedIRBuilder:
    """Type narrowing ì •ë³´ë¥¼ IRì— í¬í•¨"""
    
    def build_function_ir(self, func_node: ast.FunctionDef) -> FunctionIR:
        """
        ê¸°ì¡´ IRì— type narrowing ì •ë³´ ì¶”ê°€:
        - ê° statementë§ˆë‹¤ type state ì €ì¥
        - Call siteë§ˆë‹¤ receiver type ì €ì¥
        """
        base_ir = self.build_base_ir(func_node)
        
        # Type narrowing ë¶„ì„
        type_states = self.type_state_tracker.analyze_function(base_ir)
        
        # IRì— type ì •ë³´ ì¶”ê°€
        for stmt in base_ir.statements:
            stmt.type_state = type_states.get(stmt.id)
        
        return base_ir
```

#### êµ¬í˜„ ìœ„ì¹˜
```
src/contexts/code_foundation/infrastructure/analyzers/
â”œâ”€â”€ type_narrowing_full.py      # âœ… ê¸°ì¡´ (í™•ì¥ í•„ìš”)
â”œâ”€â”€ type_state_tracker.py       # NEW: CFG ê¸°ë°˜ type state
â”œâ”€â”€ type_guard_detector.py      # NEW: Type guard í•¨ìˆ˜ ì¸ì‹
â””â”€â”€ union_resolver.py           # NEW: Union type í•´ì„

src/contexts/code_foundation/infrastructure/graphs/
â””â”€â”€ precise_call_graph.py       # NEW: Type ê¸°ë°˜ ì •ë°€ call graph
```

#### ì„±ëŠ¥ ëª©í‘œ
- Type narrowing overhead: < 15% (IR generation ëŒ€ë¹„)
- Call graph precision: +30%
- False positives: -40%

#### ê²€ì¦ ê¸°ì¤€
```python
def test_isinstance_narrowing():
    code = """
    def process(x: Union[str, int]):
        if isinstance(x, str):
            x.upper()  # str method
        else:
            x + 1      # int operation
    """
    
    ir = build_ir(code)
    cg = build_call_graph(ir)
    
    # str.upper í˜¸ì¶œë§Œ ìˆì–´ì•¼ í•¨ (int.upperëŠ” ì—†ìŒ)
    assert ("process", "str.upper") in cg.edges
    assert ("process", "int.upper") not in cg.edges

def test_discriminated_union():
    code = """
    type Result = { status: 'ok'; data: string } | { status: 'error'; msg: string }
    
    function handle(r: Result) {
        if (r.status === 'ok') {
            console.log(r.data);  // data fieldë§Œ
        } else {
            console.log(r.msg);   // msg fieldë§Œ
        }
    }
    """
    
    ir = build_ir(code)
    # r.data, r.msg ëª¨ë‘ ì •í™•íˆ ì¸ì‹ë˜ì–´ì•¼ í•¨
```

---

### 1.3 Context-Sensitive Call Graph
**Impact**: â­â­â­â­â­ (ì •í™•ë„ ëŒ€í­ í–¥ìƒ)  
**Difficulty**: â­â­â­â­â­ (Very Hard)  
**Priority**: P0  
**Status**: ğŸš§ í˜„ì¬ context-insensitive

#### í•µì‹¬ ê°€ì¹˜
- Impact Analysis ì •í™•ë„ ì¦ê°€
- Dataflow/Controlflow ì •í™•ë„ ì¦ê°€
- Refactoring ì œì•ˆ ì •í™•ë„ ì¦ê°€
- Sourcegraph: ì—†ìŒ
- CodeQL: ì œí•œì 
- **êµ¬í˜„í•˜ë©´ ì„¸ê³„ ìµœê³ ê¸‰**

#### í˜„ì¬ vs ëª©í‘œ

**í˜„ì¬ (Context-Insensitive):**
```javascript
function run(flag) {
    if (flag) a(); 
    else b();
}

run(true);   // Case 1
run(false);  // Case 2

// í˜„ì¬ call graph (ë¶€ì •í™•):
run â†’ a
run â†’ b
```

**ëª©í‘œ (Context-Sensitive):**
```javascript
// ì •í™•í•œ call graph:
run(flag=true)  â†’ a    (Case 1ë§Œ)
run(flag=false) â†’ b    (Case 2ë§Œ)

// Contextë¡œ êµ¬ë¶„:
Call Site 1: run(true) â†’ a
Call Site 2: run(false) â†’ b
```

#### êµ¬í˜„ ì „ëµ

**Phase 1: Call Context Modeling** (1ì£¼)
```python
# src/contexts/code_foundation/infrastructure/graphs/call_context.py

@dataclass
class CallContext:
    """í˜¸ì¶œ ì»¨í…ìŠ¤íŠ¸"""
    call_site: str           # "main.py:15:4"
    caller_context: Optional['CallContext']  # Recursive
    argument_values: Dict[str, Any]  # ì¸ì ê°’ (ìƒìˆ˜ë§Œ)
    
    def context_id(self) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê³ ìœ  ID"""
        return f"{self.call_site}#{hash(self.argument_values)}"

class ContextSensitiveCallGraph:
    """Context-sensitive call graph"""
    
    def __init__(self):
        # (caller_context, callee) â†’ Set[CallContext]
        self.edges: Dict[Tuple[str, str], Set[CallContext]] = {}
    
    def add_edge(
        self,
        caller: str,
        callee: str,
        context: CallContext
    ):
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ edge ì¶”ê°€"""
        key = (caller, callee)
        if key not in self.edges:
            self.edges[key] = set()
        self.edges[key].add(context)
    
    def get_reachable(
        self,
        start: str,
        context: CallContext
    ) -> Set[Tuple[str, CallContext]]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ reachability"""
        visited = set()
        queue = [(start, context)]
        
        while queue:
            current, ctx = queue.pop(0)
            if (current, ctx) in visited:
                continue
            visited.add((current, ctx))
            
            # í˜„ì¬ contextì—ì„œ í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜ë“¤
            for callee, call_contexts in self.get_callees(current):
                for call_ctx in call_contexts:
                    if call_ctx.matches(ctx):
                        queue.append((callee, call_ctx))
        
        return visited
```

**Phase 2: Argument Value Tracking** (1ì£¼)
```python
# src/contexts/code_foundation/infrastructure/analyzers/value_tracker.py

class ArgumentValueTracker:
    """ì¸ì ê°’ ì¶”ì  (ìƒìˆ˜ ì „íŒŒ)"""
    
    def track_call(
        self,
        call_site: CallIR,
        caller_context: CallContext
    ) -> Dict[str, Any]:
        """
        í˜¸ì¶œ ì‹œì ì˜ ì¸ì ê°’ ì¶”ì :
        - ìƒìˆ˜: ê·¸ëŒ€ë¡œ ì¶”ì 
        - ë³€ìˆ˜: ê°’ ì „íŒŒ (ê°€ëŠ¥í•˜ë©´)
        - ë³µì¡í•œ ì‹: Unknown
        """
        arg_values = {}
        
        for param_name, arg_expr in call_site.arguments.items():
            if isinstance(arg_expr, ast.Constant):
                # ìƒìˆ˜ â†’ ì§ì ‘ ì‚¬ìš©
                arg_values[param_name] = arg_expr.value
            elif isinstance(arg_expr, ast.Name):
                # ë³€ìˆ˜ â†’ ê°’ ì „íŒŒ
                value = self.resolve_variable(arg_expr.id, caller_context)
                if value is not Unknown:
                    arg_values[param_name] = value
            # else: ë³µì¡í•œ ì‹ â†’ ì¶”ì  ì•ˆ í•¨
        
        return arg_values
```

**Phase 3: Context-Sensitive Analysis** (2ì£¼)
```python
# src/contexts/code_foundation/infrastructure/analyzers/context_sensitive_analyzer.py

class ContextSensitiveAnalyzer:
    """Context-sensitive ë¶„ì„"""
    
    def analyze_repository(
        self,
        repo_ir: RepositoryIR,
        max_depth: int = 5  # Call depth ì œí•œ
    ) -> ContextSensitiveCallGraph:
        """
        ì „ì²´ repositoryë¥¼ context-sensitiveí•˜ê²Œ ë¶„ì„
        
        ì•Œê³ ë¦¬ì¦˜:
        1. Entry points (main, public APIs) ì°¾ê¸°
        2. ê° entry pointì—ì„œ BFS/DFS
        3. ê° call siteë§ˆë‹¤ context ìƒì„±
        4. Contextë³„ë¡œ callee ë¶„ì„ (ì¬ê·€)
        """
        cscg = ContextSensitiveCallGraph()
        
        entry_points = self.find_entry_points(repo_ir)
        
        for entry in entry_points:
            root_context = CallContext(
                call_site="<entry>",
                caller_context=None,
                argument_values={}
            )
            self.analyze_function(entry, root_context, cscg, depth=0, max_depth=max_depth)
        
        return cscg
    
    def analyze_function(
        self,
        func: FunctionIR,
        context: CallContext,
        cscg: ContextSensitiveCallGraph,
        depth: int,
        max_depth: int
    ):
        """í•¨ìˆ˜ë¥¼ íŠ¹ì • contextì—ì„œ ë¶„ì„"""
        if depth > max_depth:
            return  # Depth ì œí•œ
        
        # ì¸ì ê°’ìœ¼ë¡œ type narrowing
        type_state = self.narrow_by_arguments(func, context.argument_values)
        
        for call in func.calls:
            # Call target í•´ì„ (type state ê¸°ë°˜)
            targets = self.resolve_call(call, type_state)
            
            for target in targets:
                # Call context ìƒì„±
                call_context = CallContext(
                    call_site=call.location,
                    caller_context=context,
                    argument_values=self.value_tracker.track_call(call, context)
                )
                
                # Edge ì¶”ê°€
                cscg.add_edge(func.symbol, target.symbol, call_context)
                
                # Recursive analysis
                self.analyze_function(target, call_context, cscg, depth + 1, max_depth)
```

**Phase 4: Impact Analysis Enhancement** (3ì¼)
```python
# src/contexts/analysis_indexing/infrastructure/impact_analyzer.py

class ContextAwareImpactAnalyzer:
    """Context-aware impact analysis"""
    
    def analyze_impact(
        self,
        changed_symbol: str,
        change_type: ChangeType,
        cscg: ContextSensitiveCallGraph
    ) -> ImpactReport:
        """
        Contextë¥¼ ê³ ë ¤í•œ ì˜í–¥ ë¶„ì„:
        
        ì˜ˆ:
        def calc(mode):
            if mode == "fast":
                return fast_calc()
            else:
                return slow_calc()
        
        fast_calc() ë³€ê²½ ì‹œ:
        - calc(mode="fast")ë§Œ ì˜í–¥ë°›ìŒ
        - calc(mode="slow")ëŠ” ì˜í–¥ë°›ì§€ ì•ŠìŒ
        """
        impact = ImpactReport()
        
        # ëª¨ë“  caller ì°¾ê¸°
        for (caller, callee), contexts in cscg.edges.items():
            if callee == changed_symbol:
                for ctx in contexts:
                    # Contextë³„ë¡œ ì˜í–¥ í‰ê°€
                    if self.is_affected(change_type, ctx):
                        impact.add_affected_call(caller, ctx)
        
        return impact
```

#### êµ¬í˜„ ìœ„ì¹˜
```
src/contexts/code_foundation/infrastructure/graphs/
â”œâ”€â”€ call_context.py             # NEW: Call context model
â””â”€â”€ context_sensitive_cg.py     # NEW: Context-sensitive call graph

src/contexts/code_foundation/infrastructure/analyzers/
â”œâ”€â”€ value_tracker.py            # NEW: Argument value tracking
â””â”€â”€ context_sensitive_analyzer.py  # NEW: Main analyzer

src/contexts/analysis_indexing/infrastructure/
â””â”€â”€ context_aware_impact.py     # NEW: Context-aware impact analysis
```

#### ì„±ëŠ¥ ëª©í‘œ
- Analysis time: < 2x of context-insensitive (acceptable tradeoff)
- Max call depth: 5 (configurable)
- Precision improvement: +40% (vs context-insensitive)
- False positives: -50%

#### ê²€ì¦ ê¸°ì¤€
```python
def test_context_sensitive_call():
    code = """
    def process(flag):
        if flag:
            fast()
        else:
            slow()
    
    process(True)   # Call site 1
    process(False)  # Call site 2
    """
    
    cscg = build_context_sensitive_cg(code)
    
    # Call site 1: process(True) â†’ fast()ë§Œ
    ctx1 = CallContext(call_site="line:8", argument_values={"flag": True})
    reachable1 = cscg.get_reachable("process", ctx1)
    assert ("fast", ctx1) in reachable1
    assert ("slow", ctx1) not in reachable1
    
    # Call site 2: process(False) â†’ slow()ë§Œ
    ctx2 = CallContext(call_site="line:9", argument_values={"flag": False})
    reachable2 = cscg.get_reachable("process", ctx2)
    assert ("slow", ctx2) in reachable2
    assert ("fast", ctx2) not in reachable2
```

---

### 1.4 Semantic Region Index (SRI)
**Impact**: â­â­â­â­â­ (LLM Augmentationì—ì„œ ì••ë„ì )  
**Difficulty**: â­â­â­â­ (Hard)  
**Priority**: P0  
**Status**: ğŸš§ TODO (ì‹ ê·œ ê¸°ëŠ¥)

#### í•µì‹¬ ê°€ì¹˜
- LLM ê¸°ë°˜ IDEì—ì„œ **ë§¤ìš° ì¤‘ìš”**í•œ ê¸°ëŠ¥
- File-level/Symbol-levelì„ ë„˜ì–´ **Region-level ì¸ë±ì‹±**
- Sourcegraph, CodeQL: ëª¨ë‘ ì§€ì› ì•ˆ í•¨
- **êµ¬í˜„í•˜ë©´ LLM Augmentation ì°¨ë³„í™”**

#### Regionì´ë€?

íŒŒì¼ì„ ì˜ë¯¸ì ìœ¼ë¡œ ì˜ê²Œ ë‚˜ëˆˆ ë‹¨ìœ„:
```python
# Region 1: Authentication Setup (lines 10-25)
def setup_auth(config):
    """ì¸ì¦ ì„¤ì • ì´ˆê¸°í™”"""
    auth = AuthProvider(config)
    auth.configure()
    return auth

# Region 2: User Validation (lines 27-45)
def validate_user(user):
    """ì‚¬ìš©ì ê²€ì¦ ë¡œì§"""
    if not user.email:
        raise ValidationError()
    if not check_permission(user):
        raise PermissionError()
    return True

# Region 3: Main Handler (lines 47-80)
def handle_request(request):
    """ìš”ì²­ ì²˜ë¦¬ ë©”ì¸ íë¦„"""
    user = extract_user(request)
    if validate_user(user):
        return process_request(request)
    return error_response()
```

ê° Regionì€:
- **Functionality**: ë¬´ì—‡ì„ í•˜ëŠ”ê°€?
- **Type Flow**: ì–´ë–¤ íƒ€ì…ë“¤ì´ íë¥´ëŠ”ê°€?
- **Responsibility**: ëˆ„êµ¬ì˜ ì±…ì„ì¸ê°€?
- **Control Flow**: ì–´ë–¤ íë¦„ì¸ê°€?
- **Semantic Tags**: ì–´ë–¤ ê°œë…ê³¼ ì—°ê´€ë˜ëŠ”ê°€?

#### êµ¬í˜„ ì „ëµ

**Phase 1: Region Segmentation** (1ì£¼)
```python
# src/contexts/code_foundation/infrastructure/region/segmenter.py

@dataclass
class CodeRegion:
    """ì½”ë“œ region"""
    id: str
    file_path: str
    start_line: int
    end_line: int
    
    # Semantic info
    functionality: str         # "Authentication setup"
    responsibility: str        # "Initialize auth provider"
    control_flow_type: str     # "setup", "validation", "handler"
    
    # Symbols in region
    symbols: Set[str]          # Functions, classes defined
    references: Set[str]       # External symbols referenced
    
    # Type flow
    input_types: Set[str]      # Types flowing in
    output_types: Set[str]     # Types flowing out
    
    # Semantic tags
    tags: Set[str]             # ["auth", "validation", "security"]

class RegionSegmenter:
    """íŒŒì¼ì„ ì˜ë¯¸ì  regionìœ¼ë¡œ ë¶„í• """
    
    def segment_file(self, file_ir: FileIR) -> List[CodeRegion]:
        """
        íŒŒì¼ì„ regionìœ¼ë¡œ ë¶„í• :
        
        ì „ëµ:
        1. Top-level symbols (í•¨ìˆ˜, í´ë˜ìŠ¤) ê¸°ì¤€ ë¶„í• 
        2. ì—°ê´€ëœ helpersëŠ” ê°™ì€ regionìœ¼ë¡œ ë¬¶ìŒ
        3. Comments, docstringsë¡œ region ê²½ê³„ íŒíŠ¸
        """
        regions = []
        
        # AST ìˆœíšŒí•˜ë©° region í›„ë³´ ì°¾ê¸°
        for node in file_ir.ast.body:
            if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                region = self.create_region_for_symbol(node, file_ir)
                regions.append(region)
        
        # Region ë³‘í•© (ê´€ë ¨ëœ ê²ƒë“¤ë¼ë¦¬)
        regions = self.merge_related_regions(regions, file_ir)
        
        return regions
```

**Phase 2: LLM-based Region Annotation** (1ì£¼)
```python
# src/contexts/code_foundation/infrastructure/region/annotator.py

class RegionAnnotator:
    """LLMìœ¼ë¡œ region ì˜ë¯¸ ì¶”ì¶œ"""
    
    async def annotate_region(
        self,
        region: CodeRegion,
        code: str
    ) -> AnnotatedRegion:
        """
        LLMì—ê²Œ region ë¶„ì„ ìš”ì²­:
        
        Prompt:
        "ë‹¤ìŒ ì½”ë“œ regionì„ ë¶„ì„í•˜ì„¸ìš”:
        1. Functionality (í•œ ë¬¸ì¥)
        2. Responsibility (í•œ ë¬¸ì¥)
        3. Semantic tags (5ê°œ ì´í•˜ í‚¤ì›Œë“œ)
        4. Control flow type (setup/validation/handler/...)"
        """
        prompt = self.build_prompt(region, code)
        response = await self.llm_client.complete(prompt)
        
        return AnnotatedRegion(
            **region.__dict__,
            functionality=response.functionality,
            responsibility=response.responsibility,
            tags=response.tags,
            control_flow_type=response.control_flow_type
        )
```

**Phase 3: Region Index** (1ì£¼)
```python
# src/contexts/multi_index/infrastructure/region_index.py

class RegionIndex:
    """Region semantic index"""
    
    def __init__(self, vector_store, tantivy):
        self.vector_store = vector_store  # Qdrant
        self.keyword_index = tantivy
    
    async def index_region(self, region: AnnotatedRegion):
        """
        Region ì¸ë±ì‹±:
        1. Vector embedding (functionality + tags)
        2. Keyword index (symbols + tags)
        3. Graph index (region dependencies)
        """
        # Vector embedding
        embedding_text = f"{region.functionality} {' '.join(region.tags)}"
        embedding = await self.embed(embedding_text)
        
        await self.vector_store.upsert(
            collection="regions",
            points=[{
                "id": region.id,
                "vector": embedding,
                "payload": {
                    "file": region.file_path,
                    "lines": (region.start_line, region.end_line),
                    "functionality": region.functionality,
                    "tags": list(region.tags),
                    "symbols": list(region.symbols)
                }
            }]
        )
        
        # Keyword index
        self.keyword_index.add_document(
            doc_id=region.id,
            fields={
                "functionality": region.functionality,
                "tags": " ".join(region.tags),
                "symbols": " ".join(region.symbols)
            }
        )
    
    async def search_regions(
        self,
        query: str,
        tags: Optional[Set[str]] = None
    ) -> List[CodeRegion]:
        """
        Region ê²€ìƒ‰:
        - Semantic search (vector)
        - Tag filter
        - Symbol filter
        """
        # Vector search
        query_embedding = await self.embed(query)
        vector_results = await self.vector_store.search(
            collection="regions",
            query_vector=query_embedding,
            limit=20
        )
        
        # Tag filter (if provided)
        if tags:
            vector_results = [
                r for r in vector_results
                if tags & set(r.payload["tags"])
            ]
        
        return vector_results
```

**Phase 4: Retrieval Integration** (3ì¼)
```python
# src/contexts/retrieval_search/infrastructure/region_aware_retriever.py

class RegionAwareRetriever:
    """Regionì„ ê³ ë ¤í•œ retrieval"""
    
    async def retrieve(
        self,
        query: str,
        repo_id: str,
        retrieval_mode: str = "auto"
    ) -> List[RetrievalResult]:
        """
        Query ì˜ë„ì— ë”°ë¼ retrieval strategy ì„ íƒ:
        
        1. "ì´ API ì–´ë””ì„œ í˜¸ì¶œ?" â†’ Symbol-level (call graph)
        2. "ì¸ì¦ ë¡œì§ ì„¤ëª…í•´ì¤˜" â†’ Region-level (SRI)
        3. "í• ì¸ ê³„ì‚° ì–´ë–»ê²Œ ë™ì‘?" â†’ Region-level + Dataflow
        """
        # Query ì˜ë„ ë¶„ë¥˜
        intent = await self.classify_intent(query)
        
        if intent == "call_reference":
            # Symbol-level ê²€ìƒ‰
            return await self.symbol_retriever.search(query)
        
        elif intent == "explanation":
            # Region-level ê²€ìƒ‰
            return await self.region_index.search_regions(query)
        
        elif intent == "dataflow":
            # Region + Dataflow ê²°í•©
            regions = await self.region_index.search_regions(query)
            dataflow = await self.dataflow_analyzer.analyze_regions(regions)
            return self.merge_results(regions, dataflow)
```

#### êµ¬í˜„ ìœ„ì¹˜
```
src/contexts/code_foundation/infrastructure/region/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ segmenter.py               # NEW: Region ë¶„í• 
â”œâ”€â”€ annotator.py               # NEW: LLM ê¸°ë°˜ annotation
â””â”€â”€ models.py                  # NEW: Region models

src/contexts/multi_index/infrastructure/
â””â”€â”€ region_index.py            # NEW: Region indexing

src/contexts/retrieval_search/infrastructure/
â””â”€â”€ region_aware_retriever.py  # NEW: Region ê¸°ë°˜ retrieval
```

#### ì„±ëŠ¥ ëª©í‘œ
- Region segmentation: < 100ms per file
- LLM annotation: < 2s per region (async batch)
- Region search: < 100ms
- Index size: ~10KB per region

#### ê²€ì¦ ê¸°ì¤€
```python
def test_region_segmentation():
    code = """
    def setup_auth(config):
        '''ì¸ì¦ ì„¤ì •'''
        pass
    
    def validate_user(user):
        '''ì‚¬ìš©ì ê²€ì¦'''
        pass
    """
    
    regions = segment_file(code)
    assert len(regions) == 2
    assert regions[0].functionality.lower() == "authentication setup"
    assert "auth" in regions[0].tags
    assert "validation" in regions[1].tags

def test_region_search():
    # "ì¸ì¦ ê´€ë ¨ ì½”ë“œ ì°¾ì•„ì¤˜"
    results = search_regions(query="authentication logic", tags={"auth"})
    assert len(results) > 0
    assert "auth" in results[0].tags
    assert "setup_auth" in results[0].symbols
```

---

## ğŸš€ P1: ì°¨ì„¸ëŒ€ ê¸°ëŠ¥ (ì—…ê³„ê°€ ì•„ì§ ëª»í•¨)

### 2.1 Impact-Based Partial Graph Rebuild
**Impact**: â­â­â­â­ (ì„±ëŠ¥ ìµœì í™”)  
**Difficulty**: â­â­â­â­ (Hard)  
**Priority**: P1  
**Status**: ğŸš§ TODO (Incremental Update í™•ì¥)

#### í•µì‹¬ ê°€ì¹˜
- Incremental Updateë¥¼ **ë”ìš± ì§€ëŠ¥ì ìœ¼ë¡œ**
- **Impact levelì— ë”°ë¼ rebuild depth ìë™ ìµœì í™”**
- í˜„ì¬ incrementalë³´ë‹¤ **2-5x ë” ë¹ ë¦„**

#### í˜„ì¬ vs ëª©í‘œ

**í˜„ì¬ (Incremental):**
```python
# íŒŒì¼ ë³€ê²½ â†’ í•´ë‹¹ íŒŒì¼ + ì˜ì¡´ íŒŒì¼ ì „ì²´ rebuild
def incremental_update(changed_file):
    affected = get_affected_files(changed_file)  # ì˜ì¡´ íŒŒì¼ ëª¨ë‘
    for file in affected:
        rebuild_ir(file)        # ì „ì²´ rebuild
        rebuild_graph(file)     # ì „ì²´ rebuild
```

**ëª©í‘œ (Impact-Based Partial):**
```python
# ë³€ê²½ íƒ€ì…ì— ë”°ë¼ ìµœì†Œí•œë§Œ rebuild
def impact_based_update(changed_file, change_type):
    if change_type == "signature_change":
        # Signature ë³€ê²½ â†’ callersë§Œ rebuild
        callers = get_direct_callers(changed_file)
        for caller in callers:
            rebuild_call_edges(caller)  # Call edgesë§Œ
    
    elif change_type == "body_change":
        # Body ë³€ê²½ â†’ CFG/DFGë§Œ update
        rebuild_cfg_dfg(changed_file)  # í•´ë‹¹ íŒŒì¼ë§Œ
    
    elif change_type == "comment_change":
        # Comment ë³€ê²½ â†’ Nothing (skip)
        pass
```

#### êµ¬í˜„ ì „ëµ

**Phase 1: Change Impact Classifier** (1ì£¼)
```python
# src/contexts/analysis_indexing/infrastructure/impact/change_classifier.py

class ChangeImpactLevel(Enum):
    """ë³€ê²½ ì˜í–¥ë„"""
    NONE = 0           # Comment, whitespace
    LOCAL = 1          # Function body ë‚´ë¶€
    SIGNATURE = 2      # Function signature
    INTERFACE = 3      # Class interface
    GLOBAL = 4         # Module exports

class ChangeImpactClassifier:
    """ë³€ê²½ì˜ ì˜í–¥ë„ ë¶„ë¥˜"""
    
    def classify_change(
        self,
        file_path: str,
        old_content: str,
        new_content: str
    ) -> ChangeImpactLevel:
        """
        ë³€ê²½ ë¶„ì„:
        1. AST diff
        2. Signature diff
        3. Export diff
        4. Impact level ê²°ì •
        """
        old_ast = parse(old_content)
        new_ast = parse(new_content)
        
        ast_diff = self.compute_ast_diff(old_ast, new_ast)
        
        if ast_diff.is_comment_only:
            return ChangeImpactLevel.NONE
        
        if ast_diff.has_signature_change:
            return ChangeImpactLevel.SIGNATURE
        
        if ast_diff.has_export_change:
            return ChangeImpactLevel.GLOBAL
        
        if ast_diff.has_body_change_only:
            return ChangeImpactLevel.LOCAL
        
        return ChangeImpactLevel.INTERFACE
```

**Phase 2: Partial Rebuild Strategies** (2ì£¼)
```python
# src/contexts/analysis_indexing/infrastructure/impact/partial_rebuilder.py

class PartialRebuilder:
    """ì˜í–¥ë„ ê¸°ë°˜ ë¶€ë¶„ rebuild"""
    
    def rebuild_by_impact(
        self,
        changed_file: str,
        impact_level: ChangeImpactLevel,
        old_ir: FileIR,
        new_ir: FileIR
    ):
        """ì˜í–¥ë„ì— ë§ëŠ” rebuild ì „ëµ"""
        
        if impact_level == ChangeImpactLevel.NONE:
            # Nothing to do
            logger.info("skip_rebuild", reason="comment_only")
            return
        
        elif impact_level == ChangeImpactLevel.LOCAL:
            # CFG/DFGë§Œ rebuild
            self.rebuild_local_graphs(changed_file, new_ir)
        
        elif impact_level == ChangeImpactLevel.SIGNATURE:
            # Callersì˜ call edgesë§Œ update
            callers = self.find_direct_callers(changed_file, old_ir)
            for caller in callers:
                self.update_call_edges(caller, old_ir, new_ir)
        
        elif impact_level == ChangeImpactLevel.GLOBAL:
            # Full rebuild (current incrementalê³¼ ë™ì¼)
            self.rebuild_full(changed_file, new_ir)
    
    def rebuild_local_graphs(self, file: str, ir: FileIR):
        """Local graphsë§Œ rebuild (CFG, DFG)"""
        for func in ir.functions:
            # CFG
            cfg = self.cfg_builder.build(func)
            self.graph_store.update_cfg(func.symbol, cfg)
            
            # DFG
            dfg = self.dfg_builder.build(func)
            self.graph_store.update_dfg(func.symbol, dfg)
    
    def update_call_edges(
        self,
        caller_file: str,
        old_callee_ir: FileIR,
        new_callee_ir: FileIR
    ):
        """Call edgesë§Œ update"""
        # Old signature â†’ new signature mapping
        signature_changes = self.compute_signature_changes(old_callee_ir, new_callee_ir)
        
        # Callerì˜ call sites update
        caller_ir = self.load_ir(caller_file)
        for call in caller_ir.calls:
            if call.target in signature_changes:
                new_signature = signature_changes[call.target]
                self.graph_store.update_call_edge(call.id, new_signature)
```

**Phase 3: Integration** (3ì¼)
```python
# src/contexts/analysis_indexing/infrastructure/orchestrator_v2/impact_based_orchestrator.py

class ImpactBasedOrchestrator:
    """Impact-based incremental orchestrator"""
    
    async def handle_file_change(
        self,
        repo_id: str,
        changed_file: str
    ):
        """
        1. ë³€ê²½ ê°ì§€
        2. Impact level ë¶„ë¥˜
        3. Partial rebuild ì‹¤í–‰
        """
        # Old content ë¡œë“œ
        old_content = await self.load_old_content(repo_id, changed_file)
        new_content = await self.load_file(changed_file)
        
        # Impact classification
        impact_level = self.classifier.classify_change(
            changed_file,
            old_content,
            new_content
        )
        
        logger.info(
            "change_detected",
            file=changed_file,
            impact_level=impact_level.name
        )
        
        # Partial rebuild
        old_ir = await self.load_ir(repo_id, changed_file)
        new_ir = await self.ir_builder.build(new_content)
        
        await self.partial_rebuilder.rebuild_by_impact(
            changed_file,
            impact_level,
            old_ir,
            new_ir
        )
```

#### êµ¬í˜„ ìœ„ì¹˜
```
src/contexts/analysis_indexing/infrastructure/impact/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ change_classifier.py       # NEW: ë³€ê²½ ì˜í–¥ë„ ë¶„ë¥˜
â”œâ”€â”€ partial_rebuilder.py       # NEW: ë¶€ë¶„ rebuild ì „ëµ
â””â”€â”€ ast_diff.py                # NEW: AST diff ê³„ì‚°
```

#### ì„±ëŠ¥ ëª©í‘œ
- Comment change: 0ms (skip)
- Local change: < 5ms (vs 50ms full rebuild)
- Signature change: < 20ms (vs 200ms affected files rebuild)
- Overall speedup: 2-5x over current incremental

---

### 2.2 Speculative Graph Execution
**Impact**: â­â­â­â­â­ (AI Agent ì°¨ë³„í™”)  
**Difficulty**: â­â­â­â­â­ (Very Hard)  
**Priority**: P1  
**Status**: ğŸš§ TODO (ì‹ ê·œ ê¸°ëŠ¥)

#### í•µì‹¬ ê°€ì¹˜
- AI Agentê°€ **ì½”ë“œ ë³€ê²½ì„ ì œì•ˆí•˜ê¸° ì „ì—**
- **ë³€ê²½ í›„ ê·¸ë˜í”„ë¥¼ ë¯¸ë¦¬ ê³„ì‚°**
- "What-if" ë¶„ì„ ê°€ëŠ¥
- **ì§„ì§œ ì°¨ì„¸ëŒ€ IDE ê¸°ëŠ¥**

#### ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

**Scenario 1: Rename Impact Preview**
```python
# Agent: "ì´ í•¨ìˆ˜ ì´ë¦„ì„ ë³€ê²½í•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œ?"
preview = speculate_rename("old_func", "new_func")

print(preview.affected_files)      # ì˜í–¥ë°›ëŠ” íŒŒì¼ ëª©ë¡
print(preview.call_graph_changes)  # Call graph ë³€í™”
print(preview.breaking_changes)    # Breaking changes ëª©ë¡
print(preview.test_impact)         # ì˜í–¥ë°›ëŠ” í…ŒìŠ¤íŠ¸

# Agentê°€ previewë¥¼ ë³´ê³  ì•ˆì „ì„± íŒë‹¨ í›„ ì‹¤í–‰
```

**Scenario 2: Refactoring Simulation**
```python
# Agent: "ì´ ì½”ë“œë¥¼ ë‹¤ë¥¸ íŒŒì¼ë¡œ ì˜®ê¸°ë©´?"
patch = generate_move_patch("src/utils.py", "src/core/utils.py")

preview = speculate_apply(patch)

print(preview.import_changes)      # Import êµ¬ì¡° ë³€í™”
print(preview.dependency_graph)    # ì˜ì¡´ì„± ê·¸ë˜í”„ ë³€í™”
print(preview.circular_deps)       # ìˆœí™˜ ì˜ì¡´ì„± ë°œìƒ ì—¬ë¶€
```

**Scenario 3: Parameter Addition**
```python
# Agent: "í•¨ìˆ˜ì— íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ë©´?"
preview = speculate_add_parameter(
    func="process_user",
    param="role: str",
    default_value="'guest'"
)

print(preview.all_call_sites)      # ëª¨ë“  í˜¸ì¶œ ì§€ì 
print(preview.need_update)         # ì—…ë°ì´íŠ¸ í•„ìš”í•œ í˜¸ì¶œë“¤
print(preview.safe_with_default)   # Defaultë¡œ ì•ˆì „í•œì§€
```

#### êµ¬í˜„ ì „ëµ

**Phase 1: Virtual IR Layer** (2ì£¼)
```python
# src/contexts/code_foundation/infrastructure/speculative/virtual_ir.py

class VirtualIR:
    """ê°€ìƒ IR (ì‹¤ì œ ì ìš© ì•ˆ ë¨)"""
    
    def __init__(self, base_ir: RepositoryIR):
        self.base = base_ir
        self.overlays: List[IRPatch] = []
    
    def apply_patch(self, patch: IRPatch) -> 'VirtualIR':
        """
        Patchë¥¼ ê°€ìƒìœ¼ë¡œ ì ìš© (immutable):
        - Rename
        - Move
        - Add/Remove parameter
        - Change signature
        """
        new_virtual = VirtualIR(self.base)
        new_virtual.overlays = self.overlays + [patch]
        return new_virtual
    
    def get_symbol(self, symbol_id: str) -> Optional[Symbol]:
        """
        Symbol ì¡°íšŒ (overlay ìš°ì„ ):
        1. Overlayì—ì„œ ì°¾ê¸°
        2. ì—†ìœ¼ë©´ baseì—ì„œ ì°¾ê¸°
        """
        for patch in reversed(self.overlays):
            if symbol_id in patch.changes:
                return patch.changes[symbol_id]
        
        return self.base.get_symbol(symbol_id)
    
    def compute_call_graph(self) -> CallGraph:
        """ê°€ìƒ IRì—ì„œ call graph ê³„ì‚°"""
        cg = CallGraph()
        
        # ëª¨ë“  symbols (base + overlays)
        all_symbols = self.get_all_symbols()
        
        for symbol in all_symbols:
            for call in symbol.calls:
                target = self.resolve_call(call)  # Virtual resolution
                cg.add_edge(symbol.id, target)
        
        return cg

@dataclass
class IRPatch:
    """IR ë³€ê²½ patch"""
    patch_type: str  # "rename", "move", "add_param", ...
    changes: Dict[str, Symbol]  # symbol_id -> new symbol
    metadata: Dict[str, Any]
```

**Phase 2: Speculative Analyzer** (2ì£¼)
```python
# src/contexts/analysis_indexing/infrastructure/speculative/speculative_analyzer.py

class SpeculativeAnalyzer:
    """Speculative ë¶„ì„"""
    
    def speculate_rename(
        self,
        repo_id: str,
        old_name: str,
        new_name: str
    ) -> SpeculativeResult:
        """
        Rename ì‹œë®¬ë ˆì´ì…˜:
        1. Base IR ë¡œë“œ
        2. Rename patch ìƒì„±
        3. Virtual IR ìƒì„±
        4. Virtual graph ê³„ì‚°
        5. Diff ê³„ì‚°
        """
        base_ir = self.load_ir(repo_id)
        
        # Rename patch
        patch = self.create_rename_patch(old_name, new_name, base_ir)
        
        # Virtual IR
        virtual_ir = VirtualIR(base_ir).apply_patch(patch)
        
        # Compute graphs
        base_cg = base_ir.call_graph
        virtual_cg = virtual_ir.compute_call_graph()
        
        # Compute diff
        cg_diff = self.compute_graph_diff(base_cg, virtual_cg)
        
        return SpeculativeResult(
            patch=patch,
            affected_files=patch.affected_files,
            call_graph_diff=cg_diff,
            breaking_changes=self.detect_breaking_changes(cg_diff),
            test_impact=self.compute_test_impact(cg_diff)
        )
    
    def speculate_apply_patch(
        self,
        repo_id: str,
        code_patch: str  # Git diff format
    ) -> SpeculativeResult:
        """
        ì„ì˜ì˜ code patch ì‹œë®¬ë ˆì´ì…˜:
        1. Patch íŒŒì‹±
        2. Affected files íŒŒì‹±
        3. Virtual IR ìƒì„±
        4. Graphs ê³„ì‚°
        5. Impact ë¶„ì„
        """
        base_ir = self.load_ir(repo_id)
        
        # Parse patch
        parsed_patch = self.parse_git_patch(code_patch)
        
        # Build virtual IR
        virtual_ir = VirtualIR(base_ir)
        for file_change in parsed_patch.changes:
            file_patch = self.build_file_patch(file_change)
            virtual_ir = virtual_ir.apply_patch(file_patch)
        
        # Compute all graphs
        virtual_cg = virtual_ir.compute_call_graph()
        virtual_imports = virtual_ir.compute_import_graph()
        virtual_deps = virtual_ir.compute_dependency_graph()
        
        # Detect issues
        circular_deps = self.detect_circular_dependencies(virtual_deps)
        breaking_changes = self.detect_breaking_changes(virtual_cg)
        
        return SpeculativeResult(
            call_graph=virtual_cg,
            import_graph=virtual_imports,
            dependency_graph=virtual_deps,
            circular_dependencies=circular_deps,
            breaking_changes=breaking_changes
        )
```

**Phase 3: Agent Integration** (1ì£¼)
```python
# src/contexts/agent_automation/infrastructure/tools/speculative_tool.py

class SpeculativeTool:
    """Agentì—ì„œ ì‚¬ìš©í•˜ëŠ” speculative tool"""
    
    async def preview_refactor(
        self,
        refactor_type: str,  # "rename", "move", "extract", ...
        **kwargs
    ) -> Dict[str, Any]:
        """
        Agentê°€ refactorë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— preview:
        
        ì˜ˆ:
        preview = await tool.preview_refactor(
            refactor_type="rename",
            old_name="process",
            new_name="process_user"
        )
        
        if preview["risk_level"] == "low":
            # ì•ˆì „ â†’ ì‹¤í–‰
            await tool.apply_refactor(...)
        else:
            # ìœ„í—˜ â†’ ì‚¬ìš©ìì—ê²Œ í™•ì¸
            await ask_user_confirmation(preview)
        """
        result = await self.speculative_analyzer.speculate(
            refactor_type,
            **kwargs
        )
        
        return {
            "affected_files": result.affected_files,
            "call_graph_changes": result.call_graph_diff.summary(),
            "breaking_changes": result.breaking_changes,
            "risk_level": self.assess_risk(result),
            "recommendations": self.generate_recommendations(result)
        }
```

#### êµ¬í˜„ ìœ„ì¹˜
```
src/contexts/code_foundation/infrastructure/speculative/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ virtual_ir.py              # NEW: Virtual IR layer
â”œâ”€â”€ ir_patch.py                # NEW: IR patch model
â””â”€â”€ patch_builder.py           # NEW: Patch ìƒì„±

src/contexts/analysis_indexing/infrastructure/speculative/
â”œâ”€â”€ speculative_analyzer.py    # NEW: Speculative ë¶„ì„
â”œâ”€â”€ graph_diff.py              # NEW: Graph diff ê³„ì‚°
â””â”€â”€ risk_assessor.py           # NEW: Risk í‰ê°€

src/contexts/agent_automation/infrastructure/tools/
â””â”€â”€ speculative_tool.py        # NEW: Agent tool
```

#### ì„±ëŠ¥ ëª©í‘œ
- Virtual IR creation: < 100ms
- Speculative analysis: < 500ms (small refactor)
- Memory overhead: < 2x base IR size

---

### 2.3 Semantic Change Detection
**Impact**: â­â­â­â­ (PR ë¦¬ë·° í’ˆì§ˆ +40%)  
**Difficulty**: â­â­â­â­ (Hard)  
**Priority**: P1  
**Status**: ğŸš§ TODO

#### í•µì‹¬ ê°€ì¹˜
- Git diff: ë‹¨ìˆœ text diff
- Graph diff: êµ¬ì¡° diff
- **Semantic diff: ì˜ë¯¸ ë³€í™” ì¶”ì **
- PR ë¦¬ë·° í’ˆì§ˆ 40% ì¦ê°€
- Breaking changes ìë™ ì˜ˆì¸¡

#### Semantic Change ì¢…ë¥˜

```python
# 1. Parameter removed (breaking!)
- def process(x, y, z):
+ def process(x, y):

# 2. Return type changed (breaking!)
- def get_user() -> User:
+ def get_user() -> Optional[User]:

# 3. Side-effect added (semantic change!)
def calculate(x):
-   return x * 2
+   log_metric("calc", x)  # Side-effect!
+   return x * 2

# 4. Error propagation changed
- def load(): return data
+ def load(): raise FileNotFoundError()  # NEW exception!

# 5. Reachable-set changed
def main():
-   safe_operation()
+   dangerous_operation()  # Different call target!
```

#### êµ¬í˜„ ì „ëµ

**Phase 1: Semantic Diff Engine** (2ì£¼)
```python
# src/contexts/analysis_indexing/infrastructure/semantic_diff/diff_engine.py

class SemanticChange(Enum):
    """Semantic change ì¢…ë¥˜"""
    PARAM_ADDED = "param_added"
    PARAM_REMOVED = "param_removed"
    RETURN_TYPE_CHANGED = "return_type_changed"
    SIDE_EFFECT_ADDED = "side_effect_added"
    EXCEPTION_ADDED = "exception_added"
    CALL_TARGET_CHANGED = "call_target_changed"
    REACHABLE_SET_CHANGED = "reachable_set_changed"

@dataclass
class SemanticChangeRecord:
    """Semantic change ê¸°ë¡"""
    change_type: SemanticChange
    symbol: str
    old_value: Any
    new_value: Any
    severity: str  # "breaking", "warning", "info"
    affected_symbols: Set[str]

class SemanticDiffEngine:
    """Semantic diff ê³„ì‚°"""
    
    def compute_semantic_diff(
        self,
        old_ir: RepositoryIR,
        new_ir: RepositoryIR
    ) -> List[SemanticChangeRecord]:
        """
        Semantic changes íƒì§€:
        1. Symbol-level diff
        2. Graph-level diff
        3. Behavior-level diff
        """
        changes = []
        
        # Symbol signature changes
        changes.extend(self.detect_signature_changes(old_ir, new_ir))
        
        # Side-effect changes
        changes.extend(self.detect_side_effect_changes(old_ir, new_ir))
        
        # Exception changes
        changes.extend(self.detect_exception_changes(old_ir, new_ir))
        
        # Call graph changes
        changes.extend(self.detect_call_changes(old_ir, new_ir))
        
        # Reachability changes
        changes.extend(self.detect_reachability_changes(old_ir, new_ir))
        
        return changes
    
    def detect_signature_changes(
        self,
        old_ir: RepositoryIR,
        new_ir: RepositoryIR
    ) -> List[SemanticChangeRecord]:
        """Signature ë³€ê²½ íƒì§€"""
        changes = []
        
        for symbol_id in old_ir.symbols & new_ir.symbols:
            old_sym = old_ir.get_symbol(symbol_id)
            new_sym = new_ir.get_symbol(symbol_id)
            
            # Parameter ë³€ê²½
            old_params = set(old_sym.parameters.keys())
            new_params = set(new_sym.parameters.keys())
            
            if old_params != new_params:
                removed = old_params - new_params
                added = new_params - old_params
                
                if removed:
                    # Breaking change!
                    changes.append(SemanticChangeRecord(
                        change_type=SemanticChange.PARAM_REMOVED,
                        symbol=symbol_id,
                        old_value=removed,
                        new_value=None,
                        severity="breaking",
                        affected_symbols=self.find_callers(symbol_id, old_ir)
                    ))
            
            # Return type ë³€ê²½
            if old_sym.return_type != new_sym.return_type:
                changes.append(SemanticChangeRecord(
                    change_type=SemanticChange.RETURN_TYPE_CHANGED,
                    symbol=symbol_id,
                    old_value=old_sym.return_type,
                    new_value=new_sym.return_type,
                    severity=self.assess_return_type_change(
                        old_sym.return_type,
                        new_sym.return_type
                    ),
                    affected_symbols=self.find_callers(symbol_id, old_ir)
                ))
        
        return changes
```

**Phase 2: PR Analysis Tool** (1ì£¼)
```python
# src/contexts/analysis_indexing/infrastructure/semantic_diff/pr_analyzer.py

class PRAnalyzer:
    """PR semantic ë¶„ì„"""
    
    def analyze_pr(
        self,
        repo_path: Path,
        base_commit: str,
        head_commit: str
    ) -> PRAnalysisReport:
        """
        PRì˜ semantic impact ë¶„ì„:
        1. Base IR ë¹Œë“œ
        2. Head IR ë¹Œë“œ
        3. Semantic diff ê³„ì‚°
        4. Risk í‰ê°€
        5. Report ìƒì„±
        """
        # Build IRs
        base_ir = self.build_ir_at_commit(repo_path, base_commit)
        head_ir = self.build_ir_at_commit(repo_path, head_commit)
        
        # Semantic diff
        semantic_changes = self.diff_engine.compute_semantic_diff(base_ir, head_ir)
        
        # Group by severity
        breaking = [c for c in semantic_changes if c.severity == "breaking"]
        warnings = [c for c in semantic_changes if c.severity == "warning"]
        info = [c for c in semantic_changes if c.severity == "info"]
        
        # Risk assessment
        risk_level = self.assess_pr_risk(semantic_changes)
        
        return PRAnalysisReport(
            breaking_changes=breaking,
            warnings=warnings,
            info=info,
            risk_level=risk_level,
            affected_files=self.compute_affected_files(semantic_changes),
            test_recommendations=self.recommend_tests(semantic_changes)
        )
```

**Phase 3: GitHub Integration** (3ì¼)
```python
# src/contexts/analysis_indexing/infrastructure/semantic_diff/github_bot.py

class SemanticDiffBot:
    """GitHub bot for semantic diff comments"""
    
    async def comment_on_pr(
        self,
        pr_number: int,
        report: PRAnalysisReport
    ):
        """
        PRì— semantic diff ì½”ë©˜íŠ¸:
        
        ì˜ˆ:
        ## ğŸ” Semantic Analysis
        
        ### âš ï¸ Breaking Changes (2)
        - `process_user`: Parameter `role` removed
          - Affects 15 call sites
          - Files: `main.py`, `api.py`, ...
        
        - `get_data`: Return type changed `User` â†’ `Optional[User]`
          - Callers may need null checks
          - Affects 8 call sites
        
        ### ğŸ’¡ Recommendations
        - Add default value for `role` parameter
        - Update callers to handle `None` return
        - Add tests for null case
        """
        comment = self.format_report(report)
        await self.github_client.create_pr_comment(pr_number, comment)
```

#### êµ¬í˜„ ìœ„ì¹˜
```
src/contexts/analysis_indexing/infrastructure/semantic_diff/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ diff_engine.py             # NEW: Semantic diff engine
â”œâ”€â”€ pr_analyzer.py             # NEW: PR analysis
â”œâ”€â”€ risk_assessor.py           # NEW: Risk í‰ê°€
â””â”€â”€ github_bot.py              # NEW: GitHub integration
```

---

### 2.4 AutoRRF â€“ Query Fusion Auto Weighting
**Impact**: â­â­â­â­ (ê²€ìƒ‰ ì •í™•ë„ +25%)  
**Difficulty**: â­â­â­ (Medium-Hard)  
**Priority**: P1  
**Status**: ğŸš§ TODO (Retrieval í™•ì¥)

#### í•µì‹¬ ê°€ì¹˜
- í˜„ì¬: RRF ê¸°ë°˜ weighted fusion (ì •ì  weight)
- ëª©í‘œ: **ì¿¼ë¦¬ ì˜ë„ì— ë”°ë¼ weight ìë™ ì¡°ì •**
- LLM feedbackìœ¼ë¡œ self-tuning

#### êµ¬í˜„ ì „ëµ

**Phase 1: Query Intent Classifier** (1ì£¼)
```python
# src/contexts/retrieval_search/infrastructure/auto_rrf/intent_classifier.py

class QueryIntent(Enum):
    """Query ì˜ë„"""
    CALL_REFERENCE = "call_reference"      # "ì–´ë””ì„œ í˜¸ì¶œ?"
    DEFINITION = "definition"              # "ì •ì˜ ì°¾ê¸°"
    EXPLANATION = "explanation"            # "ì„¤ëª…í•´ì¤˜"
    REFACTOR_LOCATION = "refactor"         # "ë¦¬íŒ©í„° ìœ„ì¹˜"
    SIMILAR_CODE = "similar"               # "ë¹„ìŠ·í•œ ì½”ë“œ"

class IntentClassifier:
    """Query ì˜ë„ ë¶„ë¥˜"""
    
    async def classify(self, query: str) -> QueryIntent:
        """
        LLMìœ¼ë¡œ query ì˜ë„ ë¶„ë¥˜:
        
        ì˜ˆ:
        - "ì´ API ì–´ë””ì„œ í˜¸ì¶œ?" â†’ CALL_REFERENCE
        - "ì´ ë¡œì§ ì„¤ëª…í•´ì¤˜" â†’ EXPLANATION
        - "ì •í™•í•œ refactor ìœ„ì¹˜" â†’ REFACTOR_LOCATION
        """
        prompt = f"""
        Classify the intent of this code search query:
        
        Query: "{query}"
        
        Intents:
        - call_reference: Finding where a function/API is called
        - definition: Finding symbol definition
        - explanation: Explaining code logic/behavior
        - refactor: Finding exact location for refactoring
        - similar: Finding similar code patterns
        
        Return only the intent name.
        """
        
        response = await self.llm_client.complete(prompt)
        return QueryIntent(response.strip().lower())
```

**Phase 2: Auto Weight Tuner** (2ì£¼)
```python
# src/contexts/retrieval_search/infrastructure/auto_rrf/auto_tuner.py

@dataclass
class RetrievalWeights:
    """Retrieval weights per intent"""
    graph_weight: float       # Call graph, import graph
    embedding_weight: float   # Vector similarity
    symbol_weight: float      # Exact symbol match
    keyword_weight: float     # Keyword search

class AutoWeightTuner:
    """Intent ê¸°ë°˜ ìë™ weight ì¡°ì •"""
    
    def __init__(self):
        # Intentë³„ base weights
        self.intent_weights = {
            QueryIntent.CALL_REFERENCE: RetrievalWeights(
                graph_weight=0.5,      # Graph ì¤‘ìš”!
                embedding_weight=0.2,
                symbol_weight=0.2,
                keyword_weight=0.1
            ),
            QueryIntent.EXPLANATION: RetrievalWeights(
                graph_weight=0.1,
                embedding_weight=0.6,  # Embedding ì¤‘ìš”!
                symbol_weight=0.1,
                keyword_weight=0.2
            ),
            QueryIntent.REFACTOR_LOCATION: RetrievalWeights(
                graph_weight=0.2,
                embedding_weight=0.1,
                symbol_weight=0.5,     # Exact match ì¤‘ìš”!
                keyword_weight=0.2
            ),
            QueryIntent.DEFINITION: RetrievalWeights(
                graph_weight=0.3,
                embedding_weight=0.1,
                symbol_weight=0.5,
                keyword_weight=0.1
            ),
            QueryIntent.SIMILAR_CODE: RetrievalWeights(
                graph_weight=0.1,
                embedding_weight=0.7,  # Embedding ì¤‘ìš”!
                symbol_weight=0.1,
                keyword_weight=0.1
            )
        }
        
        # Learning data (feedback ì¶•ì )
        self.feedback_db = FeedbackDatabase()
    
    def get_weights(
        self,
        intent: QueryIntent,
        query: str
    ) -> RetrievalWeights:
        """
        Intent + ê³¼ê±° feedbackìœ¼ë¡œ weight ê²°ì •:
        1. Base weights (intentë³„)
        2. Similar query feedback ì°¸ê³ 
        3. Adjusted weights ë°˜í™˜
        """
        base_weights = self.intent_weights[intent]
        
        # Similar query feedback
        similar_queries = self.feedback_db.find_similar(query)
        if similar_queries:
            adjustments = self.compute_adjustments(similar_queries)
            return self.apply_adjustments(base_weights, adjustments)
        
        return base_weights
    
    def learn_from_feedback(
        self,
        query: str,
        intent: QueryIntent,
        used_weights: RetrievalWeights,
        user_feedback: float  # 0.0 ~ 1.0 (ë§Œì¡±ë„)
    ):
        """
        ì‚¬ìš©ì feedbackìœ¼ë¡œ í•™ìŠµ:
        - ë§Œì¡±ë„ ë†’ìŒ (>0.8) â†’ weights ê°•í™”
        - ë§Œì¡±ë„ ë‚®ìŒ (<0.4) â†’ weights ì¡°ì • í•„ìš”
        """
        self.feedback_db.record(
            query=query,
            intent=intent,
            weights=used_weights,
            satisfaction=user_feedback
        )
        
        # Periodic retuning
        if self.feedback_db.size() % 100 == 0:
            self.retune_weights()
```

**Phase 3: Adaptive Retriever** (1ì£¼)
```python
# src/contexts/retrieval_search/infrastructure/auto_rrf/adaptive_retriever.py

class AdaptiveRetriever:
    """AutoRRF ê¸°ë°˜ adaptive retrieval"""
    
    async def retrieve(
        self,
        query: str,
        repo_id: str
    ) -> List[RetrievalResult]:
        """
        Adaptive retrieval:
        1. Query intent ë¶„ë¥˜
        2. Intentì— ë§ëŠ” weights ê²°ì •
        3. Multi-index ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ ì ìš©)
        4. RRF fusion
        5. Re-ranking
        """
        # Intent classification
        intent = await self.intent_classifier.classify(query)
        
        # Get weights
        weights = self.weight_tuner.get_weights(intent, query)
        
        logger.info(
            "adaptive_retrieval",
            intent=intent.value,
            weights=weights.__dict__
        )
        
        # Multi-index search
        graph_results = await self.graph_searcher.search(query, repo_id)
        embedding_results = await self.vector_searcher.search(query, repo_id)
        symbol_results = await self.symbol_searcher.search(query, repo_id)
        keyword_results = await self.keyword_searcher.search(query, repo_id)
        
        # Weighted RRF fusion
        fused = self.weighted_rrf_fusion(
            [
                (graph_results, weights.graph_weight),
                (embedding_results, weights.embedding_weight),
                (symbol_results, weights.symbol_weight),
                (keyword_results, weights.keyword_weight)
            ],
            k=60
        )
        
        return fused
    
    def weighted_rrf_fusion(
        self,
        results_with_weights: List[Tuple[List[Result], float]],
        k: int = 60
    ) -> List[Result]:
        """
        Weighted RRF:
        score = Î£ (weight_i * 1 / (k + rank_i))
        """
        scores = defaultdict(float)
        
        for results, weight in results_with_weights:
            for rank, result in enumerate(results):
                rrf_score = weight / (k + rank)
                scores[result.id] += rrf_score
        
        # Sort by score
        sorted_results = sorted(scores.items(), key=lambda x: -x[1])
        return [result_id for result_id, score in sorted_results]
```

#### êµ¬í˜„ ìœ„ì¹˜
```
src/contexts/retrieval_search/infrastructure/auto_rrf/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ intent_classifier.py       # NEW: Query intent ë¶„ë¥˜
â”œâ”€â”€ auto_tuner.py              # NEW: Auto weight tuning
â”œâ”€â”€ adaptive_retriever.py      # NEW: Adaptive retrieval
â””â”€â”€ feedback_db.py             # NEW: Feedback ì €ì¥
```

---

## ğŸ“… Implementation Timeline

### Phase 1 (4ì£¼) - P0 í•µì‹¬ ê¸°ëŠ¥
**ëª©í‘œ**: Must-Have 18/18 ë‹¬ì„± + Type Narrowing ì™„ì„±

| Week | Tasks | Deliverables |
|------|-------|--------------|
| W1 | Local Overlay (Phase 1-2) | Overlay IR Builder, Graph Merger |
| W2 | Local Overlay (Phase 3-4) + Testing | LSP Integration, Tests |
| W3 | Full Type Narrowing (Phase 1-2) | CFG-based Type State, Call Graph Precision |
| W4 | Full Type Narrowing (Phase 3) + Testing | IR Integration, Tests |

**ì™„ë£Œ ì‹œ ìƒíƒœ**:
- âœ… Must-Have: 18/18 (100%)
- âœ… Call Graph Precision: +30%
- âœ… IDE/Agent Accuracy: +30-50%

---

### Phase 2 (6ì£¼) - P0 ê³ ê¸‰ ê¸°ëŠ¥
**ëª©í‘œ**: Context-Sensitive + SRI ì™„ì„±

| Week | Tasks | Deliverables |
|------|-------|--------------|
| W5-6 | Context-Sensitive Call Graph (Phase 1-2) | Call Context, Value Tracking |
| W7-8 | Context-Sensitive Call Graph (Phase 3-4) | CS Analysis, Impact Analysis |
| W9-10 | Semantic Region Index (Phase 1-3) | Region Segmentation, Annotation, Index |
| W11 | SRI (Phase 4) + Integration Testing | Retrieval Integration, E2E Tests |

**ì™„ë£Œ ì‹œ ìƒíƒœ**:
- âœ… Context-Sensitive Call Graph
- âœ… Semantic Region Index
- âœ… **ì—…ê³„ SOTA í™•ì •**

---

### Phase 3 (6ì£¼) - P1 ì°¨ì„¸ëŒ€ ê¸°ëŠ¥
**ëª©í‘œ**: Speculative + Semantic Diff ì™„ì„±

| Week | Tasks | Deliverables |
|------|-------|--------------|
| W12-13 | Impact-Based Partial Rebuild | Change Classifier, Partial Rebuilder |
| W14-16 | Speculative Graph Execution | Virtual IR, Speculative Analyzer |
| W17-18 | Semantic Change Detection | Diff Engine, PR Analyzer |
| W19 | AutoRRF | Intent Classifier, Auto Tuner |

**ì™„ë£Œ ì‹œ ìƒíƒœ**:
- âœ… **ì°¨ì„¸ëŒ€ ê¸°ëŠ¥ 4ê°œ ì™„ì„±**
- âœ… **ì„¸ê³„ ìµœê³ ê¸‰ Code Intelligence Engine**

---

## ğŸ¯ Success Metrics

### P0 ì™„ë£Œ ì‹œ
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Must-Have:        18/18 (100%) âœ…
SCIP Advanced:    20/20 (100%) âœ…
Call Graph:       Precision +30%
Type Narrowing:   TS/Python Full
Context-Sensitive: âœ…
SRI:              âœ…
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Status: SOTA í™•ì • ğŸ†
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### P1 ì™„ë£Œ ì‹œ
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì°¨ì„¸ëŒ€ ê¸°ëŠ¥:      4/4 (100%) âœ…
Speculative:      âœ…
Semantic Diff:    âœ…
AutoRRF:          âœ…
Impact Rebuild:   âœ…
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Status: ì„¸ê³„ ìµœê³ ê¸‰ ğŸŒŸ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸš€ Quick Start

### ìš°ì„ ìˆœìœ„ ì¶”ì²œ

**ì§€ê¸ˆ ë‹¹ì¥ ì‹œì‘í•  ê¸°ëŠ¥ (Impact ìˆœ)**:
1. **Local Overlay** (P0, Critical) - ì •í™•ë„ ì¦‰ì‹œ 30-50% í–¥ìƒ
2. **Full Type Narrowing** (P0, High) - Call Graph +30% precision
3. **Semantic Region Index** (P0, High) - LLM ì°¨ë³„í™”
4. **Context-Sensitive CG** (P0, Very High) - ì„¸ê³„ ìµœê³ ê¸‰

**Phaseë³„ ì¶”ì²œ**:
- **Month 1**: Local Overlay + Type Narrowing â†’ Must-Have 18/18
- **Month 2-3**: Context-Sensitive + SRI â†’ SOTA í™•ì •
- **Month 4-5**: Speculative + Semantic Diff â†’ ì°¨ì„¸ëŒ€ ì—”ì§„

---

## ğŸ“ Notes

### í˜„ì¬ êµ¬ì¡° í™œìš©
ì´ë¯¸ í›Œë¥­í•œ ê¸°ë°˜ì´ êµ¬ì¶•ë˜ì–´ ìˆìŒ:
- âœ… IR ì‹œìŠ¤í…œ: `code_foundation` context
- âœ… Incremental Update: `change_detector.py`
- âœ… Type Narrowing ê¸°ë³¸ êµ¬ì¡°: `type_narrowing_full.py`
- âœ… Graph ì‹œìŠ¤í…œ: Kuzu-based
- âœ… Multi-index: Qdrant + Tantivy + Zoekt

### ì¶”ê°€ êµ¬í˜„ íŒ¨í„´
ëª¨ë“  ì‹ ê·œ ê¸°ëŠ¥ì€ ê¸°ì¡´ DDD íŒ¨í„´ ë”°ë¦„:
```
contexts/
â””â”€â”€ {context_name}/
    â”œâ”€â”€ domain/
    â”‚   â”œâ”€â”€ models.py      # Domain models
    â”‚   â””â”€â”€ ports.py       # Interfaces
    â”œâ”€â”€ infrastructure/
    â”‚   â””â”€â”€ {feature}.py   # Implementations
    â””â”€â”€ usecase/
        â””â”€â”€ {use_case}.py  # Use cases
```

---

**ë¬¸ì„œ ì‘ì„± ì™„ë£Œ**  
**Date**: 2025-12-04  
**Version**: 1.0.0  
**Status**: Ready for Implementation ğŸš€

