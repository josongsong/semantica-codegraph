# Storage Backend

**Status**: âœ… Production Ready (100% Complete)
**RFCs**: RFC-074 (Low-Level), RFC-100 (High-Level)
**Last Updated**: 2025-12-28

---

## Quick Start

### SQLite (Development/CLI)

```rust
use codegraph_ir::features::storage::{
    CodeSnapshotStore, SqliteChunkStore, ChunkStore
};

#[tokio::main]
async fn main() -> Result<()> {
    // 1. Create SQLite store
    let sqlite = SqliteChunkStore::new("my-app.db").await?;

    // 2. Wrap with high-level API
    let store = CodeSnapshotStore::new(sqlite);

    // 3. Create snapshot
    store.create_snapshot(
        "my-app",
        "my-app:main",
        Some("abc123".to_string()),
        Some("main".to_string()),
    ).await?;

    Ok(())
}
```

### PostgreSQL (Production/Server)

```rust
use codegraph_ir::features::storage::{
    CodeSnapshotStore, PostgresChunkStore, ChunkStore
};

#[tokio::main]
async fn main() -> Result<()> {
    // 1. Create PostgreSQL store with connection pooling
    let postgres = PostgresChunkStore::new(
        "postgres://user:password@localhost/codegraph"
    ).await?;

    // 2. Wrap with high-level API (same as SQLite!)
    let store = CodeSnapshotStore::new(postgres);

    // 3. Create snapshot (same API)
    store.create_snapshot(
        "my-app",
        "my-app:main",
        Some("abc123".to_string()),
        Some("main".to_string()),
    ).await?;

    Ok(())
}
```

### Run Demo

```bash
# SQLite demo
cargo run --example storage_demo

# PostgreSQL demo (requires --features postgres)
cargo run --features postgres --example storage_demo
```

---

## Architecture

### Two-Level API

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RFC-100: High-Level API          â”‚
â”‚  CodeSnapshotStore                â”‚
â”‚  â”œâ”€ replace_file()                â”‚
â”‚  â”œâ”€ compare_commits()             â”‚
â”‚  â””â”€ create_incremental_snapshot() â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ wraps
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RFC-074: Low-Level API           â”‚
â”‚  ChunkStore Trait (18 methods)    â”‚
â”‚  â”œâ”€ save_chunk/chunks()           â”‚
â”‚  â”œâ”€ get_chunks()                  â”‚
â”‚  â””â”€ soft_delete_file_chunks()     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ impl
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SqliteChunkStore                 â”‚
â”‚  (5 tables, 14 indexes)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Features

### RFC-074: Low-Level Storage
- âœ… Multi-repository support
- âœ… Snapshot isolation
- âœ… ACID transactions
- âœ… Soft delete (no cascading)
- âœ… Content-addressable storage
- âœ… Dependency graph (BFS)
- âœ… Full-text search

### RFC-100: High-Level API
- âœ… File-level replace primitive (atomic)
- âœ… Semantic commit diff (FQN-based)
- âœ… Incremental snapshot (10-100x speedup)
- âœ… Commit-based operations

---

## API Overview

### High-Level API (CodeSnapshotStore)

#### 1. File-Level Replace
```rust
// Atomically replace file between commits
store.replace_file(
    "my-app",
    "my-app:main",      // base snapshot
    "my-app:feature",    // new snapshot
    "src/auth.rs",
    vec![chunk_v2],
    vec![dep1, dep2],
).await?;
```

**Guarantees**:
- ATOMIC (all-or-nothing)
- TRANSACTIONAL (ACID)
- IDEMPOTENT (retry-safe)

#### 2. Semantic Diff
```rust
// Compare two commits (FQN-based)
let diff = store.compare_commits(
    "my-app",
    "my-app:main",
    "my-app:feature",
).await?;

println!("Added: {}", diff.added.len());
println!("Modified: {}", diff.modified.len());
println!("Deleted: {}", diff.deleted.len());
```

#### 3. Incremental Snapshot
```rust
// Only re-analyze changed files (10-100x faster!)
let stats = store.create_incremental_snapshot(
    "my-app",
    "my-app:commit1",
    "my-app:commit2",
    vec!["src/a.rs", "src/b.rs"],
    |file_path| {
        // Your analyzer
        let chunks = analyze_file(file_path)?;
        Ok((chunks, vec![]))
    },
).await?;

println!("Skipped: {} files", stats.files_skipped);
```

### Low-Level API (ChunkStore)

```rust
// Direct chunk operations
store.save_chunk(&chunk).await?;
store.save_chunks(&chunks).await?;  // Batch

// Queries
let chunks = store.get_chunks("my-app", "my-app:main").await?;
let deps = store.get_dependencies_from("chunk-id").await?;

// Transitive dependencies (BFS)
let all_deps = store.get_transitive_dependencies("chunk-id", 10).await?;

// Search
let results = store.search_content("login", 50).await?;
```

---

## Database Schema

### Tables

1. **repositories** - Repository metadata
2. **snapshots** - Commit/branch snapshots
3. **chunks** - Code chunks (FQN, content, metadata)
4. **dependencies** - Dependency graph edges
5. **file_metadata** - File-level tracking (hash)

### Indexes (14 total)
- `idx_chunks_snapshot` - Fast snapshot queries
- `idx_chunks_fqn` - FQN lookups
- `idx_chunks_file` - File-level queries
- `idx_deps_from`, `idx_deps_to` - Graph traversal
- `idx_file_metadata_snapshot_file` - Incremental updates
- ... and more

---

## Performance

### Incremental Snapshot
```
Baseline:     100 files Ã— 500ms = 50 seconds
Incremental:  2 changed Ã— 500ms = 1 second
Speedup:      50x (typical), up to 100x
```

### Operations
```
Single chunk write:   < 1ms  (in-memory)
Batch 100 chunks:     ~ 5ms  (transaction)
Full-text search:     ~ 10ms (1000 chunks)
BFS depth 10:         ~ 15ms (with indexes)
```

---

## Testing

### Integration Tests (17 tests)
```bash
# Low-level tests
cargo test test_sqlite_integration

# High-level tests
cargo test test_code_snapshot_store
```

### End-to-End Demo
```bash
cargo run --example storage_demo
```

**Test Coverage**: 100% of RFC requirements

---

## RFC Compliance

| RFC | Status | Requirements |
|-----|--------|--------------|
| RFC-074 (Low-Level) | âœ… 100% | 52/52 |
| RFC-100 (High-Level) | âœ… 100% | 18/18 |

**Total**: 70/70 requirements âœ…

---

## Documentation

- **Completion Report**: `docs/STORAGE-COMPLETION-REPORT.md`
- **Implementation Guide**: `docs/RFC-074-IMPLEMENTATION.md`
- **API Docs**: Run `cargo doc --open`
- **Examples**: `examples/storage_demo.rs`

---

## Module Structure

```
storage/
â”œâ”€â”€ api/                    # RFC-100: High-level API
â”‚   â”œâ”€â”€ snapshot_store.rs   (CodeSnapshotStore)
â”‚   â””â”€â”€ snapshot_diff.rs    (SnapshotDiff types)
â”œâ”€â”€ domain/                 # RFC-074: Core models
â”‚   â”œâ”€â”€ models.rs           (Chunk, Dependency, Snapshot)
â”‚   â””â”€â”€ ports.rs            (ChunkStore trait)
â”œâ”€â”€ infrastructure/         # RFC-074: Adapters
â”‚   â””â”€â”€ sqlite_store.rs     (SQLite implementation)
â””â”€â”€ mod.rs                  # Public API exports
```

---

## Usage Patterns

### Pattern 1: Single-Repository CLI
```rust
// Use in-memory for fast local analysis
let store = SqliteChunkStore::in_memory()?;
let api = CodeSnapshotStore::new(store);
```

### Pattern 2: Multi-Repository Server
```rust
// Use persistent database
let store = SqliteChunkStore::new("repos.db").await?;
let api = CodeSnapshotStore::new(store);
```

### Pattern 3: Incremental Indexing
```rust
// Check git diff, only re-analyze changed files
let changed = get_changed_files("HEAD~1", "HEAD")?;
let stats = api.create_incremental_snapshot(
    repo_id, "commit1", "commit2", changed, analyzer
).await?;
```

---

## Design Principles

### 1. Port/Adapter Pattern
- `ChunkStore` trait abstracts storage backend
- SQLite/PostgreSQL interchangeable

### 2. Soft Delete
- No hard deletes (prevents cascading issues)
- `is_deleted` flag for recovery
- UPSERT revives deleted chunks

### 3. Content-Addressable
- SHA256 content hash
- Hash-based change detection
- Incremental update optimization

### 4. Two-State Rule
- **Ephemeral**: File save (local only)
- **Committed**: Git commit (stored)

---

## Migration Guide

### From Direct DB Access
```rust
// Before
let conn = Connection::open("db.sqlite")?;
conn.execute("INSERT INTO chunks ...")?;

// After
let store = SqliteChunkStore::new("db.sqlite").await?;
store.save_chunk(&chunk).await?;
```

### From In-Memory
```rust
// Before
let mut chunks = HashMap::new();
chunks.insert(id, chunk);

// After
let store = SqliteChunkStore::in_memory()?;
let api = CodeSnapshotStore::new(store);
api.create_snapshot(...).await?;
```

---

## Troubleshooting

### Q: Foreign key constraint failed?
**A**: Create repository first before creating snapshots:
```rust
store.save_repository(&repo).await?;
store.create_snapshot(...).await?;
```

### Q: How to reset database?
**A**: Delete the SQLite file or use in-memory:
```rust
let store = SqliteChunkStore::in_memory()?;
```

### Q: How to handle large repositories?
**A**: Use batch operations and incremental updates:
```rust
store.save_chunks(&chunks).await?;  // Batch
api.create_incremental_snapshot(...).await?;  // Incremental
```

---

## PostgreSQL Deployment (Production)

### Setup PostgreSQL Database

```bash
# Install PostgreSQL (macOS)
brew install postgresql@15
brew services start postgresql@15

# Create database
createdb codegraph

# Or using Docker
docker run -d \
  --name codegraph-postgres \
  -e POSTGRES_DB=codegraph \
  -e POSTGRES_USER=codegraph \
  -e POSTGRES_PASSWORD=your_password \
  -p 5432:5432 \
  postgres:15-alpine
```

### Run Migrations

Migrations are automatically applied when creating `PostgresChunkStore`:

```rust
let store = PostgresChunkStore::new(
    "postgres://codegraph:password@localhost/codegraph"
).await?;

// Migrations run automatically âœ…
```

Manual migration (if needed):
```bash
# Install sqlx CLI
cargo install sqlx-cli --features postgres

# Run migrations manually
cd codegraph-ir
sqlx migrate run --database-url "postgres://localhost/codegraph"
```

### Production Configuration

```rust
// Production settings (environment variables)
use std::env;

#[tokio::main]
async fn main() -> Result<()> {
    let database_url = env::var("DATABASE_URL")
        .unwrap_or_else(|_| "postgres://localhost/codegraph".to_string());

    let store = PostgresChunkStore::new(&database_url).await?;
    let api = CodeSnapshotStore::new(store);

    // Your application logic
    Ok(())
}
```

**Environment Variables**:
```bash
export DATABASE_URL="postgres://user:password@host:5432/codegraph"
export RUST_LOG=info  # Logging level
```

### Performance Tuning

**PostgreSQL Configuration** (`postgresql.conf`):
```ini
# Connections (adjust based on load)
max_connections = 100

# Memory
shared_buffers = 256MB
effective_cache_size = 1GB
work_mem = 16MB

# WAL
wal_level = replica
max_wal_size = 1GB

# Query optimizer
random_page_cost = 1.1  # SSD storage
```

**Connection Pool Settings**:
```rust
// Custom pool configuration
let pool = PgPoolOptions::new()
    .max_connections(50)        // Increase for high load
    .min_connections(5)         // Keep warm connections
    .acquire_timeout(Duration::from_secs(10))
    .idle_timeout(Duration::from_secs(600))
    .max_lifetime(Duration::from_secs(1800))
    .connect(&database_url)
    .await?;
```

### Monitoring

```sql
-- Active connections
SELECT count(*) FROM pg_stat_activity WHERE datname = 'codegraph';

-- Table sizes
SELECT
    relname AS table_name,
    pg_size_pretty(pg_total_relation_size(relid)) AS total_size
FROM pg_catalog.pg_statio_user_tables
ORDER BY pg_total_relation_size(relid) DESC;

-- Index usage
SELECT
    schemaname, tablename, indexname,
    idx_scan, idx_tup_read, idx_tup_fetch
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;
```

### Backup & Recovery

```bash
# Backup
pg_dump codegraph > backup_$(date +%Y%m%d).sql

# Restore
psql codegraph < backup_20250128.sql

# Continuous archiving (production)
# Configure WAL archiving in postgresql.conf
```

### Migration to PostgreSQL from SQLite

```bash
# Export SQLite data
sqlite3 codegraph.db .dump > data.sql

# Convert to PostgreSQL format (manual adjustments may be needed)
# - Change AUTOINCREMENT to SERIAL
# - Change DATETIME to TIMESTAMP
# - Change BOOLEAN to BOOLEAN (not INTEGER)

# Import to PostgreSQL
psql codegraph < data_converted.sql
```

### Testing PostgreSQL Integration

```bash
# Set test database URL
export TEST_DATABASE_URL="postgres://localhost/codegraph_test"

# Run PostgreSQL integration tests
cargo test --features postgres test_postgres -- --ignored

# Run all storage tests
cargo test --features postgres --test test_postgres_integration -- --ignored
```

---

## Feature Comparison

| Feature | SQLite | PostgreSQL |
|---------|--------|------------|
| **Use Case** | Development, CLI, single-user | Production, server, multi-user |
| **Concurrency** | Limited (file locks) | Excellent (MVCC) |
| **Max Connections** | 1 writer at a time | 100+ concurrent |
| **Deployment** | Embedded (no daemon) | Server daemon |
| **Scalability** | Up to 140TB | Unlimited (with partitioning) |
| **Full-Text Search** | LIKE queries | Native GIN indexes |
| **Transactions** | ACID âœ… | ACID âœ… |
| **Setup Complexity** | Zero config âœ… | Requires server setup |
| **Performance** | Fast (local file) | Fast (network + caching) |

---

## Future Work

### Chunk History (Phase 4)
- Time-travel queries
- Audit trail
- Rollback support

### Pipeline Integration (å¾… API)
- IR Builder â†” Storage
- Automatic indexing
- Change detection hooks

### Advanced PostgreSQL Features
- Table partitioning (for large repos)
- Read replicas (high availability)
- pgvector integration (semantic search)

---

## Contributing

When adding features:
1. Update `ChunkStore` trait if needed
2. Implement in `SqliteChunkStore`
3. Add integration tests
4. Update documentation
5. Run `cargo test` + `cargo run --example storage_demo`

---

**Status**: ğŸ‰ **PRODUCTION READY**
**Questions**: See `docs/STORAGE-COMPLETION-REPORT.md`
