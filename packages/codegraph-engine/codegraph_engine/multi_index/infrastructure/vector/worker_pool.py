"""
Event-Driven Embedding Worker Pool

asyncio.Condition ê¸°ë°˜ ì¦‰ì‹œ ì²˜ë¦¬ worker pool.
Redis notifyë¡œ workerë¥¼ ê¹¨ì›Œì„œ ìš°ì„ ìˆœìœ„ ë†’ì€ chunkë¶€í„° ì²˜ë¦¬.
"""

import asyncio
import random

from codegraph_shared.common.observability import get_logger
from codegraph_engine.multi_index.infrastructure.vector.embedding_queue import EmbeddingQueue

logger = get_logger(__name__)


class EmbeddingWorkerPool:
    """
    Event-driven embedding worker pool.

    íŠ¹ì§•:
    - Nê°œ worker ë™ì‹œ ì²˜ë¦¬
    - asyncio.Conditionìœ¼ë¡œ ì¦‰ì‹œ notify
    - ìš°ì„ ìˆœìœ„ ë†’ì€ ê²ƒë¶€í„° ê°œë³„ ì²˜ë¦¬
    - í ë¹„ì—ˆì„ ë•Œ ëŒ€ê¸° (ë¦¬ì†ŒìŠ¤ ì ˆì•½)

    ì‚¬ìš©:
        pool = EmbeddingWorkerPool(
            queue=embedding_queue,
            worker_count=3,
        )

        await pool.start()
        # enqueue ì‹œ ìë™ìœ¼ë¡œ notifyë¨
        await pool.stop()
    """

    def __init__(
        self,
        queue: EmbeddingQueue,
        worker_count: int = 3,
        max_retries: int = 3,
        base_backoff_seconds: float = 1.0,
        max_backoff_seconds: float = 60.0,
    ):
        """
        Initialize worker pool.

        Args:
            queue: EmbeddingQueue ì¸ìŠ¤í„´ìŠ¤
            worker_count: Worker ê°œìˆ˜ (ë™ì‹œ ì²˜ë¦¬)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            base_backoff_seconds: ê¸°ë³¸ ë°±ì˜¤í”„ ì‹œê°„ (ì´ˆ)
            max_backoff_seconds: ìµœëŒ€ ë°±ì˜¤í”„ ì‹œê°„ (ì´ˆ)
        """
        self.queue = queue
        self.worker_count = worker_count
        self.max_retries = max_retries
        self.base_backoff_seconds = base_backoff_seconds
        self.max_backoff_seconds = max_backoff_seconds

        # asyncio.Condition for event notification
        self.has_items = asyncio.Condition()

        # Worker tasks
        self.workers: list[asyncio.Task] = []
        self.running = False

        # Stats (thread-safe with lock)
        self.stats_lock = asyncio.Lock()
        self.processed_count = 0
        self.failed_count = 0
        self.retry_count = 0

    async def notify(self):
        """
        íì— ìƒˆ í•­ëª© ì¶”ê°€ ì‹œ í˜¸ì¶œ.

        ëŒ€ê¸° ì¤‘ì¸ workerë¥¼ ì¦‰ì‹œ ê¹¨ì›€.
        """
        async with self.has_items:
            self.has_items.notify()  # 1ê°œ worker ê¹¨ìš°ê¸°

    async def notify_all(self):
        """ëª¨ë“  worker ê¹¨ìš°ê¸° (ëŒ€ëŸ‰ enqueue ì‹œ)."""
        async with self.has_items:
            self.has_items.notify_all()

    async def start(self):
        """Worker pool ì‹œì‘."""
        if self.running:
            logger.warning("worker_pool_already_running")
            return

        self.running = True

        for i in range(self.worker_count):
            task = asyncio.create_task(self._worker(i))
            self.workers.append(task)

        logger.info(
            "embedding_worker_pool_started",
            worker_count=self.worker_count,
        )

    async def stop(self):
        """Worker pool ì •ì§€."""
        if not self.running:
            return

        self.running = False

        # ëª¨ë“  worker ê¹¨ì›Œì„œ ì¢…ë£Œí•˜ê²Œ í•¨
        async with self.has_items:
            self.has_items.notify_all()

        # Worker ì¢…ë£Œ ëŒ€ê¸°
        if self.workers:
            await asyncio.gather(*self.workers, return_exceptions=True)
            self.workers = []

        logger.info(
            "embedding_worker_pool_stopped",
            processed=self.processed_count,
            failed=self.failed_count,
        )

    async def _worker(self, worker_id: int):
        """
        ê°œë³„ worker.

        ë¬´í•œ ë£¨í”„:
        1. íì—ì„œ ìš°ì„ ìˆœìœ„ ê°€ì¥ ë†’ì€ ê²ƒ 1ê°œ pop
        2. ì—†ìœ¼ë©´ notify ëŒ€ê¸°
        3. ìˆìœ¼ë©´ ì¦‰ì‹œ ì²˜ë¦¬
        """
        logger.info("worker_started", worker_id=worker_id)

        while self.running:
            try:
                # 1. ìš°ì„ ìˆœìœ„ ë†’ì€ ê²ƒ 1ê°œ ê°€ì ¸ì˜¤ê¸°
                item = await self.queue.pop_one()

                if not item:
                    # í ë¹„ì—ˆìŒ â†’ notify ëŒ€ê¸° ë˜ëŠ” ë¹ ë¥´ê²Œ ì¢…ë£Œ
                    if not self.running:
                        break  # ì¢…ë£Œ ì‹ í˜¸ë°›ìœ¼ë©´ ì¦‰ì‹œ ì¢…ë£Œ

                    async with self.has_items:
                        try:
                            # ì§§ì€ timeoutìœ¼ë¡œ ë¹ ë¥¸ ì¢…ë£Œ ê°€ëŠ¥
                            await asyncio.wait_for(
                                self.has_items.wait(),
                                timeout=1.0,  # 1ì´ˆ timeout
                            )
                        except asyncio.TimeoutError:
                            # Expected: 1ì´ˆë§ˆë‹¤ running ì²´í¬ë¥¼ ìœ„í•œ ì •ìƒ timeout
                            # Debug levelë¡œ ë¡œê¹… (í”„ë¡œë•ì…˜ì—ì„œëŠ” ë…¸ì¶œ ì•ˆë¨)
                            logger.debug(
                                "worker_wait_timeout",
                                worker_id=worker_id,
                                reason="checking_running_status",
                            )

                    continue

                # 2. ì¦‰ì‹œ ì²˜ë¦¬
                success = await self._process_item(item, worker_id)

                # Thread-safe ì¹´ìš´íŠ¸ ì¦ê°€
                async with self.stats_lock:
                    if success:
                        self.processed_count += 1
                    else:
                        self.failed_count += 1

            except asyncio.CancelledError:
                logger.info("worker_cancelled", worker_id=worker_id)
                break

            except Exception as e:
                logger.error(
                    "worker_error",
                    worker_id=worker_id,
                    error=str(e),
                    exc_info=True,
                )
                # ì—ëŸ¬ ë°œìƒí•´ë„ ê³„ì† ì‹¤í–‰

        logger.info("worker_stopped", worker_id=worker_id)

    async def _process_item(self, item: dict, worker_id: int) -> bool:
        """
        ë‹¨ì¼ item ì²˜ë¦¬ (exponential backoff ì¬ì‹œë„ í¬í•¨).

        Args:
            item: Queueì—ì„œ ê°€ì ¸ì˜¨ row
            worker_id: Worker ID

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        chunk_id = item["chunk_id"]
        repo_id = item["repo_id"]
        snapshot_id = item["snapshot_id"]

        logger.debug(
            "worker_processing",
            worker_id=worker_id,
            chunk_id=chunk_id,
            priority=item["priority"],
        )

        last_error: Exception | None = None

        for attempt in range(self.max_retries):
            try:
                # EmbeddingQueueì˜ _process_single_item í˜¸ì¶œ
                success = await self.queue.process_single_item(
                    chunk_id,
                    repo_id,
                    snapshot_id,
                )

                if success:
                    if attempt > 0:
                        # ì¬ì‹œë„ í›„ ì„±ê³µ
                        async with self.stats_lock:
                            self.retry_count += attempt
                        logger.info(
                            "worker_retry_success",
                            worker_id=worker_id,
                            chunk_id=chunk_id,
                            attempts=attempt + 1,
                        )
                    else:
                        logger.debug(
                            "worker_success",
                            worker_id=worker_id,
                            chunk_id=chunk_id,
                        )

                    return True

                # success=Falseì´ì§€ë§Œ ì˜ˆì™¸ ì—†ìŒ - ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
                return False

            except Exception as e:
                last_error = e

                # ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬
                if attempt < self.max_retries - 1:
                    # ğŸ”¥ SOTA: Exponential backoff with jitter
                    backoff = min(
                        self.base_backoff_seconds * (2**attempt),
                        self.max_backoff_seconds,
                    )
                    # Add jitter (Â±25%)
                    jitter = backoff * 0.25 * (random.random() * 2 - 1)
                    wait_time = backoff + jitter

                    logger.warning(
                        "worker_retry_scheduled",
                        worker_id=worker_id,
                        chunk_id=chunk_id,
                        attempt=attempt + 1,
                        max_retries=self.max_retries,
                        wait_seconds=round(wait_time, 2),
                        error=str(e),
                    )

                    await asyncio.sleep(wait_time)
                else:
                    # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
                    logger.error(
                        "worker_process_failed_all_retries",
                        worker_id=worker_id,
                        chunk_id=chunk_id,
                        attempts=self.max_retries,
                        error=str(last_error),
                    )

        return False

    async def get_stats(self) -> dict:
        """Worker pool í†µê³„."""
        async with self.stats_lock:
            return {
                "worker_count": self.worker_count,
                "running": self.running,
                "processed": self.processed_count,
                "failed": self.failed_count,
                "retry_count": self.retry_count,
                "workers_alive": sum(1 for w in self.workers if not w.done()),
            }
