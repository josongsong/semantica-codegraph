"""ëª¨ë“œë³„ ë²”ìœ„ í™•ì¥ ë¡œì§."""

from collections import deque
from typing import TYPE_CHECKING

from codegraph_engine.analysis_indexing.infrastructure.change_detector import ChangeSet
from codegraph_engine.analysis_indexing.infrastructure.models.mode import IndexingMode, ModeScopeLimit
from codegraph_shared.infra.observability import get_logger

if TYPE_CHECKING:
    from codegraph_engine.code_foundation.infrastructure.graph.impact_analyzer import GraphImpactAnalyzer, ImpactResult

logger = get_logger(__name__)


class ScopeExpander:
    """ëª¨ë“œë³„ ì²˜ë¦¬ ë²”ìœ„ í™•ì¥."""

    def __init__(self, graph_store=None, impact_analyzer: "GraphImpactAnalyzer | None" = None):
        """
        Args:
            graph_store: ì˜ì¡´ì„± ê·¸ë˜í”„ ì €ì¥ì†Œ (import ê´€ê³„ ì¡°íšŒìš©)
            impact_analyzer: ì‹¬ë³¼ ìˆ˜ì¤€ ì˜í–¥ë„ ë¶„ì„ê¸° (CALLS/INHERITS ê´€ê³„ìš©)
        """
        self.graph_store = graph_store
        self.impact_analyzer = impact_analyzer

    async def expand_scope(
        self,
        change_set: ChangeSet,
        mode: IndexingMode,
        repo_id: str,
        total_files: int | None = None,
        impact_result: "ImpactResult | None" = None,
    ) -> set[str]:
        """
        ëª¨ë“œë³„ ì²˜ë¦¬ ë²”ìœ„ í™•ì¥.

        Args:
            change_set: L0ì—ì„œ ê°ì§€í•œ ë³€ê²½ íŒŒì¼
            mode: ì¸ë±ì‹± ëª¨ë“œ
            repo_id: ë ˆí¬ì§€í† ë¦¬ ID
            total_files: ì „ì²´ íŒŒì¼ ê°œìˆ˜ (Deep subset ê³„ì‚°ìš©)
            impact_result: ì˜í–¥ë„ ë¶„ì„ ê²°ê³¼ (SIGNATURE_CHANGED ìë™ escalationìš©)

        Returns:
            ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ì§‘í•©
        """
        # ğŸ”¥ SOTA: SIGNATURE_CHANGED ê°ì§€ ì‹œ ìë™ìœ¼ë¡œ DEEP ëª¨ë“œë¡œ escalate
        if impact_result and self._has_signature_changes(impact_result):
            if mode in (IndexingMode.FAST, IndexingMode.BALANCED):
                logger.warning(
                    "signature_change_detected_auto_escalating_to_deep",
                    original_mode=mode.value,
                    changed_symbols=[
                        s.fqn for s in impact_result.changed_symbols if s.change_type.value == "signature_changed"
                    ][:5],  # Log first 5
                    total_signature_changes=sum(
                        1 for s in impact_result.changed_symbols if s.change_type.value == "signature_changed"
                    ),
                )
                mode = IndexingMode.DEEP  # ìë™ escalation for transitive invalidation

        if mode == IndexingMode.FAST:
            # Fast: ë³€ê²½ íŒŒì¼ë§Œ
            return change_set.all_changed

        elif mode == IndexingMode.BALANCED:
            # Balanced: ë³€ê²½ + 1-hop ì¸ì ‘
            return await self._expand_to_neighbors(
                change_set.all_changed,
                repo_id,
                depth=ModeScopeLimit.BALANCED_NEIGHBOR_DEPTH,
                max_files=ModeScopeLimit.BALANCED_MAX_NEIGHBORS,
            )

        elif mode == IndexingMode.DEEP:
            # ğŸ”¥ SOTA: DEEP ëª¨ë“œì—ì„œ impact_resultì˜ transitive_affected í™œìš©
            if impact_result and (impact_result.direct_affected or impact_result.transitive_affected):
                # Impact-based expansion: ë³€ê²½ + direct + transitive
                result = set(change_set.all_changed)
                result.update(impact_result.affected_files)

                logger.info(
                    "deep_mode_with_impact_expansion",
                    changed=len(change_set.all_changed),
                    direct_affected=len(impact_result.direct_affected),
                    transitive_affected=len(impact_result.transitive_affected),
                    total_files=len(result),
                )
                return result

            # Fallback: subset ëª¨ë“œì¸ì§€ ì „ì²´ì¸ì§€ì— ë”°ë¼
            if total_files:
                max_files = min(
                    ModeScopeLimit.DEEP_SUBSET_MAX_FILES,
                    int(total_files * ModeScopeLimit.DEEP_SUBSET_MAX_PERCENT),
                )
                return await self._expand_to_neighbors(
                    change_set.all_changed,
                    repo_id,
                    depth=ModeScopeLimit.DEEP_NEIGHBOR_DEPTH,
                    max_files=max_files,
                )
            else:
                # ì „ì²´ Deep
                return set()  # ë¹ˆ set = ì „ì²´ ì²˜ë¦¬

        elif mode == IndexingMode.BOOTSTRAP:
            # Bootstrap: ì „ì²´ ë ˆí¬
            return set()  # ë¹ˆ set = ì „ì²´ ì²˜ë¦¬

        elif mode == IndexingMode.REPAIR:
            # Repair: ë³€ê²½ íŒŒì¼ + ì˜í–¥ ë°›ì€ ì˜ì—­
            return await self._expand_for_repair(change_set, repo_id)

        else:
            logger.warning(f"Unknown mode: {mode}, defaulting to changed files only")
            return change_set.all_changed

    def _has_signature_changes(self, impact_result: "ImpactResult") -> bool:
        """
        Check if any symbol has SIGNATURE_CHANGED (breaking change).

        Args:
            impact_result: Impact analysis result

        Returns:
            True if signature changes detected
        """
        if not impact_result or not impact_result.changed_symbols:
            return False

        return any(s.change_type.value == "signature_changed" for s in impact_result.changed_symbols)

    async def _expand_to_neighbors(
        self,
        changed_files: set[str],
        repo_id: str,
        depth: int,
        max_files: int,
    ) -> set[str]:
        """
        ì˜ì¡´ì„± ê·¸ë˜í”„ë¡œ ì¸ì ‘ íŒŒì¼ í™•ì¥ (BFS).

        Args:
            changed_files: ì‹œì‘ íŒŒì¼ë“¤
            repo_id: ë ˆí¬ì§€í† ë¦¬ ID
            depth: í™•ì¥ ê¹Šì´ (1-hop, 2-hop ë“±)
            max_files: ìµœëŒ€ íŒŒì¼ ê°œìˆ˜

        Returns:
            í™•ì¥ëœ íŒŒì¼ ì§‘í•©
        """
        if not self.graph_store:
            logger.warning("GraphStore not available, cannot expand neighbors")
            return changed_files

        result = set(changed_files)
        queue = deque([(f, 0) for f in changed_files])
        visited = set(changed_files)

        while queue and len(result) < max_files:
            file_path, current_depth = queue.popleft()

            if current_depth >= depth:
                continue

            # ì–‘ë°©í–¥ íƒìƒ‰: import + imported_by
            try:
                neighbors = await self._get_file_neighbors(repo_id, file_path)

                for neighbor in neighbors:
                    if neighbor not in visited:
                        visited.add(neighbor)
                        result.add(neighbor)
                        queue.append((neighbor, current_depth + 1))

                        if len(result) >= max_files:
                            logger.info(f"Reached max files limit: {max_files}")
                            break

            except Exception as e:
                logger.warning(f"Failed to get neighbors for {file_path}: {e}")
                continue

        logger.info(f"Expanded {len(changed_files)} files to {len(result)} files (depth={depth})")
        return result

    async def _get_file_neighbors(self, repo_id: str, file_path: str) -> set[str]:
        """
        íŒŒì¼ì˜ ì¸ì ‘ íŒŒì¼ ì¡°íšŒ (import + imported_by + callers + callees + inheritors).

        Args:
            repo_id: ë ˆí¬ì§€í† ë¦¬ ID
            file_path: íŒŒì¼ ê²½ë¡œ

        Returns:
            ì¸ì ‘ íŒŒì¼ ê²½ë¡œ ì§‘í•©
        """
        if not self.graph_store:
            return set()

        neighbors = set()

        try:
            # === Import ê´€ê³„ ===
            # import ê´€ê³„ ì¡°íšŒ (file_pathê°€ importí•˜ëŠ” íŒŒì¼ë“¤)
            imports = await self.graph_store.get_imports(repo_id, file_path)
            neighbors.update(imports)

            # imported_by ê´€ê³„ ì¡°íšŒ (file_pathë¥¼ importí•˜ëŠ” íŒŒì¼ë“¤)
            imported_by = await self.graph_store.get_imported_by(repo_id, file_path)
            neighbors.update(imported_by)

            # === CALLS ê´€ê³„ (RFC SEP-G12-SCOPE-EXT) ===
            # callers: file_pathì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” íŒŒì¼ë“¤
            if hasattr(self.graph_store, "get_callers_by_file"):
                callers = await self.graph_store.get_callers_by_file(repo_id, file_path)
                neighbors.update(callers)

            # callees: file_pathê°€ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ë“¤ì˜ íŒŒì¼ë“¤
            if hasattr(self.graph_store, "get_callees_by_file"):
                callees = await self.graph_store.get_callees_by_file(repo_id, file_path)
                neighbors.update(callees)

            # === INHERITS ê´€ê³„ (RFC SEP-G12-SCOPE-EXT) ===
            # subclasses: file_pathì˜ í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ëŠ” íŒŒì¼ë“¤
            if hasattr(self.graph_store, "get_subclasses_by_file"):
                subclasses = await self.graph_store.get_subclasses_by_file(repo_id, file_path)
                neighbors.update(subclasses)

            # superclasses: file_pathê°€ ìƒì†í•˜ëŠ” í´ë˜ìŠ¤ë“¤ì˜ íŒŒì¼ë“¤
            if hasattr(self.graph_store, "get_superclasses_by_file"):
                superclasses = await self.graph_store.get_superclasses_by_file(repo_id, file_path)
                neighbors.update(superclasses)

        except Exception as e:
            logger.warning(f"Failed to query graph for {file_path}: {e}")

        return neighbors

    async def _expand_for_repair(self, change_set: ChangeSet, repo_id: str) -> set[str]:
        """
        Repair ëª¨ë“œ: ë³€ê²½ + ì˜í–¥ ë°›ì€ ì˜ì—­.

        ìŠ¤í‚¤ë§ˆ ë³€ê²½ì´ë‚˜ ì†ìƒ ë³µêµ¬ ì‹œ, í•´ë‹¹ íŒŒì¼ê³¼ ì°¸ì¡°í•˜ëŠ” ëª¨ë“  íŒŒì¼ í¬í•¨.
        Import, CALLS, INHERITS ê´€ê³„ ëª¨ë‘ ì¶”ì .

        Args:
            change_set: ë³€ê²½ íŒŒì¼
            repo_id: ë ˆí¬ì§€í† ë¦¬ ID

        Returns:
            ë³µêµ¬í•  íŒŒì¼ ì§‘í•©
        """
        # ë³€ê²½ íŒŒì¼ë¶€í„° ì‹œì‘
        result = set(change_set.all_changed)

        # ì‚­ì œëœ íŒŒì¼ì„ ì°¸ì¡°í•˜ëŠ” íŒŒì¼ë“¤ë„ í¬í•¨ (ì°¸ì¡° ë¬´ê²°ì„± ë³µêµ¬)
        for deleted_file in change_set.deleted:
            try:
                # Import ê´€ê³„
                if self.graph_store:
                    imported_by = await self.graph_store.get_imported_by(repo_id, deleted_file)
                    result.update(imported_by)

                    # CALLS ê´€ê³„ (ì‚­ì œëœ íŒŒì¼ì˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” íŒŒì¼ë“¤)
                    if hasattr(self.graph_store, "get_callers_by_file"):
                        callers = await self.graph_store.get_callers_by_file(repo_id, deleted_file)
                        result.update(callers)

                    # INHERITS ê´€ê³„ (ì‚­ì œëœ íŒŒì¼ì˜ í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ëŠ” íŒŒì¼ë“¤)
                    if hasattr(self.graph_store, "get_subclasses_by_file"):
                        subclasses = await self.graph_store.get_subclasses_by_file(repo_id, deleted_file)
                        result.update(subclasses)

            except Exception as e:
                logger.warning(f"Failed to get references for deleted {deleted_file}: {e}")

        logger.info(f"Repair scope: {len(result)} files")
        return result

    async def expand_with_impact(
        self,
        change_set: ChangeSet,
        repo_id: str,
        impact_result: "ImpactResult",
        mode: IndexingMode,
    ) -> set[str]:
        """
        ImpactAnalyzer ê²°ê³¼ë¥¼ í™œìš©í•œ ì‹¬ë³¼ ìˆ˜ì¤€ í™•ì¥.

        Args:
            change_set: ë³€ê²½ íŒŒì¼
            repo_id: ë ˆí¬ì§€í† ë¦¬ ID
            impact_result: GraphImpactAnalyzer.analyze_impact() ê²°ê³¼
            mode: ì¸ë±ì‹± ëª¨ë“œ

        Returns:
            í™•ì¥ëœ íŒŒì¼ ì§‘í•©
        """
        result = set(change_set.all_changed)

        if mode == IndexingMode.FAST:
            # Fast: ë³€ê²½ íŒŒì¼ë§Œ (impact ë¬´ì‹œ)
            return result

        elif mode == IndexingMode.BALANCED:
            # Balanced: ë³€ê²½ + direct affected íŒŒì¼
            result.update(impact_result.affected_files)
            # direct_affected ì‹¬ë³¼ì˜ íŒŒì¼ë§Œ í¬í•¨ (transitive ì œì™¸)
            logger.info(
                f"Balanced expansion with impact: "
                f"{len(change_set.all_changed)} â†’ {len(result)} files "
                f"(+{len(impact_result.direct_affected)} direct affected symbols)"
            )
            return result

        elif mode == IndexingMode.DEEP:
            # Deep: ë³€ê²½ + direct + transitive affected íŒŒì¼
            result.update(impact_result.affected_files)
            # transitive_affected ì‹¬ë³¼ì˜ íŒŒì¼ë„ í¬í•¨
            for affected in impact_result.transitive_affected:
                if hasattr(affected, "file_path") and affected.file_path:
                    result.add(affected.file_path)
            logger.info(
                f"Deep expansion with impact: "
                f"{len(change_set.all_changed)} â†’ {len(result)} files "
                f"(+{len(impact_result.direct_affected)} direct, "
                f"+{len(impact_result.transitive_affected)} transitive)"
            )
            return result

        else:
            return result

    async def expand_from_query(self, query_files: set[str], repo_id: str, total_files: int) -> set[str]:
        """
        ì¿¼ë¦¬ ê¸°ë°˜ on-demand Deep subset í™•ì¥.

        Args:
            query_files: ì¿¼ë¦¬ì—ì„œ ì¶”ì¶œí•œ íŒŒì¼/ì‹¬ë³¼ ê²½ë¡œ
            repo_id: ë ˆí¬ì§€í† ë¦¬ ID
            total_files: ì „ì²´ íŒŒì¼ ê°œìˆ˜

        Returns:
            Deep ë¶„ì„í•  íŒŒì¼ ì§‘í•©
        """
        max_files = min(
            ModeScopeLimit.DEEP_SUBSET_MAX_FILES,
            int(total_files * ModeScopeLimit.DEEP_SUBSET_MAX_PERCENT),
        )

        expanded = await self._expand_to_neighbors(
            query_files,
            repo_id,
            depth=ModeScopeLimit.DEEP_NEIGHBOR_DEPTH,
            max_files=max_files,
        )

        logger.info(f"Query-based Deep subset: {len(query_files)} â†’ {len(expanded)} files")
        return expanded
