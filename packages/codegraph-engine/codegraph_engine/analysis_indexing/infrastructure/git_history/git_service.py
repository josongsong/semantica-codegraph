"""
Git History Service

Provides git blame, commit history, and churn analysis using GitPython.

Phase: P0-1 Git History Analysis (Layer 19)
"""

import logging
import math
from datetime import datetime, timedelta
from pathlib import Path

try:
    from git import InvalidGitRepositoryError, Repo

    GIT_AVAILABLE = True
except ImportError:
    GIT_AVAILABLE = False
    Repo = None
    InvalidGitRepositoryError = Exception
    logging.warning("GitPython not installed. Git history features will be disabled.")

from codegraph_shared.common.observability import get_logger
from codegraph_engine.analysis_indexing.infrastructure.git_history.models import (
    AuthorContribution,
    ChangeType,
    ChunkChurnMetrics,
    FileHistory,
    GitBlame,
    GitCommit,
    HotspotReason,
)

logger = get_logger(__name__)


class GitService:
    """
    Service for extracting git history data.

    Provides:
    - Git blame (line-level attribution)
    - Commit history parsing
    - Churn metrics calculation
    - Hotspot detection
    """

    def __init__(self, repo_path: str | Path):
        """
        Initialize Git service.

        Args:
            repo_path: Path to git repository

        Raises:
            ValueError: If GitPython is not available
            InvalidGitRepositoryError: If path is not a git repo
        """
        if not GIT_AVAILABLE:
            raise ValueError("GitPython is required for git history features. Install with: pip install gitpython")

        self.repo_path = Path(repo_path)
        self.repo = Repo(str(self.repo_path))

        if self.repo.bare:
            raise ValueError(f"Repository at {repo_path} is bare (no working tree)")

        logger.info(f"Initialized GitService for repo: {self.repo_path}")

    def get_blame_for_file(self, file_path: str, rev: str = "HEAD") -> list[GitBlame]:
        """
        Get git blame information for a file.

        Args:
            file_path: Relative path to file from repo root
            rev: Git revision (default: HEAD)

        Returns:
            List of GitBlame entries (one per continuous attribution range)

        Example:
            >>> service = GitService("/path/to/repo")
            >>> blame_data = service.get_blame_for_file("src/main.py")
            >>> for blame in blame_data:
            ...     print(f"Lines {blame.start_line}-{blame.end_line}: {blame.author_email}")
        """
        try:
            # Get blame for file
            blame_list = self.repo.blame(rev, file_path)

            git_blame_entries = []
            current_line = 1

            for commit, lines in blame_list:
                line_count = len(lines)
                end_line = current_line + line_count - 1

                # Create GitBlame entry
                blame_entry = GitBlame(
                    id=None,  # Will be generated by database
                    repo_id=self.repo_path.name,
                    snapshot_id=rev,
                    file_path=file_path,
                    start_line=current_line,
                    end_line=end_line,
                    commit_hash=commit.hexsha,
                    author_name=commit.author.name,
                    author_email=commit.author.email,
                    commit_date=datetime.fromtimestamp(commit.committed_date),
                    chunk_id=None,  # Will be associated later
                )

                git_blame_entries.append(blame_entry)
                current_line = end_line + 1

            logger.info(f"Extracted {len(git_blame_entries)} blame entries for {file_path}")
            return git_blame_entries

        except Exception as e:
            logger.error(f"Failed to get blame for {file_path}: {e}")
            raise

    def get_commit(self, commit_hash: str) -> GitCommit:
        """
        Get commit metadata.

        Args:
            commit_hash: Git commit SHA

        Returns:
            GitCommit model
        """
        try:
            commit = self.repo.commit(commit_hash)

            # Parse parent commits
            parent_hashes = [parent.hexsha for parent in commit.parents]
            is_merge = len(parent_hashes) > 1

            # Extract message summary (first line)
            message_lines = commit.message.strip().split("\n")
            message_summary = message_lines[0] if message_lines else ""

            # Get commit statistics
            stats = commit.stats.total
            files_changed = stats.get("files", 0)
            insertions = stats.get("insertions", 0)
            deletions = stats.get("deletions", 0)

            return GitCommit(
                commit_hash=commit.hexsha,
                repo_id=self.repo_path.name,
                author_name=commit.author.name,
                author_email=commit.author.email,
                author_date=datetime.fromtimestamp(commit.authored_date),
                committer_name=commit.committer.name,
                committer_email=commit.committer.email,
                commit_date=datetime.fromtimestamp(commit.committed_date),
                message=commit.message.strip(),
                message_summary=message_summary,
                parent_hashes=parent_hashes,
                is_merge=is_merge,
                files_changed=files_changed,
                insertions=insertions,
                deletions=deletions,
                tags=[],  # Will be populated separately
                branches=[],  # Will be populated separately
            )

        except Exception as e:
            logger.error(f"Failed to get commit {commit_hash}: {e}")
            raise

    def get_file_history(self, file_path: str, max_commits: int | None = None) -> list[tuple[GitCommit, FileHistory]]:
        """
        Get commit history for a specific file.

        Args:
            file_path: Relative path to file from repo root
            max_commits: Maximum number of commits to retrieve (None = all)

        Returns:
            List of (GitCommit, FileHistory) tuples, ordered newest to oldest
        """
        try:
            commits = list(self.repo.iter_commits(paths=file_path, max_count=max_commits))
            history = []

            for i, commit in enumerate(commits):
                # Determine change type for this commit
                change_type = ChangeType.MODIFIED
                old_file_path = None

                # Check diffs to determine change type
                if i + 1 < len(commits):
                    parent = commits[i + 1]
                    diffs = parent.diff(commit, paths=file_path)

                    if diffs:
                        diff = diffs[0]

                        # Determine change type
                        if diff.new_file:
                            change_type = ChangeType.ADDED
                        elif diff.deleted_file:
                            change_type = ChangeType.DELETED
                        elif diff.renamed_file:
                            change_type = ChangeType.RENAMED
                            old_file_path = diff.rename_from
                        elif diff.copied_file:
                            change_type = ChangeType.COPIED

                        # Get line statistics
                        stats = diff.diff.decode("utf-8", errors="ignore")
                        lines_added = stats.count("\n+")
                        lines_deleted = stats.count("\n-")
                    else:
                        lines_added = 0
                        lines_deleted = 0
                else:
                    # First commit (file added)
                    change_type = ChangeType.ADDED
                    lines_added = 0
                    lines_deleted = 0

                # Create models
                git_commit = self.get_commit(commit.hexsha)

                file_history = FileHistory(
                    id=None,  # Will be generated by database
                    repo_id=self.repo_path.name,
                    file_path=file_path,
                    commit_hash=commit.hexsha,
                    change_type=change_type,
                    old_file_path=old_file_path,
                    lines_added=lines_added,
                    lines_deleted=lines_deleted,
                    file_size_bytes=None,  # Could be extracted from blob
                    language=None,  # Could be detected
                )

                history.append((git_commit, file_history))

            logger.info(f"Extracted {len(history)} commits for {file_path}")
            return history

        except Exception as e:
            logger.error(f"Failed to get file history for {file_path}: {e}")
            raise

    def calculate_churn_metrics(
        self, chunk_id: str, file_path: str, start_line: int, end_line: int
    ) -> ChunkChurnMetrics:
        """
        Calculate churn metrics for a chunk.

        Args:
            chunk_id: Chunk identifier
            file_path: File containing the chunk
            start_line: Start line of chunk
            end_line: End line of chunk

        Returns:
            ChunkChurnMetrics with aggregated statistics
        """
        try:
            # Get file history
            history = self.get_file_history(file_path)

            # Aggregate metrics
            total_commits = 0
            total_lines_added = 0
            total_lines_deleted = 0
            author_stats: dict[str, AuthorContribution] = {}

            first_commit = None
            last_commit = None

            for git_commit, file_history in history:
                # Filter by line range overlap
                if self._commit_affects_line_range(git_commit.commit_hash, file_path, start_line, end_line):
                    total_commits += 1
                    total_lines_added += file_history.lines_added
                    total_lines_deleted += file_history.lines_deleted

                    # Track authors
                    email = git_commit.author_email
                    if email not in author_stats:
                        author_stats[email] = AuthorContribution(
                            name=git_commit.author_name,
                            email=email,
                            commit_count=0,
                        )
                    author_stats[email].commit_count += 1

                    # Track first/last commits
                    if first_commit is None:
                        first_commit = git_commit
                    last_commit = git_commit

            # Calculate churn score
            total_lines = end_line - start_line + 1
            churn_score = self._calculate_churn_score(
                total_commits,
                total_lines_added + total_lines_deleted,
                total_lines,
                len(author_stats),
            )

            # Determine primary author (most commits)
            primary_author = None
            if author_stats:
                primary_author = max(author_stats.values(), key=lambda a: a.commit_count).email

            # Calculate age metrics
            age_days = None
            days_since_last_change = None
            if last_commit:
                age_days = (datetime.now() - last_commit.commit_date).days
            if first_commit:
                days_since_last_change = (datetime.now() - first_commit.commit_date).days

            # Hotspot detection
            is_hotspot = churn_score > 0.7
            hotspot_reason = None
            if is_hotspot:
                if churn_score > 0.8:
                    hotspot_reason = HotspotReason.HIGH_CHURN
                elif len(author_stats) >= 5:
                    hotspot_reason = HotspotReason.MANY_AUTHORS
                elif days_since_last_change is not None and days_since_last_change < 7:
                    hotspot_reason = HotspotReason.RECENT_ACTIVITY

            return ChunkChurnMetrics(
                chunk_id=chunk_id,
                repo_id=self.repo_path.name,
                total_commits=total_commits,
                total_lines_added=total_lines_added,
                total_lines_deleted=total_lines_deleted,
                total_lines_modified=total_lines_added + total_lines_deleted,
                churn_score=churn_score,
                primary_author=primary_author,
                author_count=len(author_stats),
                authors=list(author_stats.values()),
                first_commit_hash=last_commit.commit_hash if last_commit else None,
                first_commit_date=last_commit.commit_date if last_commit else None,
                last_commit_hash=first_commit.commit_hash if first_commit else None,
                last_commit_date=first_commit.commit_date if first_commit else None,
                age_days=age_days,
                days_since_last_change=days_since_last_change,
                is_hotspot=is_hotspot,
                hotspot_reason=hotspot_reason,
            )

        except Exception as e:
            logger.error(f"Failed to calculate churn metrics for {chunk_id}: {e}")
            raise

    def _calculate_churn_score(
        self,
        total_commits: int,
        total_lines_changed: int,
        total_lines: int,
        author_count: int,
    ) -> float:
        """
        Calculate normalized churn score (0-1).

        Formula:
            churn_score = normalize(
                (lines_changed / lines) *
                sqrt(commits) *
                log(authors + 1)
            )

        Args:
            total_commits: Number of commits
            total_lines_changed: Total lines added + deleted
            total_lines: Current total lines in chunk
            author_count: Number of distinct authors

        Returns:
            Churn score in range [0.0, 1.0]
        """
        if total_lines == 0 or total_commits == 0:
            return 0.0

        # Change rate (how much code changed relative to size)
        change_rate = min(total_lines_changed / total_lines, 10.0)  # Cap at 10x

        # Commit frequency factor (sqrt to dampen large values)
        commit_factor = math.sqrt(total_commits)

        # Author diversity factor (more authors = more complex)
        author_factor = math.log(author_count + 1, 2)

        # Combine factors
        raw_score = change_rate * commit_factor * author_factor

        # Normalize to [0, 1] using sigmoid-like function
        # Typical raw_score ranges from 0 to ~50 for high churn
        normalized = 1 / (1 + math.exp(-0.1 * (raw_score - 10)))

        return min(max(normalized, 0.0), 1.0)

    def get_recent_commits(self, days: int = 30, max_count: int = 100) -> list[GitCommit]:
        """
        Get recent commits from the repository.

        Args:
            days: Number of days to look back
            max_count: Maximum number of commits to return

        Returns:
            List of GitCommit models, ordered newest to oldest
        """
        try:
            since_date = datetime.now() - timedelta(days=days)
            commits = []

            for commit in self.repo.iter_commits(max_count=max_count):
                commit_date = datetime.fromtimestamp(commit.committed_date)
                if commit_date < since_date:
                    break

                commits.append(self.get_commit(commit.hexsha))

            logger.info(f"Retrieved {len(commits)} commits from last {days} days")
            return commits

        except Exception as e:
            logger.error(f"Failed to get recent commits: {e}")
            raise

    def get_file_authors(self, file_path: str) -> dict[str, AuthorContribution]:
        """
        Get all authors who contributed to a file.

        Args:
            file_path: Relative path to file from repo root

        Returns:
            Dictionary mapping email to AuthorContribution
        """
        try:
            history = self.get_file_history(file_path)
            author_stats: dict[str, AuthorContribution] = {}

            for git_commit, _ in history:
                email = git_commit.author_email
                if email not in author_stats:
                    author_stats[email] = AuthorContribution(
                        name=git_commit.author_name,
                        email=email,
                        commit_count=0,
                    )
                author_stats[email].commit_count += 1

            return author_stats

        except Exception as e:
            logger.error(f"Failed to get file authors for {file_path}: {e}")
            raise

    def _commit_affects_line_range(self, commit_hash: str, file_path: str, start_line: int, end_line: int) -> bool:
        """
        Check if a commit affects the specified line range.

        Uses git blame to determine if any line in the range was last modified by this commit.

        Args:
            commit_hash: Commit hash to check
            file_path: File path
            start_line: Start line (1-indexed)
            end_line: End line (1-indexed, inclusive)

        Returns:
            True if commit affects the line range
        """
        try:
            # Get blame for the line range
            blame = self.get_blame(file_path, start_line, end_line)

            # Check if any line was last modified by this commit
            for line_blame in blame.lines:
                if line_blame.commit_hash == commit_hash:
                    return True

            return False

        except Exception as e:
            # If blame fails, fall back to assuming the commit affects the range
            logger.debug(f"Blame failed for {file_path}:{start_line}-{end_line}, assuming affected: {e}")
            return True


# ============================================================
# Convenience Functions
# ============================================================


def create_git_service(repo_path: str | Path) -> GitService | None:
    """
    Create GitService with error handling.

    Args:
        repo_path: Path to git repository

    Returns:
        GitService instance or None if git is not available
    """
    if not GIT_AVAILABLE:
        logger.warning("GitPython not available, git history features disabled")
        return None

    try:
        return GitService(repo_path)
    except Exception as e:
        logger.error(f"Failed to create GitService for {repo_path}: {e}")
        return None
