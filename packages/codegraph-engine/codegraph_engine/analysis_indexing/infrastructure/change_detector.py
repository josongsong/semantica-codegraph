"""L0: ë³€ê²½ ê°ì§€ ë ˆì´ì–´."""

import hashlib
from dataclasses import dataclass
from pathlib import Path

from codegraph_shared.infra.observability import get_logger

logger = get_logger(__name__)


@dataclass
class ChangeSet:
    """ë³€ê²½ëœ íŒŒì¼ ì§‘í•©."""

    added: set[str]  # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒì¼
    modified: set[str]  # ìˆ˜ì •ëœ íŒŒì¼
    deleted: set[str]  # ì‚­ì œëœ íŒŒì¼
    renamed: dict[str, str] = None  # ë¦¬ë„¤ì„ëœ íŒŒì¼: {old_path: new_path}

    def __post_init__(self):
        """Initialize renamed dict if None."""
        if self.renamed is None:
            self.renamed = {}

    @property
    def all_changed(self) -> set[str]:
        """ëª¨ë“  ë³€ê²½ íŒŒì¼ (ì¶”ê°€ + ìˆ˜ì • + ë¦¬ë„¤ì„ëœ ìƒˆ íŒŒì¼)."""
        changed = self.added | self.modified
        # Renamed files: include new paths
        if self.renamed:
            changed.update(self.renamed.values())
        return changed

    @property
    def total_count(self) -> int:
        """ì „ì²´ ë³€ê²½ íŒŒì¼ ê°œìˆ˜."""
        return len(self.added) + len(self.modified) + len(self.deleted) + len(self.renamed)

    def is_empty(self) -> bool:
        """ë³€ê²½ì´ ì—†ëŠ”ì§€ í™•ì¸."""
        return self.total_count == 0

    def mark_as_renamed(self, old_path: str, new_path: str) -> None:
        """
        íŒŒì¼ì„ renamedë¡œ í‘œì‹œ.

        Args:
            old_path: ì´ì „ íŒŒì¼ ê²½ë¡œ
            new_path: ìƒˆ íŒŒì¼ ê²½ë¡œ
        """
        # renamed ì¶”ê°€
        self.renamed[old_path] = new_path

        # added/deletedì—ì„œ ì œê±°
        self.added.discard(new_path)
        self.deleted.discard(old_path)


class ChangeDetector:
    """ë³€ê²½ ê°ì§€ (L0 ë ˆì´ì–´)."""

    def __init__(
        self,
        git_helper=None,
        file_hash_store=None,
        rename_similarity_threshold: float = 0.90,
        enable_content_similarity: bool = True,
    ):
        """
        Args:
            git_helper: GitHelper ì¸ìŠ¤í„´ìŠ¤ (git diff ì‚¬ìš©)
            file_hash_store: íŒŒì¼ í•´ì‹œ ì €ì¥ì†Œ (mtime/hash ê¸°ë°˜ ê°ì§€)
            rename_similarity_threshold: Rename íŒì •ì„ ìœ„í•œ content similarity ì„ê³„ê°’ (0.90 = 90%)
            enable_content_similarity: Content similarity ê¸°ë°˜ rename detection í™œì„±í™” ì—¬ë¶€
        """
        self.git_helper = git_helper
        self.file_hash_store = file_hash_store
        self.rename_similarity_threshold = rename_similarity_threshold
        self.enable_content_similarity = enable_content_similarity

    def detect_changes(
        self,
        repo_path: Path,
        repo_id: str,
        base_commit: str | None = None,
        use_git: bool = True,
        use_mtime: bool = True,
        use_hash: bool = True,
    ) -> ChangeSet:
        """
        ë³€ê²½ íŒŒì¼ ê°ì§€ (L0).

        ì „ëµ:
        1. git diff (ë¹ ë¦„, ì •í™•)
        2. mtime (git ì—†ì„ ë•Œ)
        3. content hash (ìµœì¢… ê²€ì¦)

        Args:
            repo_path: ë ˆí¬ì§€í† ë¦¬ ê²½ë¡œ
            repo_id: ë ˆí¬ì§€í† ë¦¬ ID
            base_commit: ë¹„êµ ê¸°ì¤€ ì»¤ë°‹ (Noneì´ë©´ HEAD)
            use_git: git diff ì‚¬ìš© ì—¬ë¶€
            use_mtime: mtime ì²´í¬ ì‚¬ìš© ì—¬ë¶€
            use_hash: content hash ì²´í¬ ì‚¬ìš© ì—¬ë¶€

        Returns:
            ChangeSet (added, modified, deleted)
        """
        change_set = ChangeSet(added=set(), modified=set(), deleted=set())

        # 1. Git diff (ìš°ì„ )
        if use_git and self.git_helper:
            try:
                git_changes = self._detect_git_changes(repo_path, base_commit)
                change_set.added.update(git_changes.added)
                change_set.modified.update(git_changes.modified)
                change_set.deleted.update(git_changes.deleted)
                logger.info(
                    "git_diff_detected",
                    added=len(git_changes.added),
                    modified=len(git_changes.modified),
                    deleted=len(git_changes.deleted),
                )
            except Exception as e:
                logger.warning("git_diff_failed", error=str(e), fallback="mtime/hash")

        # 2. mtime + hash (git ì‹¤íŒ¨ ì‹œ ë˜ëŠ” ì¶”ê°€ ê²€ì¦)
        if (use_mtime or use_hash) and self.file_hash_store:
            try:
                hash_changes = self._detect_hash_changes(repo_path, repo_id, use_mtime, use_hash)
                # Gitê³¼ merge (union)
                change_set.added.update(hash_changes.added)
                change_set.modified.update(hash_changes.modified)
                change_set.deleted.update(hash_changes.deleted)
                logger.info(
                    "hash_mtime_detected",
                    added=len(hash_changes.added),
                    modified=len(hash_changes.modified),
                    deleted=len(hash_changes.deleted),
                )
            except Exception as e:
                logger.warning("hash_mtime_detection_failed", error=str(e))

        # 3. Content similarityë¡œ rename ê°ì§€ (Git ì—†ê±°ë‚˜ ì‹¤íŒ¨í–ˆì„ ë•Œ)
        if self.enable_content_similarity and (not use_git or not self.git_helper):
            change_set = self._detect_renames_by_similarity(repo_path, change_set)

        logger.info(
            "total_changes_detected",
            added=len(change_set.added),
            modified=len(change_set.modified),
            deleted=len(change_set.deleted),
            renamed=len(change_set.renamed),
            total=change_set.total_count,
        )

        return change_set

    def _detect_git_changes(self, repo_path: Path, base_commit: str | None) -> ChangeSet:
        """Git diff ê¸°ë°˜ ë³€ê²½ ê°ì§€."""
        if not self.git_helper:
            return ChangeSet(added=set(), modified=set(), deleted=set())

        # git diff --name-status
        diff_output = self.git_helper.get_diff_files(repo_path, base_commit)

        added = set()
        modified = set()
        deleted = set()

        for line in diff_output.splitlines():
            if not line.strip():
                continue

            parts = line.split("\t")
            if len(parts) < 2:
                continue

            status, file_path = parts[0], parts[1]

            if status == "A":
                added.add(file_path)
            elif status == "M":
                modified.add(file_path)
            elif status == "D":
                deleted.add(file_path)
            elif status.startswith("R"):  # Rename
                # R100 old_path new_path
                # Gitì´ renameì„ ê°ì§€í–ˆìœ¼ë©´ renamed dictì— ì €ì¥
                if len(parts) >= 3:
                    old_path = parts[1]
                    new_path = parts[2]
                    result = ChangeSet(added=added, modified=modified, deleted=deleted, renamed={old_path: new_path})
                    return result

        return ChangeSet(added=added, modified=modified, deleted=deleted, renamed={})

    def _detect_hash_changes(self, repo_path: Path, repo_id: str, use_mtime: bool, use_hash: bool) -> ChangeSet:
        """íŒŒì¼ í•´ì‹œ/mtime ê¸°ë°˜ ë³€ê²½ ê°ì§€."""
        if not self.file_hash_store:
            return ChangeSet(added=set(), modified=set(), deleted=set())

        added = set()
        modified = set()
        deleted = set()

        # í˜„ì¬ íŒŒì¼ ëª©ë¡
        current_files = {str(f.relative_to(repo_path)) for f in repo_path.rglob("*") if f.is_file()}

        # DBì—ì„œ ì´ì „ ìƒíƒœ ë¡œë“œ
        previous_state = self.file_hash_store.get_repo_state(repo_id)

        # ìƒˆë¡œ ì¶”ê°€ëœ íŒŒì¼
        new_files = current_files - previous_state.keys()
        added.update(new_files)

        # ì‚­ì œëœ íŒŒì¼
        removed_files = previous_state.keys() - current_files
        deleted.update(removed_files)

        # ê¸°ì¡´ íŒŒì¼ ì¤‘ ë³€ê²½ ì²´í¬
        for file_path in current_files & previous_state.keys():
            full_path = repo_path / file_path
            prev_state = previous_state[file_path]

            changed = False

            # mtime ì²´í¬
            if use_mtime:
                current_mtime = full_path.stat().st_mtime
                if current_mtime > prev_state.get("mtime", 0):
                    changed = True

            # hash ì²´í¬ (ë” ì •í™•)
            if use_hash and not changed:
                current_hash = self._compute_file_hash(full_path)
                if current_hash != prev_state.get("hash"):
                    changed = True

            if changed:
                modified.add(file_path)

        return ChangeSet(added=added, modified=modified, deleted=deleted)

    def _compute_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ content hash ê³„ì‚°."""
        hasher = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(8192), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception as e:
            logger.warning("hash_computation_failed", file_path=str(file_path), error=str(e))
            return ""

    def _detect_renames_by_similarity(
        self,
        repo_path: Path,
        change_set: ChangeSet,
    ) -> ChangeSet:
        """
        Content similarityë¡œ rename ê°ì§€ (SOTA - O(n) ìµœì í™”).

        ì „ëµ:
        1. Extensionìœ¼ë¡œ ë¨¼ì € ê·¸ë£¹í•‘ (O(n))
        2. ê°™ì€ extension ë‚´ì—ì„œë§Œ ë¹„êµ (O(kÂ²), këŠ” ê°™ì€ íƒ€ì… íŒŒì¼ ìˆ˜)
        3. file_hash_storeì—ì„œ deleted íŒŒì¼ ë‚´ìš© ë³µì›

        Args:
            repo_path: ë ˆí¬ì§€í† ë¦¬ ê²½ë¡œ
            change_set: ë³€ê²½ ì§‘í•©

        Returns:
            Renameì´ ê°ì§€ëœ ChangeSet
        """
        if not change_set.deleted or not change_set.added:
            return change_set

        logger.info(
            "rename_detection_started",
            deleted_count=len(change_set.deleted),
            added_count=len(change_set.added),
            threshold=self.rename_similarity_threshold,
        )

        # ğŸ”¥ O(n) ìµœì í™”: Extensionë³„ë¡œ ê·¸ë£¹í•‘
        deleted_by_ext: dict[str, list[str]] = {}
        added_by_ext: dict[str, list[str]] = {}

        for deleted_file in change_set.deleted:
            ext = Path(deleted_file).suffix or ".none"
            if ext not in deleted_by_ext:
                deleted_by_ext[ext] = []
            deleted_by_ext[ext].append(deleted_file)

        for added_file in change_set.added:
            ext = Path(added_file).suffix or ".none"
            if ext not in added_by_ext:
                added_by_ext[ext] = []
            added_by_ext[ext].append(added_file)

        # ğŸ”¥ ê°œì„ : file_hash_storeì—ì„œ deleted íŒŒì¼ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        deleted_metadata: dict[str, dict] = {}
        if self.file_hash_store:
            try:
                # Get deleted file metadata (size, hash, etc.)
                for deleted_file in change_set.deleted:
                    metadata = self.file_hash_store.get_file_metadata(deleted_file)
                    if metadata:
                        deleted_metadata[deleted_file] = metadata
                logger.debug(
                    "loaded_deleted_metadata",
                    count=len(deleted_metadata),
                )
            except Exception as e:
                logger.warning("failed_to_load_deleted_metadata", error=str(e))

        matched_renames: list[tuple[str, str, float]] = []  # (old_path, new_path, similarity)

        # Extensionë³„ë¡œ ë¹„êµ (O(kÂ²), këŠ” ê°™ì€ extension íŒŒì¼ ìˆ˜)
        for ext in added_by_ext.keys():
            if ext not in deleted_by_ext:
                continue  # ê°™ì€ extension ì—†ìœ¼ë©´ skip

            for added_file in added_by_ext[ext]:
                new_path = repo_path / added_file
                if not new_path.exists():
                    continue

                # Get new file metadata
                try:
                    new_stat = new_path.stat()
                    new_size = new_stat.st_size
                except Exception as e:
                    logger.debug("failed_to_stat_added_file", file=added_file, error=str(e))
                    continue

                best_match = None
                best_score = 0.0

                for deleted_file in deleted_by_ext[ext]:
                    # ğŸ”¥ Fast filter: Size similarity (Â±10%)
                    if deleted_file in deleted_metadata:
                        old_size = deleted_metadata[deleted_file].get("size", 0)
                        if old_size > 0:
                            size_ratio = min(new_size, old_size) / max(new_size, old_size)
                            if size_ratio < 0.90:  # Size ì°¨ì´ 10% ì´ìƒì´ë©´ skip
                                continue

                    # File name similarity (Jaccard on path components)
                    name_sim = self._filename_similarity(deleted_file, added_file)

                    if name_sim > best_score:
                        best_score = name_sim
                        best_match = deleted_file

                # Renameìœ¼ë¡œ ê°„ì£¼ (ì„ê³„ê°’ í†µê³¼)
                if best_score >= self.rename_similarity_threshold and best_match:
                    matched_renames.append((best_match, added_file, best_score))

        # ChangeSetì— rename ì ìš©
        for old_path, new_path, similarity in matched_renames:
            change_set.mark_as_renamed(old_path, new_path)
            logger.info(
                "rename_detected_by_similarity",
                old_path=old_path,
                new_path=new_path,
                similarity=f"{similarity:.2f}",
            )

        logger.info(
            "rename_detection_completed",
            renamed_count=len(matched_renames),
            optimization="O(kÂ²) per extension",
        )

        return change_set

    def _filename_similarity(self, path1: str, path2: str) -> float:
        """
        íŒŒì¼ëª… ìœ ì‚¬ë„ ê³„ì‚° (Jaccard similarity).

        Args:
            path1: íŒŒì¼ ê²½ë¡œ 1
            path2: íŒŒì¼ ê²½ë¡œ 2

        Returns:
            ìœ ì‚¬ë„ (0.0 ~ 1.0)
        """
        # Path componentsë¡œ í† í°í™”
        tokens1 = set(Path(path1).parts)
        tokens2 = set(Path(path2).parts)

        # Jaccard similarity
        intersection = tokens1 & tokens2
        union = tokens1 | tokens2

        if not union:
            return 0.0

        return len(intersection) / len(union)


# NOTE: íŒŒì¼ í•´ì‹œ ì €ì¥ì†ŒëŠ” content_hash_checker.pyì˜ HashStoreë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
# from codegraph_engine.analysis_indexing.infrastructure.content_hash_checker import HashStore, InMemoryHashStore, RedisHashStore
