"""
Expression Builder

Extracts expression entities from AST with Pyright type information.
"""

from collections import OrderedDict
from pathlib import Path
from typing import TYPE_CHECKING, Any

from codegraph_shared.common.observability import get_logger
from codegraph_engine.code_foundation.infrastructure.ir.models import Span
from codegraph_engine.code_foundation.infrastructure.semantic_ir.config import _DEBUG_ENABLED
from codegraph_engine.code_foundation.infrastructure.semantic_ir.expression.models import Expression, ExprKind
from codegraph_engine.code_foundation.infrastructure.semantic_ir.id_utils import parse_node_id

logger = get_logger(__name__)

if TYPE_CHECKING:
    from tree_sitter import Node as TSNode

    from codegraph_engine.code_foundation.infrastructure.ir.external_analyzers import ExternalAnalyzer
    from codegraph_engine.code_foundation.infrastructure.parsing import AstTree, SourceFile
    from codegraph_engine.code_foundation.infrastructure.semantic_ir.bfg.models import BasicFlowBlock


class LRUCache:
    """
    Simple LRU (Least Recently Used) cache implementation.

    When cache is full, evicts the least recently used item.
    Includes memory monitoring for safety.
    """

    def __init__(self, max_size: int = 100):
        """
        Initialize LRU cache.

        Args:
            max_size: Maximum number of items to cache

        Note:
            Each AST tree is typically 5-15MB in memory.
            max_size=100 could consume ~500MB-1.5GB.
            Aligned with BfgBuilder cache size for consistency.
        """
        self.max_size = max_size
        self._cache: OrderedDict = OrderedDict()
        self._eviction_count = 0
        self._hit_count = 0
        self._miss_count = 0

    def get(self, key):
        """
        Get value from cache and mark as recently used.

        Args:
            key: Cache key

        Returns:
            Cached value or None if not found
        """
        if key not in self._cache:
            self._miss_count += 1
            return None

        # Move to end (most recently used)
        self._cache.move_to_end(key)
        self._hit_count += 1
        return self._cache[key]

    def put(self, key, value):
        """
        Put value in cache, evicting LRU item if needed.

        Args:
            key: Cache key
            value: Value to cache
        """
        if key in self._cache:
            # Update existing, move to end
            self._cache.move_to_end(key)
        else:
            # Add new item
            self._cache[key] = value

            # Evict LRU if over capacity
            if len(self._cache) > self.max_size:
                self._cache.popitem(last=False)  # Remove oldest (first) item
                self._eviction_count += 1

    def __contains__(self, key):
        """Check if key exists in cache"""
        return key in self._cache

    def __setitem__(self, key, value):
        """Set item using [] syntax"""
        self.put(key, value)

    def __getitem__(self, key):
        """Get item using [] syntax"""
        result = self.get(key)
        if result is None:
            raise KeyError(key)
        return result

    def get_stats(self) -> dict:
        """
        Get cache statistics.

        Returns:
            Dictionary with cache stats
        """
        total_requests = self._hit_count + self._miss_count
        hit_rate = (self._hit_count / total_requests * 100) if total_requests > 0 else 0.0

        return {
            "size": len(self._cache),
            "max_size": self.max_size,
            "utilization": len(self._cache) / self.max_size if self.max_size > 0 else 0.0,
            "hits": self._hit_count,
            "misses": self._miss_count,
            "evictions": self._eviction_count,
            "hit_rate": hit_rate,
        }

    def clear(self):
        """Clear the cache and reset stats"""
        self._cache.clear()
        self._hit_count = 0
        self._miss_count = 0
        self._eviction_count = 0


class ExpressionBuilder:
    """
    Builds expression entities from AST.

    Extracts expression-level nodes and optionally enriches with Pyright type info.

    Memory Safety:
    - AST cache limited to 100 trees
    - Each tree ~5-15MB, total ~500MB-1.5GB
    - Cache stats available via get_cache_stats()
    - Aligned with BfgBuilder cache size for consistency

    Performance (SOTA):
    - Statement index cache: O(1) lookup per block instead of O(ast_nodes)
    - Builds statement index ONCE per file, queries for each block
    - 10-17x faster than naive AST traversal per block
    """

    def __init__(
        self,
        external_analyzer: "ExternalAnalyzer | None" = None,
        max_ast_cache_size: int = 100,
        project_root: Path | None = None,
        expression_analyzer: Any = None,  # v2: Language-specific analyzer (DI)
    ):
        """
        Initialize expression builder.

        Args:
            external_analyzer: Optional Pyright/LSP client for type inference
            max_ast_cache_size: Maximum number of AST trees to cache (default: 100, ~500MB-1.5GB)
                               Aligned with BfgBuilder for consistency
            project_root: Project root for resolving relative paths to absolute paths
                         Required for Pyright LSP queries (SOTA receiver type resolution)
            expression_analyzer: Language-specific expression analyzer (v2, optional)
                                If provided, delegates AST type checking to analyzer.
                                If None, uses legacy Python-hardcoded logic (backward compatibility).
        """
        from codegraph_engine.code_foundation.infrastructure.ir.external_analyzers.batch_lsp_fetcher import (
            BatchLSPFetcher,
        )
        from codegraph_engine.code_foundation.infrastructure.semantic_ir.expression.statement_index import (
            FileStatementIndexCache,
        )
        from codegraph_engine.code_foundation.infrastructure.semantic_ir.expression_arena import ExpressionArena
        from codegraph_engine.code_foundation.infrastructure.semantic_ir.typing.resolver import TypeResolver
        from codegraph_engine.code_foundation.infrastructure.semantic_ir.var_id_registry import VarIdRegistry

        self.pyright = external_analyzer
        self.project_root = project_root
        self._expr_counter = 0
        self._ast_cache = LRUCache(max_size=max_ast_cache_size)  # LRU cache for AST by file path

        # SOTA: Variable ID registry (Phase A)
        self._var_id_registry = VarIdRegistry()

        # SOTA: Expression Arena (SoA - Phase C)
        self._use_arena = True  # Toggle for A/B testing
        self._arena = ExpressionArena(var_id_registry=self._var_id_registry) if self._use_arena else None

        # v2: Language-specific expression analyzer (DI)
        # If provided, delegates statement processing to analyzer
        # If None, uses legacy Python-hardcoded logic for backward compatibility
        self._expression_analyzer = expression_analyzer
        # SOTA: Statement index cache for O(log n) lookup instead of O(ast_nodes)
        self._statement_index_cache = FileStatementIndexCache(max_size=max_ast_cache_size)
        # SOTA: TypeResolver for fast type resolution (before Pyright fallback)
        self._type_resolver = TypeResolver(repo_id="default")
        # SOTA: BatchLSPFetcher for parallel hover/definition calls (20-30x speedup)
        self._batch_lsp_fetcher: BatchLSPFetcher | None = None
        if external_analyzer:
            self._batch_lsp_fetcher = BatchLSPFetcher(external_analyzer, max_workers=32)

    def get_cache_stats(self) -> dict:
        """
        Get AST, statement index, and LSP batch cache statistics for monitoring.

        Returns:
            Dictionary with cache statistics for all caches
        """
        stats = {
            "ast_cache": self._ast_cache.get_stats(),
            "statement_index_cache": self._statement_index_cache.get_stats(),
        }
        if self._batch_lsp_fetcher:
            stats["batch_lsp_fetcher"] = self._batch_lsp_fetcher.stats
        return stats

    def clear_caches(self) -> None:
        """Clear all caches (AST, statement index, and reset LSP stats)."""
        self._ast_cache._cache.clear()
        self._statement_index_cache.clear()
        if self._batch_lsp_fetcher:
            self._batch_lsp_fetcher.reset_stats()

    def _get_cache_key(self, file_path: str) -> tuple[str, float]:
        """
        Generate cache key with file path and modification time.

        FIX: High #5 - Include file mtime to invalidate cache when file changes

        Args:
            file_path: Path to source file

        Returns:
            Tuple of (file_path, mtime)
        """
        import os

        try:
            mtime = os.path.getmtime(file_path)
        except (OSError, FileNotFoundError):
            # If file doesn't exist or can't get mtime, use 0
            # This ensures we don't cache non-existent files
            mtime = 0.0

        return (file_path, mtime)

    def build_from_block(
        self,
        block: "BasicFlowBlock",
        source_file: "SourceFile",
        ast_tree: "AstTree" = None,
        *,
        _defer_pyright: bool = False,
    ) -> list[Expression]:
        """
        Extract all expressions from a BFG block.

        Args:
            block: BFG block
            source_file: Source file
            ast_tree: Optional pre-parsed AST tree (avoids re-parsing, 60-70% improvement)
            _defer_pyright: Internal flag - if True, skip Pyright enrichment here.
                           Caller must call batch_enrich_expressions() afterward.

        Returns:
            List of expression entities
        """
        from codegraph_engine.code_foundation.infrastructure.parsing import AstTree

        if block.span is None:
            if _DEBUG_ENABLED:
                logger.debug(f"[ExprBuilder] Block {block.id} has no span, returning empty")
            return []

        try:
            # Get or parse AST (with caching)
            # FIX: High #5 - Include file mtime in cache key for proper invalidation
            file_path = source_file.file_path
            if _DEBUG_ENABLED:
                logger.debug(f"[ExprBuilder] Processing block {block.id}, span={block.span}")

            # Use pre-parsed AST if provided, otherwise parse/cache
            if ast_tree is not None:
                if _DEBUG_ENABLED:
                    logger.debug(f"[ExprBuilder] Using pre-parsed AST for {file_path} (avoid re-parsing)")
            else:
                # Create cache key with file path and modification time
                cache_key = self._get_cache_key(file_path)

                if cache_key not in self._ast_cache:
                    if _DEBUG_ENABLED:
                        logger.debug(f"[ExprBuilder] Parsing AST for {file_path}")
                    self._ast_cache[cache_key] = AstTree.parse(source_file)
                ast_tree = self._ast_cache[cache_key]

            # SOTA: Use statement index for O(log n) lookup instead of O(ast_nodes)
            # Build index once per file, query for each block
            statements = self._get_statements_via_index(ast_tree, file_path, block.span.start_line, block.span.end_line)
            if _DEBUG_ENABLED:
                logger.debug(f"[ExprBuilder] Found {len(statements)} statements in span (via index)")

            # RFC CRITICAL FIX: Extract repo_id from function_node_id using parse_node_id()
            parsed_func_id = parse_node_id(block.function_node_id)
            if parsed_func_id and parsed_func_id.is_valid:
                ctx_repo_id = parsed_func_id.repo_id
            else:
                # Fallback: try to extract from IR doc (should not happen)
                logger.warning(
                    "expression_build_invalid_function_id",
                    block_id=block.id,
                    function_node_id=block.function_node_id,
                )
                ctx_repo_id = "unknown"

            # Extract expressions from each statement (without Pyright)
            expressions: list[Expression] = []
            for stmt_node in statements:
                stmt_exprs = self.build_from_statement(
                    stmt_node=stmt_node,
                    block_id=block.id,
                    function_fqn=block.function_node_id,  # Use function_node_id as FQN
                    ctx_repo_id=ctx_repo_id,
                    ctx_file_path=source_file.file_path,
                    source_file=None,  # Defer Pyright enrichment
                )
                if _DEBUG_ENABLED:
                    logger.debug(f"[ExprBuilder] Statement {stmt_node.type} generated {len(stmt_exprs)} expressions")
                expressions.extend(stmt_exprs)

            if _DEBUG_ENABLED:
                logger.debug(f"[ExprBuilder] Total expressions extracted: {len(expressions)}")

            # Batch enrich with Pyright (skip if _defer_pyright=True for file-level batching)
            if self.pyright and expressions and not _defer_pyright:
                self._batch_enrich_with_pyright(expressions, source_file)

            return expressions

        except Exception as e:
            # FIX: Medium #7 - Add detailed error context for debugging
            logger.error(
                f"[ExprBuilder] FAILED to build expressions from block {block.id}: {e}\n"
                f"  Context:\n"
                f"    - File: {source_file.file_path}\n"
                f"    - Block span: {block.span}\n"
                f"    - Function: {block.function_node_id}\n"
                f"    - Block kind: {block.kind if hasattr(block, 'kind') else 'unknown'}\n"
                f"  Stack trace follows:",
                exc_info=True,
            )
            return []

    def batch_enrich_expressions(
        self,
        expressions: list[Expression],
        source_file: "SourceFile",
    ) -> None:
        """
        SOTA: File-level Pyright batch enrichment.

        Call this ONCE per file after collecting all expressions from all blocks.
        This reduces Pyright LSP calls from O(blocks) to O(1) per file.

        Performance:
        - Before: 57 blocks Ã— 13ms = 741ms per file
        - After: 1 call Ã— ~50ms = 50ms per file (15x faster)

        Args:
            expressions: All expressions from the file
            source_file: Source file for Pyright context
        """
        if self.pyright and expressions:
            self._batch_enrich_with_pyright(expressions, source_file)

    def _batch_enrich_with_pyright(
        self,
        expressions: list[Expression],
        source_file: "SourceFile",
    ):
        """
        Enrich multiple expressions with Pyright in batch.

        SOTA Optimization: Only query Pyright for expressions without inferred_type.
        This reduces Pyright calls by 80-90% when combined with fast type inference.

        Enhanced with:
        - Better type parsing from hover results
        - Definition location tracking for cross-file linking
        - Generic type parameter extraction
        - ğŸ”¥ SOTA: Receiver type resolution for method calls

        Args:
            expressions: List of expressions to enrich
            source_file: Source file
        """
        if not self.pyright:
            return

        # SOTA: Filter to only expressions that need Pyright (no inferred_type yet)
        unresolved_expressions = [e for e in expressions if not e.inferred_type]

        if not unresolved_expressions:
            if _DEBUG_ENABLED:
                logger.debug(f"[ExprBuilder] All {len(expressions)} expressions already have types, skipping Pyright")
            return

        if _DEBUG_ENABLED:
            logger.debug(
                f"[ExprBuilder] Pyright fallback for {len(unresolved_expressions)}/{len(expressions)} "
                f"unresolved expressions ({100 - len(unresolved_expressions) * 100 // max(len(expressions), 1)}% resolved locally)"
            )

        # ğŸ”¥ SOTA FIX: Resolve to absolute path for Pyright LSP
        # source_file.file_path is relative, Pyright needs absolute path
        if unresolved_expressions:
            rel_path = Path(unresolved_expressions[0].file_path)
            if self.project_root and not rel_path.is_absolute():
                abs_file_path = self.project_root / rel_path
            else:
                abs_file_path = rel_path
        else:
            return

        # SOTA OPTIMIZATION: Collect ALL positions (expression + receiver) for batch hover
        # This reduces hover() calls by batching receiver positions together

        # Group expressions by unique (line, col) to avoid duplicate queries
        unique_positions: dict[tuple[int, int], list[Expression]] = {}
        # Track receiver positions separately: (line, col) -> list of (expr, attr_key)
        receiver_positions: dict[tuple[int, int], list[tuple[Expression, str]]] = {}

        for expr in unresolved_expressions:
            if expr.span:
                pos = (expr.span.start_line, expr.span.start_col)
                if pos not in unique_positions:
                    unique_positions[pos] = []
                unique_positions[pos].append(expr)

            # Collect receiver positions for CALL expressions
            if expr.kind == ExprKind.CALL and "receiver_span" in expr.attrs:
                receiver_span = expr.attrs["receiver_span"]
                recv_pos = (receiver_span["line"], receiver_span["col"])
                if recv_pos not in receiver_positions:
                    receiver_positions[recv_pos] = []
                receiver_positions[recv_pos].append((expr, "receiver_type"))

        # Merge unique positions: all positions we need to query
        all_positions = list(set(unique_positions.keys()) | set(receiver_positions.keys()))

        # SOTA: Use BatchLSPFetcher for parallel hover + definition fetching
        # Before: hover parallel, definition sequential (10+ seconds)
        # After: both parallel via BatchLSPFetcher (~0.5-1 second, 20-30x speedup)
        from codegraph_engine.code_foundation.domain.ports.lsp_ports import LSPOperationType, LSPPosition

        # Build position list for batch fetcher
        lsp_positions = [LSPPosition(line=pos[0], column=pos[1]) for pos in all_positions]

        # Collect positions that need definition lookup (CALL/NAME_LOAD/ATTRIBUTE only)
        definition_positions: set[LSPPosition] = set()
        for (line, col), exprs_at_pos in unique_positions.items():
            if any(expr.kind in (ExprKind.CALL, ExprKind.NAME_LOAD, ExprKind.ATTRIBUTE) for expr in exprs_at_pos):
                definition_positions.add(LSPPosition(line=line, column=col))

        # Fetch hover for ALL positions, definition only for those that need it
        hover_cache: dict[tuple[int, int], dict] = {}
        definition_cache: dict[tuple[int, int], dict] = {}

        if self._batch_lsp_fetcher:
            # SOTA: Use BatchLSPFetcher for parallel execution
            # Fetch hover for all positions
            hover_results = self._batch_lsp_fetcher.fetch_hover_batch(abs_file_path, lsp_positions)
            for pos, result in hover_results.items():
                if result.success:
                    hover_cache[(pos.line, pos.column)] = result.raw

            # Fetch definition only for positions that need it
            if definition_positions:
                def_results = self._batch_lsp_fetcher.fetch_definition_batch(abs_file_path, list(definition_positions))
                for pos, result in def_results.items():
                    if result.success:
                        definition_cache[(pos.line, pos.column)] = {
                            "file": result.file,
                            "line": result.line,
                            "fqn": result.fqn,
                        }
        else:
            # Fallback: Sequential fetching (no BatchLSPFetcher)
            for pos in all_positions:
                try:
                    hover_info = self.pyright.hover(abs_file_path, pos[0], pos[1])
                    if hover_info:
                        hover_cache[pos] = hover_info
                except Exception as e:
                    if _DEBUG_ENABLED:
                        logger.debug(f"Pyright hover failed at {pos[0]}:{pos[1]}: {e}")

            for pos in definition_positions:
                try:
                    definition = self.pyright.definition(abs_file_path, pos.line, pos.column)
                    if definition:
                        definition_cache[(pos.line, pos.column)] = definition
                except Exception:
                    pass

        enriched_count = 0
        definition_linked = 0
        receiver_types_resolved = 0

        # Process expression positions (type + definition)
        for (line, col), exprs_at_pos in unique_positions.items():
            hover_info = hover_cache.get((line, col))

            if hover_info:
                inferred_type = hover_info.get("type")
                if inferred_type:
                    normalized_type = self._normalize_pyright_type(inferred_type)
                    for expr in exprs_at_pos:
                        expr.inferred_type = normalized_type
                        enriched_count += 1
                        if "[" in normalized_type:
                            expr.attrs["generic_params"] = self._extract_generic_params(normalized_type)

            # Apply definition from cache (already fetched in parallel)
            definition = definition_cache.get((line, col))
            if definition:
                for expr in exprs_at_pos:
                    if expr.kind in (ExprKind.CALL, ExprKind.NAME_LOAD, ExprKind.ATTRIBUTE):
                        expr.attrs["definition_file"] = definition.get("file")
                        expr.attrs["definition_line"] = definition.get("line")
                        expr.attrs["definition_fqn"] = definition.get("fqn")
                        definition_linked += 1

        # Process receiver positions (already fetched in hover_cache)
        for recv_pos, expr_list in receiver_positions.items():
            hover_info = hover_cache.get(recv_pos)
            if hover_info:
                receiver_type = hover_info.get("type")
                if receiver_type:
                    normalized_receiver = self._normalize_pyright_type(receiver_type)
                    for expr, attr_key in expr_list:
                        expr.attrs[attr_key] = normalized_receiver
                        receiver_types_resolved += 1
                        if _DEBUG_ENABLED:
                            logger.debug(
                                f"[ExprBuilder] Resolved receiver type: "
                                f"{expr.attrs.get('receiver_name')} -> {normalized_receiver}"
                            )

        if _DEBUG_ENABLED:
            logger.debug(
                f"[ExprBuilder] Pyright enrichment: {enriched_count} types, "
                f"{definition_linked} definitions linked, {receiver_types_resolved} receiver types resolved"
            )

    def _normalize_pyright_type(self, type_str: str) -> str:
        """
        Normalize Pyright type string for consistent matching.

        Handles:
        - Module prefix removal (e.g., "builtins.str" -> "str")
        - Literal type simplification
        - Union type formatting

        Args:
            type_str: Raw type string from Pyright

        Returns:
            Normalized type string
        """
        import re

        if not type_str:
            return ""

        normalized = type_str.strip()

        # Remove builtins prefix
        normalized = re.sub(r"\bbuiltins\.", "", normalized)

        # Remove typing module prefix for common types
        normalized = re.sub(r"\btyping\.(List|Dict|Set|Tuple|Optional|Union|Callable)\b", r"\1", normalized)

        # Simplify Literal types: Literal["foo"] -> str (for matching purposes)
        # But preserve the full type for attrs
        if normalized.startswith("Literal["):
            # Keep original for precise matching
            pass

        # Normalize whitespace
        normalized = re.sub(r"\s+", " ", normalized)
        normalized = re.sub(r"\s*\[\s*", "[", normalized)
        normalized = re.sub(r"\s*\]\s*", "]", normalized)
        normalized = re.sub(r"\s*,\s*", ", ", normalized)
        normalized = re.sub(r"\s*\|\s*", " | ", normalized)

        return normalized

    def _extract_generic_params(self, type_str: str) -> list[str]:
        """
        Extract generic type parameters from type string.

        Args:
            type_str: Type string (e.g., "List[str]", "Dict[str, int]")

        Returns:
            List of parameter type strings
        """
        if "[" not in type_str:
            return []

        # Extract content between brackets
        start = type_str.index("[")
        end = type_str.rindex("]")
        params_str = type_str[start + 1 : end]

        # Split respecting nested brackets
        params = []
        current = []
        depth = 0

        for char in params_str:
            if char == "[":
                depth += 1
                current.append(char)
            elif char == "]":
                depth -= 1
                current.append(char)
            elif char == "," and depth == 0:
                param = "".join(current).strip()
                if param:
                    params.append(param)
                current = []
            else:
                current.append(char)

        # Add last parameter
        param = "".join(current).strip()
        if param:
            params.append(param)

        return params

    def build_from_statement(
        self,
        stmt_node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None" = None,
    ) -> list[Expression]:
        """
        Extract all expressions from a statement.

        v2: Delegates to expression_analyzer if provided (multi-language support).
        Otherwise uses legacy Python-hardcoded logic (backward compatibility).

        Args:
            stmt_node: Statement AST node
            block_id: CFG block ID
            function_fqn: Function FQN (None for module-level)
            ctx_repo_id: Repository ID
            ctx_file_path: File path
            source_file: Source file (for Pyright queries)

        Returns:
            List of expression entities
        """
        # v2: Delegate to language-specific analyzer if available
        if self._expression_analyzer:
            try:
                return self._expression_analyzer.process_statement(
                    stmt_node=stmt_node,
                    block_id=block_id,
                    function_fqn=function_fqn,
                    repo_id=ctx_repo_id,
                    file_path=ctx_file_path,
                )
            except Exception as e:
                logger.warning(
                    "expression_analyzer_failed",
                    analyzer=self._expression_analyzer.__class__.__name__,
                    error=str(e),
                )
                # Fallback to legacy logic

        # Legacy: Python-hardcoded logic (backward compatibility)
        expressions: list[Expression] = []

        # Special handling for assignment statements
        # Python tree-sitter wraps assignments in "expression_statement", so check both
        if stmt_node.type == "assignment":
            return self._handle_assignment(stmt_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file)

        # Handle augmented assignment (x += func())
        if stmt_node.type == "augmented_assignment":
            return self._handle_augmented_assignment(
                stmt_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
            )

        # Check if this is an expression_statement containing an assignment or standalone call
        if stmt_node.type == "expression_statement":
            for child in stmt_node.children:
                if child.type == "assignment":
                    return self._handle_assignment(
                        child, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
                    )
                if child.type == "augmented_assignment":
                    return self._handle_augmented_assignment(
                        child, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
                    )
                # SOTA: Handle standalone call (e.g., os.system(cmd))
                if child.type == "call":
                    return self._handle_standalone_call(
                        child, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
                    )

        # Special handling for return statements
        if stmt_node.type == "return_statement":
            return self._handle_return_statement(
                stmt_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
            )

        # Handle for statement (for item in source():)
        if stmt_node.type == "for_statement":
            return self._handle_for_statement(
                stmt_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
            )

        # Handle with statement (with source() as f:)
        if stmt_node.type == "with_statement":
            return self._handle_with_statement(
                stmt_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
            )

        # Handle if statement (for walrus operator: if (x := input()): ...)
        if stmt_node.type == "if_statement":
            return self._handle_if_statement(stmt_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file)

        # Handle while statement (for walrus operator: while (x := input()): ...)
        if stmt_node.type == "while_statement":
            return self._handle_while_statement(
                stmt_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
            )

        # SOTA: Handle standalone call at module level (tree-sitter Python quirk)
        # At module level, `os.system(cmd)` appears as `call` node directly, not wrapped in expression_statement
        if stmt_node.type == "call":
            return self._handle_standalone_call(
                stmt_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
            )

        def traverse(node: "TSNode", parent_expr_id: str | None = None):
            """Recursively traverse AST and extract expressions"""
            expr = self._create_expression(node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file)

            if expr:
                # Link parent-child relationship
                if parent_expr_id:
                    expr.parent_expr_id = parent_expr_id

                expressions.append(expr)
                current_expr_id = expr.id
            else:
                current_expr_id = parent_expr_id

            # Recurse to children
            for child in node.children:
                traverse(child, current_expr_id)

        traverse(stmt_node, None)
        return expressions

    def _handle_assignment(
        self,
        assignment_node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None" = None,
    ) -> list[Expression]:
        """
        Handle assignment statement specially to set defines_var.

        Refactored to use helper methods for better maintainability.
        Reduced from 161 lines â†’ ~50 lines (69% reduction).

        Strategy:
        1. Find left and right children
        2. Process right side (reads)
        3. Process left side by type:
           - Simple identifier assignment
           - Tuple unpacking assignment
           - Complex assignment (attribute, subscript)

        Args:
            assignment_node: Assignment AST node
            block_id: CFG block ID
            function_fqn: Function FQN
            ctx_repo_id: Repository ID
            ctx_file_path: File path
            source_file: Source file

        Returns:
            List of expression entities
        """
        expressions: list[Expression] = []

        # Step 1: Find left and right children
        left_node = None
        right_node = None

        for child in assignment_node.children:
            if child.type == "=":
                continue
            elif left_node is None:
                left_node = child
            else:
                right_node = child
                break

        # Step 2: Process right side first (reads)
        if right_node:
            right_expressions = self._process_right_side_expressions(
                right_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
            )
            expressions.extend(right_expressions)

        # Step 3: Process left side (defines_var) by type
        if left_node:
            if left_node.type == "identifier":
                # Simple assignment: result = x
                # ğŸ”¥ Set defines_var on the appropriate right side expression
                target_var_name = left_node.text.decode("utf-8") if left_node.text else None
                if target_var_name and right_expressions:
                    # For binary_operator (e.g., query = 'SELECT ' + input()),
                    # set defines_var on ALL expressions that contribute to taint flow
                    # This includes both the BIN_OP and any CALL inside it
                    top_expr = right_expressions[0]

                    # Always set on top-level expression
                    if not top_expr.defines_var:
                        top_expr.defines_var = target_var_name

                    # Also set on any CALL expressions (for taint tracking)
                    # This ensures `input()` in `x = 'a' + input()` gets defines_var
                    for expr in right_expressions:
                        if expr.kind == ExprKind.CALL and not expr.defines_var:
                            expr.defines_var = target_var_name

                    if _DEBUG_ENABLED:
                        logger.debug(f"[ExprBuilder] Set defines_var={target_var_name} on {top_expr.id}")

                expr = self._create_simple_assignment(
                    left_node, expressions, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
                )
                if expr:
                    expressions.append(expr)

            elif left_node.type in ("pattern_list", "tuple_pattern", "list_pattern"):
                # Tuple unpacking assignment: a, b = get_pair()
                # For taint tracking, set defines_var to the tuple representation
                # e.g., "a, b" for `a, b = get_pair()`
                target_names = []
                for child in left_node.children:
                    if child.type == "identifier":
                        name = child.text.decode("utf-8") if child.text else None
                        if name:
                            target_names.append(name)

                if target_names and right_expressions:
                    # Use comma-separated names as defines_var
                    tuple_var_name = ", ".join(target_names)
                    top_expr = right_expressions[0]
                    if not top_expr.defines_var:
                        top_expr.defines_var = tuple_var_name

                    for expr in right_expressions:
                        if expr.kind == ExprKind.CALL and not expr.defines_var:
                            expr.defines_var = tuple_var_name

                tuple_expressions = self._create_tuple_unpacking_assignments(
                    left_node, expressions, block_id, function_fqn, ctx_repo_id, ctx_file_path
                )
                expressions.extend(tuple_expressions)

            else:
                # Complex assignment (attribute, subscript, etc.)
                complex_expressions = self._create_complex_assignment(
                    left_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
                )
                expressions.extend(complex_expressions)

        return expressions

    def _handle_return_statement(
        self,
        return_node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None" = None,
    ) -> list[Expression]:
        """
        Handle return statement specially to mark expressions with is_return.

        Return statement structure:
        - return_statement
          - "return" (keyword)
          - expression (the returned value)

        Args:
            return_node: Return statement AST node
            block_id: CFG block ID
            function_fqn: Function FQN
            ctx_repo_id: Repository ID
            ctx_file_path: File path
            source_file: Source file

        Returns:
            List of expressions with is_return marking
        """
        expressions: list[Expression] = []

        # Find the returned expression (skip "return" keyword)
        return_expr_node = None
        for child in return_node.children:
            if child.type != "return":
                return_expr_node = child
                break

        if return_expr_node is None:
            # Empty return statement
            return expressions

        # Extract expressions from the returned value
        def traverse(node: "TSNode", parent_expr_id: str | None = None):
            expr = self._create_expression(node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file)
            if expr:
                # Mark as return expression
                expr.attrs["is_return"] = True

                if parent_expr_id:
                    expr.parent_expr_id = parent_expr_id
                expressions.append(expr)
                current_expr_id = expr.id
            else:
                current_expr_id = parent_expr_id

            for child in node.children:
                traverse(child, current_expr_id)

        traverse(return_expr_node, None)

        return expressions

    def _handle_augmented_assignment(
        self,
        aug_node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None" = None,
    ) -> list[Expression]:
        """
        Handle augmented assignment (x += func(), x -= val, etc.).

        AST structure:
        - augmented_assignment
          - identifier (target variable)
          - += / -= / etc. (operator)
          - expression (right side value)

        For taint tracking:
        - x += input() means x is BOTH read AND written
        - The CALL expression should have defines_var = 'x'

        Args:
            aug_node: Augmented assignment AST node
            block_id: CFG block ID
            function_fqn: Function FQN
            ctx_repo_id: Repository ID
            ctx_file_path: File path
            source_file: Source file

        Returns:
            List of expression entities
        """
        expressions: list[Expression] = []

        # Find left (target) and right (value) nodes
        # Left can be: identifier (x), attribute (self.x), subscript (arr[0])
        left_node = None
        right_node = None
        operator = None
        aug_operators = ("+=", "-=", "*=", "/=", "//=", "%=", "**=", "&=", "|=", "^=", ">>=", "<<=")

        for child in aug_node.children:
            if child.type in ("identifier", "attribute", "subscript") and left_node is None:
                left_node = child
            elif child.type in aug_operators:
                operator = child.type
            elif left_node is not None and operator is not None:
                right_node = child
                break

        # Process right side (reads)
        right_expressions: list[Expression] = []
        if right_node:
            right_expressions = self._process_right_side_expressions(
                right_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
            )
            expressions.extend(right_expressions)

        # Extract target variable name based on left node type
        target_var_name = None
        if left_node:
            if left_node.type == "identifier":
                target_var_name = left_node.text.decode("utf-8") if left_node.text else None
            elif left_node.type == "attribute":
                # self.x -> use full "self.x" as target for tracking
                target_var_name = left_node.text.decode("utf-8") if left_node.text else None
            elif left_node.type == "subscript":
                # arr[0] -> use "arr" as the base variable
                for child in left_node.children:
                    if child.type == "identifier":
                        target_var_name = child.text.decode("utf-8") if child.text else None
                        break

        # Set defines_var on right side expressions
        if target_var_name and right_expressions:
            # Set on top-level and all CALLs (same logic as regular assignment)
            top_expr = right_expressions[0]
            if not top_expr.defines_var:
                top_expr.defines_var = target_var_name

            for expr in right_expressions:
                if expr.kind == ExprKind.CALL and not expr.defines_var:
                    expr.defines_var = target_var_name

            # Also add the target variable to reads_vars (x += y means x is read too)
            # For attribute (self.x), add the base object (self) to reads
            base_var = target_var_name.split(".")[0] if "." in target_var_name else target_var_name
            for expr in right_expressions:
                if base_var not in expr.reads_vars:
                    expr.reads_vars.append(base_var)

        return expressions

    def _handle_standalone_call(
        self,
        call_node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None" = None,
    ) -> list[Expression]:
        """
        Handle standalone function call (e.g., os.system(cmd), print(x)).

        SOTA: Module-level and function-level standalone calls need proper
        expression extraction for taint analysis.

        Args:
            call_node: Call AST node
            block_id: CFG block ID
            function_fqn: Function FQN
            ctx_repo_id: Repository ID
            ctx_file_path: File path
            source_file: Source file

        Returns:
            List of expressions including the call and its arguments
        """
        expressions: list[Expression] = []

        def traverse(node: "TSNode", parent_expr_id: str | None = None):
            """Recursively traverse AST and extract expressions"""
            expr = self._create_expression(node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file)

            if expr:
                if parent_expr_id:
                    expr.parent_expr_id = parent_expr_id
                expressions.append(expr)
                current_expr_id = expr.id
            else:
                current_expr_id = parent_expr_id

            for child in node.children:
                traverse(child, current_expr_id)

        traverse(call_node, None)
        return expressions

    def _handle_for_statement(
        self,
        for_node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None" = None,
    ) -> list[Expression]:
        """
        Handle for statement to track taint from iterable to loop variable.

        AST structure:
        - for_statement
          - "for"
          - identifier (target: item)
          - "in"
          - call/expression (iterable: get_items())
          - ":"
          - block

        For taint tracking:
        - for item in get_items(): means item gets values from get_items()
        - So get_items() should have defines_var = 'item'
        """
        expressions: list[Expression] = []

        # Find target (loop variable) and iterable
        target_node = None
        iterable_node = None
        found_in = False

        for child in for_node.children:
            if child.type == "for":
                continue
            elif child.type in ("identifier", "pattern_list", "tuple_pattern") and not found_in:
                target_node = child
            elif child.type == "in":
                found_in = True
            elif found_in and child.type not in (":", "block", "comment"):
                iterable_node = child
                break

        # Process iterable expressions
        if iterable_node:
            iterable_expressions = self._process_right_side_expressions(
                iterable_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
            )
            expressions.extend(iterable_expressions)

        # Extract target variable name
        target_var_name = None
        if target_node:
            if target_node.type == "identifier":
                target_var_name = target_node.text.decode("utf-8") if target_node.text else None
            elif target_node.type in ("pattern_list", "tuple_pattern"):
                # Tuple unpacking: for k, v in items
                target_names = []
                for child in target_node.children:
                    if child.type == "identifier":
                        name = child.text.decode("utf-8") if child.text else None
                        if name:
                            target_names.append(name)
                if target_names:
                    target_var_name = ", ".join(target_names)

        # Set defines_var on iterable expressions
        if target_var_name and iterable_expressions:
            top_expr = iterable_expressions[0]
            if not top_expr.defines_var:
                top_expr.defines_var = target_var_name

            for expr in iterable_expressions:
                if expr.kind == ExprKind.CALL and not expr.defines_var:
                    expr.defines_var = target_var_name

        return expressions

    def _handle_with_statement(
        self,
        with_node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None" = None,
    ) -> list[Expression]:
        """
        Handle with statement to track taint from context manager to alias.

        AST structure:
        - with_statement
          - "with"
          - with_clause
            - with_item
              - as_pattern
                - call (context manager: open(f))
                - "as"
                - as_pattern_target
                  - identifier (alias: handle)
          - ":"
          - block

        For taint tracking:
        - with open(user_input) as f: means f gets value from open()
        - So open() should have defines_var = 'f'
        """
        expressions: list[Expression] = []

        # Find context manager expression and alias
        def find_with_items(node):
            """Recursively find with_item nodes"""
            items = []
            if node.type == "with_item":
                items.append(node)
            for child in node.children:
                items.extend(find_with_items(child))
            return items

        with_items = find_with_items(with_node)

        for with_item in with_items:
            context_expr = None
            alias_name = None

            # Find as_pattern or direct expression
            for child in with_item.children:
                if child.type == "as_pattern":
                    # Has alias: with expr as name
                    for as_child in child.children:
                        if as_child.type in ("call", "identifier", "attribute"):
                            context_expr = as_child
                        elif as_child.type == "as_pattern_target":
                            for target_child in as_child.children:
                                if target_child.type == "identifier":
                                    alias_name = target_child.text.decode("utf-8") if target_child.text else None
                elif child.type in ("call", "identifier", "attribute"):
                    # No alias: with expr
                    context_expr = child

            # Process context manager expression
            if context_expr:
                ctx_expressions = self._process_right_side_expressions(
                    context_expr, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
                )
                expressions.extend(ctx_expressions)

                # Set defines_var if there's an alias
                if alias_name and ctx_expressions:
                    top_expr = ctx_expressions[0]
                    if not top_expr.defines_var:
                        top_expr.defines_var = alias_name

                    for expr in ctx_expressions:
                        if expr.kind == ExprKind.CALL and not expr.defines_var:
                            expr.defines_var = alias_name

        return expressions

    def _handle_if_statement(
        self,
        if_node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None" = None,
    ) -> list[Expression]:
        """
        Handle if statement to extract walrus operator (named_expression).

        Pattern: if (x := input()): ...
        AST: if_statement > parenthesized_expression > named_expression > [identifier, :=, call]

        The walrus operator defines a variable within the condition.
        """
        expressions: list[Expression] = []

        def find_named_expressions(node) -> list["TSNode"]:
            """Recursively find named_expression nodes"""
            results = []
            if node.type == "named_expression":
                results.append(node)
            for child in node.children:
                results.extend(find_named_expressions(child))
            return results

        # Find all named_expression in the condition
        named_exprs = find_named_expressions(if_node)

        for named_expr in named_exprs:
            target_name = None
            value_node = None

            # Parse named_expression: identifier := value
            found_walrus = False
            for child in named_expr.children:
                if child.type == "identifier" and not found_walrus:
                    target_name = child.text.decode("utf-8") if child.text else None
                elif child.type == ":=":
                    found_walrus = True
                elif found_walrus:
                    value_node = child
                    break

            # Process value expression
            if value_node:
                value_expressions = self._process_right_side_expressions(
                    value_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
                )
                expressions.extend(value_expressions)

                # Set defines_var on value expressions
                if target_name and value_expressions:
                    top_expr = value_expressions[0]
                    if not top_expr.defines_var:
                        top_expr.defines_var = target_name

                    for expr in value_expressions:
                        if expr.kind == ExprKind.CALL and not expr.defines_var:
                            expr.defines_var = target_name

        # Also process the rest of the if statement via normal traversal
        # (but skip named_expression children as we handled them)
        def traverse_except_named(node, parent_expr_id=None):
            if node.type == "named_expression":
                return  # Already handled
            if node.type == "block":
                return  # Don't descend into body blocks

            expr = self._create_expression(node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file)

            if expr:
                if parent_expr_id:
                    expr.parent_expr_id = parent_expr_id
                expressions.append(expr)
                current_id = expr.id
            else:
                current_id = parent_expr_id

            for child in node.children:
                traverse_except_named(child, current_id)

        traverse_except_named(if_node)

        return expressions

    def _handle_while_statement(
        self,
        while_node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None" = None,
    ) -> list[Expression]:
        """
        Handle while statement to extract walrus operator (named_expression).

        Pattern: while (line := readline()): ...
        AST: while_statement > parenthesized_expression > named_expression > [identifier, :=, call]

        Similar to if_statement handling.
        """
        expressions: list[Expression] = []

        def find_named_expressions(node) -> list["TSNode"]:
            """Recursively find named_expression nodes"""
            results = []
            if node.type == "named_expression":
                results.append(node)
            for child in node.children:
                results.extend(find_named_expressions(child))
            return results

        # Find all named_expression in the condition
        named_exprs = find_named_expressions(while_node)

        for named_expr in named_exprs:
            target_name = None
            value_node = None

            # Parse named_expression: identifier := value
            found_walrus = False
            for child in named_expr.children:
                if child.type == "identifier" and not found_walrus:
                    target_name = child.text.decode("utf-8") if child.text else None
                elif child.type == ":=":
                    found_walrus = True
                elif found_walrus:
                    value_node = child
                    break

            # Process value expression
            if value_node:
                value_expressions = self._process_right_side_expressions(
                    value_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file
                )
                expressions.extend(value_expressions)

                # Set defines_var on value expressions
                if target_name and value_expressions:
                    top_expr = value_expressions[0]
                    if not top_expr.defines_var:
                        top_expr.defines_var = target_name

                    for expr in value_expressions:
                        if expr.kind == ExprKind.CALL and not expr.defines_var:
                            expr.defines_var = target_name

        # Also process the rest of the while statement via normal traversal
        # (but skip named_expression children as we handled them)
        def traverse_except_named(node, parent_expr_id=None):
            if node.type == "named_expression":
                return  # Already handled
            if node.type == "block":
                return  # Don't descend into body blocks

            expr = self._create_expression(node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file)

            if expr:
                if parent_expr_id:
                    expr.parent_expr_id = parent_expr_id
                expressions.append(expr)
                current_id = expr.id
            else:
                current_id = parent_expr_id

            for child in node.children:
                traverse_except_named(child, current_id)

        traverse_except_named(while_node)

        return expressions

    def _get_statements_via_index(
        self,
        ast_tree: "AstTree",
        file_path: str,
        start_line: int,
        end_line: int,
    ) -> list["TSNode"]:
        """
        SOTA: Get statements via cached index for O(log n) lookup.

        Builds statement index once per file, then queries for each block.
        This is 10-17x faster than _find_statements_in_span for many blocks.

        Args:
            ast_tree: Parsed AST tree
            file_path: Source file path (cache key)
            start_line: Start line (1-indexed)
            end_line: End line (1-indexed)

        Returns:
            List of statement nodes in the span

        Performance:
            - First call per file: O(ast_nodes) - builds index
            - Subsequent calls: O(log n + k) - binary search + filter
            - With 200 blocks/file: 200x speedup vs naive traversal
        """
        # Get or build statement index for this file
        index = self._statement_index_cache.get_or_build(
            ast_tree,
            file_path,
            self._is_statement_node,
        )

        # Query index - O(log n + k)
        return index.query_span(start_line, end_line)

    def _find_statements_in_span(self, ast_tree: "AstTree", start_line: int, end_line: int) -> list["TSNode"]:
        """
        Find all statement nodes within a line span.

        DEPRECATED: Use _get_statements_via_index for better performance.
        Kept for backward compatibility and fallback.

        Args:
            ast_tree: Parsed AST tree
            start_line: Start line (1-indexed)
            end_line: End line (1-indexed)

        Returns:
            List of statement nodes
        """
        statements = []

        def traverse(node):
            if node is None:
                return

            node_start = node.start_point[0] + 1  # Convert to 1-indexed

            # Check if node is a statement type
            if self._is_statement_node(node):
                # Check if node starts within span
                if start_line <= node_start <= end_line:
                    statements.append(node)

            # Always recurse to children (statements may be nested)
            for child in node.children:
                traverse(child)

        # Start from root
        if hasattr(ast_tree, "root"):
            traverse(ast_tree.root)

        return statements

    def _is_statement_node(self, node) -> bool:
        """Check if node is a statement"""
        statement_types = {
            "expression_statement",
            "assignment",
            "augmented_assignment",  # x += func()
            "return_statement",
            "if_statement",
            "for_statement",
            "while_statement",
            "with_statement",
            "try_statement",
            "raise_statement",
            "assert_statement",
            "delete_statement",
            "pass_statement",
            "break_statement",
            "continue_statement",
            "import_statement",
            "import_from_statement",
        }
        if node.type in statement_types:
            return True

        # NOTE: tree-sitter Python grammar aliases expression_statement,
        # so standalone `call` appears directly as a child of `block` or `module`.
        # Example: `execute(query)` is a `call` node, not `expression_statement > call`.
        # But `input("Name")` inside `user_input = input("Name")` should NOT be a statement.
        # Only treat `call` as statement when its parent is `block` or `module` (standalone call).
        if node.type == "call" and node.parent and node.parent.type in ("block", "module"):
            return True

        return False

    def _create_expression(
        self,
        node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        repo_id: str,
        file_path: str,
        source_file: "SourceFile | None",
    ) -> Expression | None:
        """
        Create expression entity from AST node.

        Args:
            node: AST node
            block_id: CFG block ID
            function_fqn: Function FQN
            repo_id: Repository ID
            file_path: File path
            source_file: Source file

        Returns:
            Expression entity or None if not an expression
        """
        # Map node type to ExprKind
        expr_kind = self._map_node_to_expr_kind(node.type)
        if not expr_kind:
            return None

        # Generate expression ID
        self._expr_counter += 1
        expr_id = f"expr:{repo_id}:{file_path}:{node.start_point[0] + 1}:{node.start_point[1]}:{self._expr_counter}"

        # Create span (interned for memory efficiency)
        from codegraph_engine.code_foundation.infrastructure.ir.models.span_pool import SpanPool

        span = SpanPool.intern(
            start_line=node.start_point[0] + 1,
            start_col=node.start_point[1],
            end_line=node.end_point[0] + 1,
            end_col=node.end_point[1],
        )

        # Extract expression-specific attributes
        attrs = self._extract_attributes(node, expr_kind)

        # Extract DFG information (reads_vars, defines_var)
        # SOTA: Returns integer IDs (not names)
        reads_var_ids, defines_var_id = self._extract_dfg_info(node, expr_kind, attrs)

        # SOTA: Arena mode - zero-copy wrapper
        if self._use_arena and self._arena:
            return self._arena.add_and_return(
                id=expr_id,
                kind=expr_kind,
                repo_id=repo_id,
                file_path=file_path,
                function_fqn=function_fqn,
                span=span,
                block_id=block_id,
                reads_vars=reads_var_ids,
                defines_var=defines_var_id,
            )

        # Legacy mode: Full Expression object
        expr = Expression(
            id=expr_id,
            kind=expr_kind,
            repo_id=repo_id,
            file_path=file_path,
            function_fqn=function_fqn,
            span=span,
            block_id=block_id,
            attrs=attrs,
            reads_vars=reads_var_ids,
            defines_var=defines_var_id,
            _var_id_registry=self._var_id_registry,
        )

        # SOTA: Try fast type inference first (no Pyright)
        # This resolves 80%+ of types instantly
        source_bytes = source_file.content.encode("utf-8") if source_file and source_file.content else None
        fast_type = self._try_resolve_type_fast(node, source_bytes)
        if fast_type:
            expr.inferred_type = fast_type
        # Note: Pyright fallback happens in _batch_enrich_with_pyright for remaining cases

        return expr

    def _map_node_to_expr_kind(self, node_type: str) -> ExprKind | None:
        """
        Map tree-sitter node type to ExprKind.

        Args:
            node_type: Tree-sitter node type

        Returns:
            ExprKind or None if not an expression
        """
        mapping = {
            # Value access
            "identifier": ExprKind.NAME_LOAD,
            "attribute": ExprKind.ATTRIBUTE,
            "subscript": ExprKind.SUBSCRIPT,
            # Operations
            "binary_expression": ExprKind.BIN_OP,
            "unary_expression": ExprKind.UNARY_OP,
            "comparison_operator": ExprKind.COMPARE,
            "boolean_operator": ExprKind.BOOL_OP,
            # Calls
            "call": ExprKind.CALL,
            # Literals
            "integer": ExprKind.LITERAL,
            "float": ExprKind.LITERAL,
            "string": ExprKind.LITERAL,
            "true": ExprKind.LITERAL,
            "false": ExprKind.LITERAL,
            "none": ExprKind.LITERAL,
            # Collections
            "list": ExprKind.COLLECTION,
            "dictionary": ExprKind.COLLECTION,
            "set": ExprKind.COLLECTION,
            "tuple": ExprKind.COLLECTION,
            # Special
            "lambda": ExprKind.LAMBDA,
            "list_comprehension": ExprKind.COMPREHENSION,
            "dictionary_comprehension": ExprKind.COMPREHENSION,
            "set_comprehension": ExprKind.COMPREHENSION,
            # Ternary
            "conditional_expression": ExprKind.CONDITIONAL,
        }

        return mapping.get(node_type)

    # =========================================================================
    # Attribute Extraction - Operator Sets (class-level for fast lookup)
    # =========================================================================
    _BIN_OP_OPERATORS = frozenset({"+", "-", "*", "/", "//", "%", "**", "<<", ">>", "&", "|", "^"})
    _UNARY_OP_OPERATORS = frozenset({"-", "+", "not", "~"})
    _COMPARE_OPERATORS = frozenset({"<", ">", "<=", ">=", "==", "!=", "in", "not in", "is", "is not"})
    _BOOL_OP_OPERATORS = frozenset({"and", "or"})

    def _extract_bin_op_attrs(self, node: "TSNode", attrs: dict) -> None:
        """Extract binary operation attributes."""
        for child in node.children:
            if child.type in self._BIN_OP_OPERATORS:
                attrs["operator"] = child.type
                break

    def _extract_unary_op_attrs(self, node: "TSNode", attrs: dict) -> None:
        """Extract unary operation attributes."""
        for child in node.children:
            if child.type in self._UNARY_OP_OPERATORS:
                attrs["operator"] = child.type
                break

    def _extract_compare_attrs(self, node: "TSNode", attrs: dict) -> None:
        """Extract comparison attributes."""
        operators = [child.type for child in node.children if child.type in self._COMPARE_OPERATORS]
        if operators:
            attrs["operators"] = operators

    def _extract_bool_op_attrs(self, node: "TSNode", attrs: dict) -> None:
        """Extract boolean operation attributes."""
        for child in node.children:
            if child.type in self._BOOL_OP_OPERATORS:
                attrs["operator"] = child.type
                break

    def _extract_call_attrs(self, node: "TSNode", attrs: dict) -> None:
        """Extract function call attributes."""
        # Extract callee name and receiver info for method calls
        if node.child_count > 0:
            callee_node = node.children[0]
            if callee_node.type == "identifier":
                # Simple function call: func()
                attrs["callee_name"] = callee_node.text.decode("utf-8") if callee_node.text else ""
            elif callee_node.type == "attribute":
                # Method call: obj.method() or module.func()
                # Full callee name for pattern matching
                attrs["callee_name"] = callee_node.text.decode("utf-8") if callee_node.text else ""

                # ğŸ”¥ SOTA FIX: Extract receiver info for type-aware matching
                # For cursor.execute() â†’ receiver_name = "cursor", method_name = "execute"
                obj_node = callee_node.child_by_field_name("object")
                attr_node = callee_node.child_by_field_name("attribute")

                if obj_node:
                    receiver_text = obj_node.text.decode("utf-8") if obj_node.text else ""
                    attrs["receiver_name"] = receiver_text
                    # Store receiver span for later Pyright type resolution
                    attrs["receiver_span"] = {
                        "line": obj_node.start_point[0] + 1,
                        "col": obj_node.start_point[1],
                    }

                if attr_node:
                    method_name = attr_node.text.decode("utf-8") if attr_node.text else ""
                    attrs["method_name"] = method_name

        # Extract call arguments (for DFG param_to_arg edges)
        call_args, call_kwargs = self._extract_call_arguments(node)
        if call_args:
            attrs["call_args"] = call_args
        # ğŸ”¥ SOTA FIX: Store kwargs for constraint validation (e.g., shell=True)
        if call_kwargs:
            attrs["call_kwargs"] = call_kwargs

        # Best-effort: preserve positional argument texts in-order (including literals)
        # This enables TRCR integration even when arg_expr_ids are not available.
        pos_arg_texts: list[str] = []
        for child in node.children:
            if child.type != "argument_list":
                continue
            for arg_child in child.children:
                if arg_child.type in (",", "(", ")"):
                    continue
                # Skip keyword args (already in call_kwargs)
                if arg_child.type == "keyword_argument":
                    continue
                pos_arg_texts.append(arg_child.text.decode("utf-8") if arg_child.text else "")

        if pos_arg_texts:
            attrs["call_arg_texts"] = pos_arg_texts

    def _extract_call_arguments(self, node: "TSNode") -> tuple[list[str], dict[str, str]]:
        """
        Extract argument variable names and keyword arguments from function call.

        Also counts total arguments (including literals, tuples) for
        parameterized query detection.

        Returns:
            Tuple of (positional_args, keyword_args_dict)
        """
        call_args: list[str] = []
        call_kwargs: dict[str, str] = {}  # ğŸ”¥ SOTA FIX: Extract kwargs
        arg_count = 0  # ğŸ”¥ FIX: Count ALL arguments, not just identifiers

        for child in node.children:
            if child.type != "argument_list":
                continue

            for arg_child in child.children:
                # Skip punctuation (commas, parentheses)
                if arg_child.type in (",", "(", ")"):
                    continue

                # Count all actual arguments
                arg_count += 1

                # Extract variable names where possible
                if arg_child.type == "identifier":
                    arg_name = arg_child.text.decode("utf-8") if arg_child.text else ""
                    if arg_name:
                        call_args.append(arg_name)
                elif arg_child.type == "keyword_argument":
                    # ğŸ”¥ SOTA FIX: Extract both key and value
                    kw_key, kw_val = self._extract_keyword_arg_pair(arg_child)
                    if kw_key and kw_val is not None:
                        call_kwargs[kw_key] = kw_val
                    # Also add value to call_args for DFG tracking
                    if kw_val and isinstance(kw_val, str) and not kw_val.startswith("<"):
                        call_args.append(kw_val)
                else:
                    # ğŸ”¥ FIX: Add placeholder for non-identifier args (literals, tuples)
                    # This ensures call_args length matches actual arg count
                    call_args.append(f"<{arg_child.type}>")

        return call_args, call_kwargs

    def _extract_keyword_arg_pair(self, arg_child: "TSNode") -> tuple[str | None, str | None]:
        """
        Extract key-value pair from keyword argument.

        e.g., shell=True returns ('shell', 'True')
             timeout=x returns ('timeout', 'x')
        """
        if arg_child.child_count == 0:
            return None, None

        kw_key: str | None = None
        kw_val: str | None = None

        for kw_child in arg_child.children:
            if kw_child.type == "identifier" and kw_key is None:
                # First identifier is the key
                kw_key = kw_child.text.decode("utf-8") if kw_child.text else None
            elif kw_key is not None and kw_child.type == "=":
                continue  # Skip equals sign
            elif kw_key is not None:
                # Everything after key= is the value
                kw_val = kw_child.text.decode("utf-8") if kw_child.text else None
                break

        return kw_key, kw_val

    def _extract_keyword_arg_value(self, arg_child: "TSNode") -> str | None:
        """Extract value variable from keyword argument (e.g., y=x returns 'x')."""
        _, val = self._extract_keyword_arg_pair(arg_child)
        return val

    def _extract_attribute_attrs(self, node: "TSNode", attrs: dict) -> None:
        """Extract attribute access attributes."""
        if node.child_count == 0:
            return

        first_child_end = node.children[0].end_point[1]
        for child in node.children:
            if child.type == "identifier" and child.start_point[1] > first_child_end:
                attrs["attr_name"] = child.text.decode("utf-8") if child.text else ""
                break

    def _extract_name_load_attrs(self, node: "TSNode", attrs: dict) -> None:
        """Extract variable name attributes."""
        attrs["var_name"] = node.text.decode("utf-8") if node.text else ""

    def _extract_literal_attrs(self, node: "TSNode", attrs: dict) -> None:
        """Extract literal value attributes."""
        attrs["value"] = node.text.decode("utf-8") if node.text else ""
        attrs["value_type"] = node.type

    def _extract_collection_attrs(self, node: "TSNode", attrs: dict) -> None:
        """Extract collection literal attributes."""
        # tree-sitter node.type conveys collection kind: list/dictionary/set/tuple
        attrs["value_type"] = node.type

    def _extract_conditional_attrs(self, node: "TSNode", attrs: dict) -> None:
        """Extract conditional (ternary) attributes."""
        # We rely on parent/child expression links for operand picking in local inference.
        # Keep minimal metadata for debugging.
        attrs["kind"] = "conditional_expression"

    def _extract_attributes(self, node: "TSNode", expr_kind: ExprKind) -> dict:
        """
        Extract expression-specific attributes from AST node.

        Uses dispatch table pattern for cleaner code and better maintainability.

        Args:
            node: AST node
            expr_kind: Expression kind

        Returns:
            Attributes dictionary
        """
        attrs: dict = {}

        # Dispatch table for attribute extraction
        extractors = {
            ExprKind.BIN_OP: self._extract_bin_op_attrs,
            ExprKind.UNARY_OP: self._extract_unary_op_attrs,
            ExprKind.COMPARE: self._extract_compare_attrs,
            ExprKind.BOOL_OP: self._extract_bool_op_attrs,
            ExprKind.CALL: self._extract_call_attrs,
            ExprKind.ATTRIBUTE: self._extract_attribute_attrs,
            ExprKind.NAME_LOAD: self._extract_name_load_attrs,
            ExprKind.LITERAL: self._extract_literal_attrs,
            ExprKind.COLLECTION: self._extract_collection_attrs,
            ExprKind.CONDITIONAL: self._extract_conditional_attrs,
        }

        extractor = extractors.get(expr_kind)
        if extractor:
            extractor(node, attrs)

        return attrs

    # =========================================================================
    # DFG Extraction Methods (dispatch table pattern)
    # =========================================================================

    def _extract_dfg_name_load(self, node: "TSNode", attrs: dict, reads_vars: list[str]) -> None:
        """Extract DFG info for variable read."""
        var_name = attrs.get("var_name")
        if var_name:
            reads_vars.append(var_name)

    def _extract_dfg_call(self, node: "TSNode", attrs: dict, reads_vars: list[str]) -> None:
        """
        Extract DFG info for function call.

        Call reads:
        - Arguments (variables passed to call)
        - NOT the callee name (function name is not a variable)
        """
        # Only add call arguments (ì‹¤ì œ ë³€ìˆ˜)
        # callee_nameì€ í•¨ìˆ˜ëª…ì´ë¯€ë¡œ ì œì™¸ âœ…
        call_args = attrs.get("call_args", [])
        for arg in call_args:
            if arg and isinstance(arg, str):
                reads_vars.append(arg)

    def _extract_dfg_attribute(self, node: "TSNode", attrs: dict, reads_vars: list[str]) -> None:
        """Extract DFG info for attribute access (obj.field reads obj)."""
        if node.child_count > 0:
            obj_node = node.children[0]
            if obj_node.type == "identifier":
                obj_name = obj_node.text.decode("utf-8") if obj_node.text else ""
                if obj_name:
                    reads_vars.append(obj_name)

    def _extract_dfg_subscript(self, node: "TSNode", attrs: dict, reads_vars: list[str]) -> None:
        """Extract DFG info for subscript access (arr[i] reads both arr and i)."""
        for child in node.children:
            if child.type == "identifier":
                var_name = child.text.decode("utf-8") if child.text else ""
                if var_name:
                    reads_vars.append(var_name)

    def _extract_dfg_comprehension(self, node: "TSNode", attrs: dict, reads_vars: list[str]) -> None:
        """Extract DFG info for comprehension (reads iterable, condition, expression vars)."""
        all_vars = self._extract_identifiers_recursive(node)

        # Deduplicate while preserving order
        seen = set()
        for var in all_vars:
            if var not in seen:
                reads_vars.append(var)
                seen.add(var)

    def _extract_identifiers_recursive(self, ast_node: "TSNode") -> list[str]:
        """Recursively extract all identifier names from AST node."""
        identifiers: list[str] = []
        if ast_node.type == "identifier":
            name = ast_node.text.decode("utf-8") if ast_node.text else ""
            if name:
                identifiers.append(name)
        for child in ast_node.children:
            identifiers.extend(self._extract_identifiers_recursive(child))
        return identifiers

    def _extract_dfg_info(self, node: "TSNode", expr_kind: ExprKind, attrs: dict) -> tuple[list[int], int]:
        """
        Extract DFG information (reads_vars, defines_var) from AST node.

        SOTA Phase A: Returns integer IDs instead of string names.

        Uses dispatch table pattern for cleaner code structure.

        Args:
            node: AST node
            expr_kind: Expression kind
            attrs: Extracted attributes

        Returns:
            (reads_var_ids, defines_var_id) - Integer IDs (0 = None)
        """
        reads_var_names: list[str] = []
        defines_var_name: str | None = None

        # Dispatch table for DFG extraction
        dfg_extractors = {
            ExprKind.NAME_LOAD: self._extract_dfg_name_load,
            ExprKind.CALL: self._extract_dfg_call,
            ExprKind.ATTRIBUTE: self._extract_dfg_attribute,
            ExprKind.SUBSCRIPT: self._extract_dfg_subscript,
            ExprKind.COMPREHENSION: self._extract_dfg_comprehension,
        }

        extractor = dfg_extractors.get(expr_kind)
        if extractor:
            extractor(node, attrs, reads_var_names)

        # SOTA: Convert names â†’ IDs
        reads_var_ids = self._var_id_registry.intern_list(reads_var_names)
        defines_var_id = self._var_id_registry.intern(defines_var_name) if defines_var_name else 0

        # Note: defines_var is set at the parent level (assignment statement)
        return reads_var_ids, defines_var_id

    def _enrich_with_pyright(self, expr: Expression, source_file: "SourceFile"):
        """
        Enrich expression with Pyright type information.

        Args:
            expr: Expression entity
            source_file: Source file
        """
        if not self.pyright:
            return

        try:
            # Query Pyright hover
            hover_info = self.pyright.hover(
                Path(expr.file_path),
                expr.span.start_line,
                expr.span.start_col,
            )

            if hover_info:
                inferred_type = hover_info.get("type")
                if inferred_type:
                    expr.inferred_type = inferred_type
                    # Note: Type linking happens in post-processing (TypeLinker)

        except Exception as e:
            # FIX: High - Add logging for better debuggability
            # Pyright enrichment is optional, so we don't fail the entire process
            if _DEBUG_ENABLED:
                logger.debug(f"Pyright enrichment failed for expression {expr.id}: {e}")

    # ============================================================
    # Helper methods (refactored from _handle_assignment)
    # ============================================================

    def _process_right_side_expressions(
        self,
        right_node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None",
    ) -> list[Expression]:
        """
        Process right side of assignment (reads).

        Recursively traverses right side AST and creates expressions.

        Args:
            right_node: Right side AST node
            block_id: CFG block ID
            function_fqn: Function FQN
            ctx_repo_id: Repository ID
            ctx_file_path: File path
            source_file: Source file

        Returns:
            List of expressions from right side
        """
        expressions: list[Expression] = []

        def traverse_right(node: "TSNode", parent_expr_id: str | None = None):
            expr = self._create_expression(node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file)
            if expr:
                if parent_expr_id:
                    expr.parent_expr_id = parent_expr_id
                expressions.append(expr)
                current_expr_id = expr.id
            else:
                current_expr_id = parent_expr_id

            for child in node.children:
                traverse_right(child, current_expr_id)

        traverse_right(right_node, None)
        return expressions

    def _create_simple_assignment(
        self,
        left_node: "TSNode",
        right_expressions: list[Expression],
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None",
    ) -> Expression | None:
        """
        Create expression for simple assignment (e.g., result = x).

        Args:
            left_node: Left side identifier node
            right_expressions: Expressions from right side
            block_id: CFG block ID
            function_fqn: Function FQN
            ctx_repo_id: Repository ID
            ctx_file_path: File path
            source_file: Source file

        Returns:
            Expression with defines_var set, or None
        """
        from codegraph_engine.code_foundation.infrastructure.semantic_ir.expression.models import ExprKind

        var_name = left_node.text.decode("utf-8") if left_node.text else ""
        if not var_name:
            return None

        # Create expression for left side with defines_var
        expr = self._create_expression(left_node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file)
        if not expr:
            return None

        expr.defines_var = var_name

        # Collect reads_vars from right side expressions
        right_reads = [var for expr in right_expressions for var in expr.reads_vars]
        has_call = any(expr.kind == ExprKind.CALL for expr in right_expressions)
        expr.reads_vars = right_reads

        # Mark if right side contains a function call (for DFG edge kind)
        if has_call:
            expr.attrs["has_call_rhs"] = True

        return expr

    def _create_tuple_unpacking_assignments(
        self,
        left_node: "TSNode",
        right_expressions: list[Expression],
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
    ) -> list[Expression]:
        """
        Create expressions for tuple unpacking assignment (e.g., x, y = a, b).

        Args:
            left_node: Left side pattern node
            right_expressions: Expressions from right side
            block_id: CFG block ID
            function_fqn: Function FQN
            ctx_repo_id: Repository ID
            ctx_file_path: File path

        Returns:
            List of expressions (one per variable in pattern)
        """
        from codegraph_engine.code_foundation.infrastructure.ir.models import Span
        from codegraph_engine.code_foundation.infrastructure.ir.models.span_pool import SpanPool
        from codegraph_engine.code_foundation.infrastructure.semantic_ir.expression.models import ExprKind

        # Extract all variable names from the pattern
        var_names = []

        def extract_identifiers(node: "TSNode"):
            """Recursively extract identifier names from pattern"""
            if node.type == "identifier":
                name = node.text.decode("utf-8") if node.text else ""
                if name:
                    var_names.append(name)
            else:
                for child in node.children:
                    extract_identifiers(child)

        extract_identifiers(left_node)

        # Collect reads_vars from right side expressions (shared across all targets)
        right_reads = []
        has_call = False
        for right_expr in right_expressions:
            right_reads.extend(right_expr.reads_vars)
            if right_expr.kind == ExprKind.CALL:
                has_call = True

        # Create an expression for each variable in the tuple
        expressions: list[Expression] = []
        for var_name in var_names:
            # SOTA: Convert var_name to var_id
            var_id = self._var_id_registry.intern(var_name)

            # Create a minimal expression manually (since _create_expression doesn't handle pattern_list)
            expr_id = f"expr:{ctx_repo_id}:{ctx_file_path}:{left_node.start_point[0] + 1}:{left_node.start_point[1]}"
            span = SpanPool.intern(
                start_line=left_node.start_point[0] + 1,
                start_col=left_node.start_point[1],
                end_line=left_node.end_point[0] + 1,
                end_col=left_node.end_point[1],
            )

            # SOTA: Arena mode - zero-copy
            if self._use_arena and self._arena:
                expr = self._arena.add_and_return(
                    id=expr_id,
                    kind=ExprKind.NAME_LOAD,
                    repo_id=ctx_repo_id,
                    file_path=ctx_file_path,
                    function_fqn=function_fqn,
                    span=span,
                    block_id=block_id,
                    reads_vars=right_reads,
                    defines_var=var_id,
                )
            else:
                # Legacy mode
                expr = Expression(
                    id=expr_id,
                    kind=ExprKind.NAME_LOAD,
                    repo_id=ctx_repo_id,
                    file_path=ctx_file_path,
                    function_fqn=function_fqn,
                    span=span,
                    block_id=block_id,
                    attrs={},
                )
                expr.defines_var = var_id
                expr.reads_vars = right_reads
                expr._var_id_registry = self._var_id_registry

            # Mark if right side contains a function call
            if has_call:
                expr.attrs["has_call_rhs"] = True

            # Mark as tuple unpacking for tracking
            expr.attrs["is_tuple_unpacking"] = True

            expressions.append(expr)

        return expressions

    def _create_complex_assignment(
        self,
        left_node: "TSNode",
        block_id: str,
        function_fqn: str | None,
        ctx_repo_id: str,
        ctx_file_path: str,
        source_file: "SourceFile | None",
    ) -> list[Expression]:
        """
        Create expressions for complex assignment (attribute, subscript, etc.).

        Args:
            left_node: Left side node
            block_id: CFG block ID
            function_fqn: Function FQN
            ctx_repo_id: Repository ID
            ctx_file_path: File path
            source_file: Source file

        Returns:
            List of expressions from left side traversal
        """
        expressions: list[Expression] = []

        def traverse_left(node: "TSNode", parent_expr_id: str | None = None):
            expr = self._create_expression(node, block_id, function_fqn, ctx_repo_id, ctx_file_path, source_file)
            if expr:
                if parent_expr_id:
                    expr.parent_expr_id = parent_expr_id
                expressions.append(expr)
                current_expr_id = expr.id
            else:
                current_expr_id = parent_expr_id

            for child in node.children:
                traverse_left(child, current_expr_id)

        traverse_left(left_node, None)
        return expressions

    # ============================================================
    # SOTA: Fast Type Inference (Pyright-free)
    # ============================================================

    def _infer_literal_type(self, node: "TSNode") -> str | None:
        """
        Infer type from literal AST node without Pyright.

        This handles 80%+ of cases instantly:
        - String literals â†’ str
        - Integer literals â†’ int
        - Float literals â†’ float
        - Boolean literals â†’ bool
        - None â†’ None
        - List/Dict literals â†’ list/dict

        Args:
            node: Tree-sitter AST node

        Returns:
            Inferred type string or None if not a literal
        """
        node_type = node.type

        # String literals
        if node_type in ("string", "concatenated_string"):
            return "str"

        # Numeric literals
        if node_type == "integer":
            return "int"
        if node_type == "float":
            return "float"

        # Boolean literals
        if node_type in ("true", "false"):
            return "bool"

        # None literal
        if node_type == "none":
            return "None"

        # Collection literals (basic inference)
        if node_type == "list":
            return "list"
        if node_type in ("dictionary", "dict"):
            return "dict"
        if node_type == "set":
            return "set"
        if node_type == "tuple":
            return "tuple"

        # Bytes literal
        if node_type == "bytes":
            return "bytes"

        return None

    def _extract_type_annotation(self, node: "TSNode", source_bytes: bytes) -> str | None:
        """
        Extract type annotation from typed variable/parameter.

        Handles:
        - typed_parameter: (x: int)
        - annotated_assignment: x: int = 5
        - function return type: def foo() -> int

        Args:
            node: AST node (typed_parameter, assignment, etc.)
            source_bytes: Source code bytes

        Returns:
            Type annotation string or None
        """
        # Look for type annotation child
        for child in node.children:
            if child.type == "type":
                # Extract the type text
                try:
                    return child.text.decode("utf-8") if isinstance(child.text, bytes) else child.text
                except Exception:
                    return None

        return None

    def _try_resolve_type_fast(self, node: "TSNode", source_bytes: bytes | None = None) -> str | None:
        """
        Try to resolve type without Pyright (fast path).

        Resolution order:
        1. Literal type inference (instant)
        2. Type annotation extraction (instant)
        3. TypeResolver for known types (fast)

        Args:
            node: AST node
            source_bytes: Source code bytes (optional)

        Returns:
            Resolved type string or None (needs Pyright fallback)
        """
        # 1. Try literal type inference
        literal_type = self._infer_literal_type(node)
        if literal_type:
            return literal_type

        # 2. Try type annotation if source available
        if source_bytes and node.parent:
            annotation = self._extract_type_annotation(node.parent, source_bytes)
            if annotation:
                # Validate with TypeResolver
                type_entity = self._type_resolver.resolve_type(annotation)
                return type_entity.raw if type_entity else annotation

        # 3. For identifiers, check if it's a known builtin
        if node.type == "identifier":
            try:
                name = node.text.decode("utf-8") if isinstance(node.text, bytes) else node.text
                if name in self._type_resolver.BUILTIN_TYPES:
                    return name
            except Exception:
                pass

        return None
