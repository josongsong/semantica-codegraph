"""
Tree-of-Thought Scoring Engine (v8.1)

SOTA: Multi-Criteria Decision Making for Code Domain
"""

import logging
from typing import TYPE_CHECKING

from .tot_models import (
    CodeStrategy,
    ExecutionResult,
    ScoringWeights,
    StrategyScore,
    ToTResult,
)

if TYPE_CHECKING:
    pass

logger = logging.getLogger(__name__)


class ToTScoringEngine:
    """
    Tree-of-Thought Scoring Engine (Domain Service)

    ì±…ì„:
    - ì „ëµë³„ ì‹¤í–‰ ê²°ê³¼ í‰ê°€
    - Multi-Criteria ì ìˆ˜ ê³„ì‚°
    - Top-K ì „ëµ ì„ íƒ

    SOTA ê¸°ë²•:
    - MCDM (Multi-Criteria Decision Making)
    - Weighted Sum Model
    - Pareto Optimality ê³ ë ¤
    """

    def __init__(self, weights: dict[str, float] | None = None):
        """
        Args:
            weights: ì»¤ìŠ¤í…€ ê°€ì¤‘ì¹˜ (Optional)
        """
        self.weights = weights or ScoringWeights.get_weights()

        # ê°€ì¤‘ì¹˜ ê²€ì¦
        total = sum(self.weights.values())
        if abs(total - 1.0) > 0.001:
            raise ValueError(f"Weights must sum to 1.0, got {total}")

        logger.info(f"ToT Scoring Engine initialized with weights: {self.weights}")

    def score_strategy(self, strategy: CodeStrategy, execution_result: ExecutionResult) -> StrategyScore:
        """
        ì „ëµ ì ìˆ˜ ê³„ì‚° (Multi-Criteria)

        Args:
            strategy: ì „ëµ
            execution_result: ì‹¤í–‰ ê²°ê³¼

        Returns:
            StrategyScore
        """
        logger.debug(f"Scoring strategy: {strategy.strategy_id}")

        # ê°œë³„ ì ìˆ˜ ê³„ì‚°
        correctness = self._score_correctness(execution_result)
        quality = self._score_quality(execution_result)
        security = self._score_security(execution_result)
        maintainability = self._score_maintainability(execution_result)
        performance = self._score_performance(execution_result)

        # Weighted Sum
        total_score = (
            correctness * self.weights["correctness"]
            + quality * self.weights["quality"]
            + security * self.weights["security"]
            + maintainability * self.weights["maintainability"]
            + performance * self.weights["performance"]
        )

        # Security Veto (SOTA: Critical/High ë³´ì•ˆ ì´ìŠˆëŠ” ê±°ë¶€ê¶Œ)
        if execution_result.security_severity in ("critical", "high"):
            logger.warning(f"Security veto applied: {execution_result.security_severity}")
            total_score = min(total_score, 0.4)  # ê°•ì œë¡œ ë‚®ì¶¤

        # Confidence (LLM + ì‹¤í–‰ ê²°ê³¼ ì¡°í•©)
        confidence = self._calculate_confidence(strategy, execution_result)

        # Reasoning
        strengths, weaknesses = self._analyze_strengths_weaknesses(execution_result, correctness, quality, security)

        recommendation = self._generate_recommendation(total_score, confidence, strengths, weaknesses)

        score = StrategyScore(
            strategy_id=strategy.strategy_id,
            correctness_score=correctness,
            quality_score=quality,
            security_score=security,
            maintainability_score=maintainability,
            performance_score=performance,
            total_score=total_score,
            confidence=confidence,
            strengths=strengths,
            weaknesses=weaknesses,
            recommendation=recommendation,
        )

        logger.info(f"Strategy {strategy.strategy_id}: total={total_score:.2f}, confidence={confidence:.2f}")

        return score

    def rank_strategies(self, strategies: list[CodeStrategy], results: dict[str, ExecutionResult]) -> ToTResult:
        """
        ì „ëµë“¤ ìˆœìœ„ ë§¤ê¸°ê¸°

        Args:
            strategies: ì „ëµ ë¦¬ìŠ¤íŠ¸
            results: {strategy_id: ExecutionResult}

        Returns:
            ToTResult
        """
        logger.info(f"Ranking {len(strategies)} strategies")

        # ê° ì „ëµ ì ìˆ˜ ê³„ì‚°
        scores = {}
        executed_strategies = []

        for strategy in strategies:
            if strategy.strategy_id not in results:
                logger.warning(f"No result for {strategy.strategy_id}, skipping")
                continue

            result = results[strategy.strategy_id]
            score = self.score_strategy(strategy, result)
            scores[strategy.strategy_id] = score
            executed_strategies.append(strategy)

        # Best ì°¾ê¸°
        best_strategy_id = None
        best_score = 0.0

        if scores:
            best_strategy_id = max(scores.keys(), key=lambda sid: scores[sid].get_ranking_key())
            best_score = scores[best_strategy_id].total_score

        # í†µê³„
        total_passed = sum(1 for score in scores.values() if score.is_acceptable())

        tot_result = ToTResult(
            all_strategies=strategies,
            executed_strategies=executed_strategies,
            scores=scores,
            best_strategy_id=best_strategy_id,
            best_score=best_score,
            total_generated=len(strategies),
            total_executed=len(executed_strategies),
            total_passed=total_passed,
        )

        logger.info(f"Ranking complete: {total_passed}/{len(executed_strategies)} passed")

        return tot_result

    # ========================================================================
    # Individual Scoring Methods (Domain Logic)
    # ========================================================================

    def _score_correctness(self, result: ExecutionResult) -> float:
        """
        ì •í™•ì„± ì ìˆ˜ (Correctness)

        ê¸°ì¤€:
        - ì»´íŒŒì¼ ì„±ê³µ (í•„ìˆ˜)
        - í…ŒìŠ¤íŠ¸ í†µê³¼ìœ¨
        """
        if not result.compile_success:
            return 0.0

        # ì»´íŒŒì¼ ì„±ê³µ: ê¸°ë³¸ ì ìˆ˜ 0.3
        score = 0.3

        # í…ŒìŠ¤íŠ¸ í†µê³¼ìœ¨ (0.7)
        score += result.test_pass_rate * 0.7

        return min(score, 1.0)

    def _score_quality(self, result: ExecutionResult) -> float:
        """
        í’ˆì§ˆ ì ìˆ˜ (Quality)

        ê¸°ì¤€:
        - Lint ì—ëŸ¬/ê²½ê³  (ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
        - Type ì—ëŸ¬ (ì—†ì–´ì•¼ í•¨)
        - ë³µì¡ë„ ê°œì„  (ë‚®ì•„ì§€ë©´ ì¢‹ìŒ)
        """
        score = 1.0

        # Lint ì—ëŸ¬ í˜ë„í‹° (ìµœëŒ€ -0.3)
        lint_penalty = min(result.lint_errors * 0.05, 0.3)
        score -= lint_penalty

        # Lint ê²½ê³  í˜ë„í‹° (ìµœëŒ€ -0.1)
        warning_penalty = min(result.lint_warnings * 0.02, 0.1)
        score -= warning_penalty

        # Type ì—ëŸ¬ í˜ë„í‹° (ìµœëŒ€ -0.2)
        type_penalty = min(result.type_errors * 0.1, 0.2)
        score -= type_penalty

        # ë³µì¡ë„ ê°œì„  ë³´ë„ˆìŠ¤ (ìµœëŒ€ +0.2)
        if result.complexity_delta < 0:  # ë³µì¡ë„ ê°ì†Œ
            complexity_bonus = min(abs(result.complexity_delta) * 0.01, 0.2)
            score += complexity_bonus
        elif result.complexity_delta > 0:  # ë³µì¡ë„ ì¦ê°€
            complexity_penalty = min(result.complexity_delta * 0.01, 0.2)
            score -= complexity_penalty

        return max(score, 0.0)

    def _score_security(self, result: ExecutionResult) -> float:
        """
        ë³´ì•ˆ ì ìˆ˜ (Security)

        ê¸°ì¤€:
        - ë³´ì•ˆ ì´ìŠˆ ìˆ˜
        - ë³´ì•ˆ ì‹¬ê°ë„
        """
        if result.security_severity == "critical":
            return 0.0
        elif result.security_severity == "high":
            return 0.2
        elif result.security_severity == "medium":
            return 0.5
        elif result.security_severity == "low":
            return 0.8
        else:  # none
            return 1.0

    def _score_maintainability(self, result: ExecutionResult) -> float:
        """
        ìœ ì§€ë³´ìˆ˜ì„± ì ìˆ˜ (Maintainability)

        ê¸°ì¤€:
        - CFG ë³€ê²½ (ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
        - DFG ë³€ê²½ (ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        score = 1.0

        # CFG ë³€ê²½ í˜ë„í‹°
        cfg_changes = abs(result.cfg_nodes_added) + abs(result.cfg_nodes_removed)
        cfg_penalty = min(cfg_changes * 0.01, 0.5)
        score -= cfg_penalty

        # DFG ë³€ê²½ í˜ë„í‹°
        dfg_penalty = min(result.dfg_edges_changed * 0.01, 0.3)
        score -= dfg_penalty

        return max(score, 0.0)

    def _score_performance(self, result: ExecutionResult) -> float:
        """
        ì„±ëŠ¥ ì ìˆ˜ (Performance)

        ê¸°ì¤€:
        - ì‹¤í–‰ ì‹œê°„ (ë¹ ë¥¼ìˆ˜ë¡ ì¢‹ìŒ)
        - ë©”ëª¨ë¦¬ ì‚¬ìš© (ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        score = 1.0

        # ì‹¤í–‰ ì‹œê°„ í˜ë„í‹° (10ì´ˆ ê¸°ì¤€)
        if result.execution_time > 10.0:
            time_penalty = min((result.execution_time - 10.0) * 0.05, 0.5)
            score -= time_penalty

        # ë©”ëª¨ë¦¬ í˜ë„í‹° (100MB ê¸°ì¤€)
        if result.memory_delta > 100_000_000:
            memory_penalty = min((result.memory_delta - 100_000_000) / 100_000_000 * 0.3, 0.3)
            score -= memory_penalty

        return max(score, 0.0)

    # ========================================================================
    # Helper Methods
    # ========================================================================

    def _calculate_confidence(self, strategy: CodeStrategy, result: ExecutionResult) -> float:
        """
        ì‹ ë¢°ë„ ê³„ì‚°

        ì¡°í•©:
        - LLM ì‹ ë¢°ë„ (0.4)
        - ì‹¤í–‰ ì„±ê³µ (0.6)
        """
        llm_conf = strategy.llm_confidence * 0.4

        # ì‹¤í–‰ ì„±ê³µë„
        exec_conf = 0.0
        if result.compile_success:
            exec_conf = 0.3
            exec_conf += result.test_pass_rate * 0.3

        return min(llm_conf + exec_conf, 1.0)

    def _analyze_strengths_weaknesses(
        self, result: ExecutionResult, correctness: float, quality: float, security: float
    ) -> tuple[list[str], list[str]]:
        """ê°•ì /ì•½ì  ë¶„ì„"""
        strengths = []
        weaknesses = []

        # Correctness
        if correctness > 0.8:
            strengths.append(f"ë†’ì€ í…ŒìŠ¤íŠ¸ í†µê³¼ìœ¨ ({result.test_pass_rate:.0%})")
        elif correctness < 0.5:
            weaknesses.append(f"ë‚®ì€ í…ŒìŠ¤íŠ¸ í†µê³¼ìœ¨ ({result.test_pass_rate:.0%})")

        # Quality
        if quality > 0.8:
            strengths.append("ìš°ìˆ˜í•œ ì½”ë“œ í’ˆì§ˆ")
        if result.lint_errors > 5:
            weaknesses.append(f"Lint ì—ëŸ¬ {result.lint_errors}ê°œ")

        # Security
        if security == 1.0:
            strengths.append("ë³´ì•ˆ ì´ìŠˆ ì—†ìŒ")
        elif result.security_severity in ("high", "critical"):
            weaknesses.append(f"ì‹¬ê°í•œ ë³´ì•ˆ ì´ìŠˆ ({result.security_severity})")

        # Complexity
        if result.complexity_delta < -5:
            strengths.append(f"ë³µì¡ë„ {abs(result.complexity_delta):.0f} ê°ì†Œ")
        elif result.complexity_delta > 10:
            weaknesses.append(f"ë³µì¡ë„ {result.complexity_delta:.0f} ì¦ê°€")

        return strengths, weaknesses

    def _generate_recommendation(
        self, total_score: float, confidence: float, strengths: list[str], weaknesses: list[str]
    ) -> str:
        """ì¶”ì²œ ë©”ì‹œì§€ ìƒì„±"""
        if total_score >= 0.8 and confidence >= 0.7:
            return "âœ… ê°•ë ¥ ì¶”ì²œ: ìš°ìˆ˜í•œ ì†”ë£¨ì…˜"
        elif total_score >= 0.6 and confidence >= 0.5:
            return "âš ï¸ ì¡°ê±´ë¶€ ì¶”ì²œ: ì•½ì  ë³´ì™„ í•„ìš”"
        elif total_score >= 0.4:
            return "ğŸ”„ ì¬ê²€í†  í•„ìš”: ê°œì„  ì—¬ì§€ í¼"
        else:
            return "âŒ ë¹„ì¶”ì²œ: ëŒ€ì•ˆ ê²€í†  í•„ìš”"
