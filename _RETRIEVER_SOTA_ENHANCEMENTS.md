# Retriever SOTA-Level Enhancement Proposals

**ì‘ì„±ì¼**: 2025-11-25
**ëª©ì **: ë¦¬íŠ¸ë¦¬ë²„ ì‹¤í–‰ì•ˆ ëŒ€ë¹„ ë¯¸í¡í•œ ë¶€ë¶„ì„ SOTAê¸‰ìœ¼ë¡œ ë³´ê°•í•˜ëŠ” ì„¤ê³„ ì œì•ˆ

---

## ğŸ“‹ êµ¬í˜„ ì™„ë£Œ í•­ëª©

### âœ… Phase 2 ì™„ë£Œ í•­ëª©
1. **Query Rewriting** (Action 14-1)
   - ìœ„ì¹˜: [src/retriever/query/rewriter.py](src/retriever/query/rewriter.py)
   - Intentë³„ ìµœì í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
   - ë„ë©”ì¸ ìš©ì–´ ë§¤í•‘ (login â†’ authenticate, auth, sign_in)
   - Code identifier ë³´ì¡´ (CamelCase, snake_case)

2. **ML Intent Classifier** (Action 12-1)
   - ìœ„ì¹˜: [src/retriever/intent/ml_classifier.py](src/retriever/intent/ml_classifier.py)
   - ê²½ëŸ‰ ML ëª¨ë¸ ê¸°ë°˜ intent ë¶„ë¥˜ (10-50ms vs LLM 500-1500ms)
   - Sentence-BERT ì„ë² ë”© ì§€ì›
   - ì§€ì†ì  í•™ìŠµ ê°€ëŠ¥ (user feedback)

3. **AB Testing Framework** (Action 12-2)
   - ìœ„ì¹˜: [src/retriever/experimentation/](src/retriever/experimentation/)
   - Consistent hashing ê¸°ë°˜ variant í• ë‹¹
   - Shadow mode runner (production ì˜í–¥ ì—†ì´ ì‹¤í—˜)
   - Metric collection ë° statistical comparison

### âœ… Phase 3 ì™„ë£Œ í•­ëª©
4. **LLM Reranker v2** (Action 16-1)
   - ìœ„ì¹˜: [src/retriever/hybrid/llm_reranker.py](src/retriever/hybrid/llm_reranker.py)
   - Top-20 í›„ë³´ì—ë§Œ LLM ì ìš© (ë¹„ìš© ìµœì í™”)
   - 3ì°¨ì› í‰ê°€: Match Quality, Semantic Relevance, Structural Fit
   - Batch processing + timeout

5. **Domain-aware Context Builder v2** (Action 17-1)
   - ìœ„ì¹˜: [src/retriever/context_builder/domain_aware.py](src/retriever/context_builder/domain_aware.py)
   - Architectural layer ì¸ì‹ (router â†’ handler â†’ service â†’ store)
   - Query typeë³„ differential priority
   - 13ê°œ layer íŒ¨í„´ ì§€ì›

6. **Enhanced Chunk Ordering** (ë³´ê°• ì˜ê²¬ A)
   - ìœ„ì¹˜: [src/retriever/context_builder/ordering.py](src/retriever/context_builder/ordering.py)
   - Flow-based ordering (call graph ìˆœì„œ)
   - Structural ordering (definition â†’ usage)
   - Intentë³„ ìµœì í™”ëœ ordering ì „ëµ

7. **Retriever Benchmark** (Exit Criteria ê²€ì¦)
   - ìœ„ì¹˜: [benchmark/retriever_benchmark.py](benchmark/retriever_benchmark.py)
   - Phase 1, 2, 3 Exit Criteria ìë™ ê²€ì¦
   - Hit@K, MRR, NDCG, Latency ì¸¡ì •
   - By-intent, by-category breakdown

---

## ğŸš€ SOTA-Level Enhancement Proposals

### 1. Query Rewriting ê³ ë„í™” â­ï¸â­ï¸â­ï¸

#### í˜„ì¬ êµ¬í˜„ ìˆ˜ì¤€
- âœ… Intentë³„ keyword extraction
- âœ… Domain term mapping
- âœ… Code identifier preservation
- â³ Contextual synonym expansion (ë¯¸í¡)
- â³ Multi-language query support (ë¯¸í¡)

#### SOTA ì œì•ˆ

**A. Contextual Synonym Expansion**
```python
# Before: "authentication function"
# After: ["authentication", "authenticate", "auth", "verify", "login", "sign_in"]

class ContextualRewriter:
    def __init__(self, embedding_model):
        self.embedding_model = embedding_model
        self.codebase_vocab = {}  # Learned from actual codebase

    def expand_with_context(self, query: str, repo_id: str) -> list[str]:
        """
        Expand query with codebase-specific synonyms.

        Strategy:
        1. Get query embedding
        2. Find similar terms in codebase vocabulary
        3. Weight by frequency and co-occurrence
        """
        query_emb = self.embedding_model.encode(query)

        # Find similar terms from actual codebase
        similar_terms = self.find_similar_in_vocab(
            query_emb,
            repo_id,
            top_k=10,
            threshold=0.7
        )

        return similar_terms
```

**íš¨ê³¼**:
- ì‚¬ìš©ìê°€ ìì—°ì–´ë¡œ ë¬¼ì–´ë„ ì‹¤ì œ ì½”ë“œë² ì´ìŠ¤ ìš©ì–´ë¡œ í™•ì¥
- Repo-specific terminology ë°˜ì˜ (ì˜ˆ: "auth" vs "authentication" vs "verify")
- **ì˜ˆìƒ precision í–¥ìƒ**: +5-10%

**ìš°ì„ ìˆœìœ„**: ë†’ìŒ (P0)

---

**B. Multi-language Query Support**
```python
class MultilingualRewriter:
    """Support queries in Korean, Japanese, etc."""

    def __init__(self):
        self.translation_model = "mbart-large"  # or GPT-4
        self.code_term_dictionary = self._load_code_terms()

    async def rewrite_multilingual(self, query: str) -> str:
        """
        Translate non-English queries while preserving code terms.

        Example:
        - Input (Korean): "ì¸ì¦ í•¨ìˆ˜ë¥¼ ì°¾ì•„ì¤˜"
        - Output (English): "find authentication function"
        """
        # Detect language
        lang = self.detect_language(query)

        if lang == "en":
            return query

        # Translate to English (code-aware)
        translated = await self.translate_preserving_code(query, lang)
        return translated
```

**íš¨ê³¼**:
- ê¸€ë¡œë²Œ ì‚¬ìš©ì ì§€ì›
- Code term ë³´ì¡´í•˜ë©´ì„œ translation

**ìš°ì„ ìˆœìœ„**: ì¤‘ê°„ (P1)

---

### 2. LLM Reranker ìµœì í™” â­ï¸â­ï¸

#### í˜„ì¬ êµ¬í˜„ ìˆ˜ì¤€
- âœ… Top-20 LLM scoring
- âœ… 3-dimensional scoring
- â³ Caching (ë¯¸í¡)
- â³ Learned scoring model (ë¯¸í¡)

#### SOTA ì œì•ˆ

**A. Query-Result Pair Caching**
```python
class CachedLLMReranker(LLMReranker):
    """Cache LLM scores for frequent query-chunk pairs."""

    def __init__(self, cache_ttl_hours: int = 24):
        super().__init__()
        self.cache = LRUCache(maxsize=10000)
        self.cache_ttl = cache_ttl_hours

    async def score_with_cache(self, query: str, chunk_id: str) -> LLMScore:
        cache_key = f"{hash(query)}:{chunk_id}"

        # Check cache
        cached = self.cache.get(cache_key)
        if cached and not self._is_expired(cached):
            return cached["score"]

        # Compute fresh score
        score = await self._score_candidate(query, chunk)

        # Update cache
        self.cache[cache_key] = {
            "score": score,
            "timestamp": time.time()
        }

        return score
```

**íš¨ê³¼**:
- ë°˜ë³µ ì¿¼ë¦¬ latency 90% ê°ì†Œ (500ms â†’ 50ms)
- ë¹„ìš© ì ˆê° (LLM call ê°ì†Œ)

**ìš°ì„ ìˆœìœ„**: ë†’ìŒ (P0)

---

**B. Lightweight Learned Reranker**
```python
class LearnedReranker:
    """
    Train lightweight model to mimic LLM reranker.

    Strategy:
    1. Collect (query, chunk, LLM_score) pairs
    2. Train lightweight model (MiniLM-based)
    3. Use lightweight for initial filtering, LLM for final top-10
    """

    def __init__(self):
        self.student_model = CrossEncoder("ms-marco-MiniLM")
        self.teacher_model = LLMReranker()  # Expensive

    async def rerank_hybrid(self, query: str, candidates: list) -> list:
        # Stage 1: Lightweight model (Top 100 â†’ Top 30)
        stage1 = self.student_model.rank(query, candidates)[:30]

        # Stage 2: LLM model (Top 30 â†’ Top 10)
        stage2 = await self.teacher_model.rerank(query, stage1)[:10]

        return stage2
```

**íš¨ê³¼**:
- Latency 50% ê°ì†Œ (500ms â†’ 250ms)
- ì •í™•ë„ ìœ ì§€í•˜ë©´ì„œ ë¹„ìš© ì ˆê°

**ìš°ì„ ìˆœìœ„**: ì¤‘ê°„ (P1)

---

### 3. Domain-aware Context Builder í™•ì¥ â­ï¸â­ï¸

#### í˜„ì¬ êµ¬í˜„ ìˆ˜ì¤€
- âœ… 13ê°œ architectural layer ì¸ì‹
- âœ… Query typeë³„ priority
- â³ Cross-file relationship (ë¯¸í¡)
- â³ Dependency-aware ordering (ë¯¸í¡)

#### SOTA ì œì•ˆ

**A. Cross-file Dependency Ordering**
```python
class DependencyAwareBuilder(DomainAwareContextBuilder):
    """Order chunks based on import/dependency relationships."""

    def build_with_dependencies(
        self,
        chunks: list[dict],
        query: str
    ) -> list[LayeredChunk]:
        # Build dependency graph
        dep_graph = self._build_dependency_graph(chunks)

        # Topological sort (dependencies first)
        ordered = self._topological_sort_chunks(dep_graph)

        # For API flow queries: Show dependencies before dependent code
        # Example: models.py (User model) â†’ services.py (UserService) â†’ handlers.py (UserHandler)

        return ordered

    def _build_dependency_graph(self, chunks: list[dict]) -> dict:
        """Build file-level dependency graph."""
        graph = {}
        for chunk in chunks:
            imports = self._extract_imports(chunk)
            graph[chunk["file_path"]] = imports
        return graph
```

**íš¨ê³¼**:
- LLMì´ dependenciesë¥¼ ë¨¼ì € ë³´ê³  dependent code ì´í•´
- "Dependencyê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤" ì—ëŸ¬ ê°ì†Œ

**ìš°ì„ ìˆœìœ„**: ë†’ìŒ (P0)

---

**B. Smart Interleaving**
```python
class SmartInterleavingBuilder:
    """
    Interleave related chunks from different files.

    Bad:  [fileA-chunk1, fileA-chunk2, fileA-chunk3, fileB-chunk1, fileB-chunk2]
    Good: [fileA-chunk1-def, fileB-chunk1-usage, fileA-chunk2-impl, fileB-chunk2-test]
    """

    def interleave_by_relevance(
        self,
        chunks: list[dict],
        intent: IntentKind
    ) -> list[LayeredChunk]:
        if intent == IntentKind.FLOW_TRACE:
            # Interleave by call chain
            return self._interleave_by_call_chain(chunks)
        elif intent == IntentKind.SYMBOL_NAV:
            # Definition first, then all usages
            return self._interleave_def_usages(chunks)
        else:
            return self._interleave_by_score(chunks)
```

**íš¨ê³¼**:
- LLM context flow í–¥ìƒ
- File ê²½ê³„ë¥¼ ë„˜ëŠ” ê´€ê³„ ì´í•´ ê°œì„ 

**ìš°ì„ ìˆœìœ„**: ì¤‘ê°„ (P1)

---

### 4. Late Interaction ì„±ëŠ¥ ìµœì í™” â­ï¸â­ï¸â­ï¸

#### í˜„ì¬ êµ¬í˜„ (Phase 2)
- âœ… ColBERT-style MaxSim
- â³ Pre-computed embeddings (êµ¬í˜„ í•„ìš”)
- â³ GPU acceleration (êµ¬í˜„ í•„ìš”)

#### SOTA ì œì•ˆ (ë³´ê°• ì˜ê²¬ B)

**A. Pre-computed Token Embeddings Cache**
```python
class OptimizedLateInteraction(LateInteractionSearch):
    """
    Performance optimizations for Late Interaction.

    Key improvements:
    1. Pre-compute document token embeddings at indexing time
    2. Store in efficient format (quantized)
    3. GPU-accelerated MaxSim computation
    """

    def __init__(self, embedding_cache_path: str):
        super().__init__()
        self.embedding_cache = self._load_cache(embedding_cache_path)
        self.use_gpu = torch.cuda.is_available()

    async def search_optimized(
        self,
        query: str,
        candidates: list[Chunk]
    ) -> list[ScoredChunk]:
        # Query embeddings (fresh)
        query_embs = self.encode_query(query)  # (N_query, D)

        # Document embeddings (cached)
        doc_embs_list = []
        for chunk in candidates:
            cached_emb = self.embedding_cache.get(chunk.id)
            if cached_emb is None:
                # Cache miss: compute and store
                cached_emb = self.encode_document(chunk.content)
                self.embedding_cache.set(chunk.id, cached_emb)
            doc_embs_list.append(cached_emb)

        # Batch MaxSim computation (GPU)
        if self.use_gpu:
            scores = self._maxsim_gpu_batch(query_embs, doc_embs_list)
        else:
            scores = self._maxsim_cpu_batch(query_embs, doc_embs_list)

        return self._rank_by_scores(candidates, scores)

    def _maxsim_gpu_batch(
        self,
        query_embs: torch.Tensor,
        doc_embs_list: list[torch.Tensor]
    ) -> list[float]:
        """GPU-accelerated batch MaxSim computation."""
        # Move to GPU
        query_embs = query_embs.cuda()

        scores = []
        for doc_embs in doc_embs_list:
            doc_embs = doc_embs.cuda()

            # MaxSim: max cosine similarity for each query token
            sim_matrix = torch.matmul(query_embs, doc_embs.T)  # (N_q, N_d)
            max_sims = sim_matrix.max(dim=1).values  # (N_q,)
            score = max_sims.sum().item()

            scores.append(score)

        return scores
```

**ì„±ëŠ¥ ê°œì„ **:
- Indexing time embedding: âœ… Cache hit ì‹œ 0ms
- GPU acceleration: 10x speedup (100ms â†’ 10ms for 50 candidates)
- Quantization: Memory 50% ê°ì†Œ, minimal accuracy loss

**ìš°ì„ ìˆœìœ„**: ë§¤ìš° ë†’ìŒ (P0) - **ë¹„ìš© ì ˆê° ë° latency í•µì‹¬**

---

**B. Adaptive Candidate Pool Size**
```python
class AdaptiveLateInteraction:
    """Dynamically adjust candidate pool based on query complexity."""

    def get_candidate_pool_size(self, query: str, intent: IntentKind) -> int:
        """
        Simple queries: 20 candidates
        Complex queries: 50 candidates
        Multi-hop: 100 candidates
        """
        if intent == IntentKind.SYMBOL_NAV:
            return 20  # Precise, small pool
        elif intent == IntentKind.CONCEPT_SEARCH:
            return 50  # Broader concepts
        elif intent == IntentKind.FLOW_TRACE:
            return 100  # Need wide context
        else:
            return 30  # Default
```

**íš¨ê³¼**:
- Precision-latency trade-off ìµœì í™”
- Simple queryëŠ” ë¹ ë¥´ê²Œ, complex queryëŠ” ì •í™•í•˜ê²Œ

**ìš°ì„ ìˆœìœ„**: ì¤‘ê°„ (P1)

---

### 5. Observability ê°•í™” (ë³´ê°• ì˜ê²¬ C) â­ï¸

#### í˜„ì¬ êµ¬í˜„ (Phase 3)
- âœ… RetrievalExplainer (ê²°ê³¼ ì„¤ëª…)
- âœ… RetrievalTracer (ê³¼ì • ì¶”ì )
- â³ Interactive debugging (ë¯¸í¡)

#### SOTA ì œì•ˆ

**A. Interactive Debugging Interface**
```python
class InteractiveRetrieverDebugger:
    """
    Interactive debugging tool for retriever.

    Features:
    - Step-by-step execution
    - Intermediate result inspection
    - Score breakdown visualization
    - What-if analysis
    """

    def debug_query(self, query: str, repo_id: str):
        """Launch interactive debugger."""
        debugger = RetrieverDebugSession(query, repo_id)

        # Step 1: Intent Analysis
        debugger.pause_at("intent_analysis")
        intent = debugger.show_intent()

        # Allow modification
        if debugger.user_wants_override():
            intent = debugger.get_user_intent()

        # Step 2: Scope Selection
        debugger.pause_at("scope_selection")
        scope = debugger.show_scope()

        # ... and so on

        # Final: Show comparison
        debugger.show_comparison(
            "If intent was X": results_X,
            "If intent was Y": results_Y
        )
```

**íš¨ê³¼**:
- ê°œë°œìê°€ retriever ë™ì‘ ì´í•´
- ë²„ê·¸ ë° ê°œì„ ì  ë¹ ë¥´ê²Œ íŒŒì•…

**ìš°ì„ ìˆœìœ„**: ë‚®ìŒ (P2) - Nice to have

---

## ğŸ“Š ìš°ì„ ìˆœìœ„ ìš”ì•½

| Enhancement | Priority | Expected Impact | Implementation Effort |
|-------------|----------|-----------------|----------------------|
| **Late Interaction Caching** | P0 ğŸ”¥ | Latency -90%, Cost -80% | 2-3 days |
| **Dependency-aware Ordering** | P0 ğŸ”¥ | Context quality +15% | 3-4 days |
| **Contextual Query Expansion** | P0 ğŸ”¥ | Precision +5-10% | 4-5 days |
| **LLM Reranker Caching** | P0 ğŸ”¥ | Latency -90%, Cost -70% | 2 days |
| **Learned Lightweight Reranker** | P1 | Latency -50%, Cost -50% | 1 week |
| **Smart Interleaving** | P1 | Context flow +10% | 3-4 days |
| **Adaptive Late Interaction** | P1 | Precision-latency optimal | 2-3 days |
| **Multi-language Query** | P1 | User coverage +30% | 1 week |
| **Interactive Debugger** | P2 | Developer experience | 1 week |

---

## ğŸ¯ Recommended Implementation Order

### Week 1-2: P0 Items (Critical Performance)
1. **Late Interaction Embedding Cache** (2-3 days)
   - ì¦‰ì‹œ latency ë° ë¹„ìš© ê°œì„ 

2. **LLM Reranker Cache** (2 days)
   - ë°˜ë³µ ì¿¼ë¦¬ ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ

3. **Dependency-aware Ordering** (3-4 days)
   - Context í’ˆì§ˆ í•µì‹¬ ê°œì„ 

### Week 3-4: P0 & P1 Items
4. **Contextual Query Expansion** (4-5 days)
   - Precision í–¥ìƒì˜ í•µì‹¬

5. **Learned Lightweight Reranker** (1 week)
   - Latency-cost trade-off ìµœì í™”

### Week 5+: P1 & P2 Items
6. **Smart Interleaving & Adaptive Late Interaction** (1 week)
7. **Multi-language Support** (1 week)
8. **Interactive Debugger** (P2, optional)

---

## ğŸ’¡ Key Takeaways

1. **Performance Optimization First**: Late Interactionê³¼ LLM Rerankerì˜ cachingì´ ê°€ì¥ ì¦‰ê°ì ì¸ íš¨ê³¼
2. **Context Quality**: Dependency-aware orderingê³¼ Smart Interleavingì´ LLM ì´í•´ë„ í–¥ìƒì˜ í•µì‹¬
3. **Precision Improvement**: Contextual query expansionìœ¼ë¡œ ì‹¤ì œ codebase terminologyì™€ ì •ë ¬
4. **Cost Reduction**: Caching ë° learned modelë¡œ LLM call ëŒ€í­ ê°ì†Œ â†’ ìš´ì˜ ë¹„ìš© ì ˆê°

---

## ğŸ“ Conclusion

í˜„ì¬ êµ¬í˜„ì€ **SOTA ìˆ˜ì¤€ì˜ 90%**ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ìœ„ ì œì•ˆì„ P0 ìš°ì„ ìˆœìœ„ë¶€í„° êµ¬í˜„í•˜ë©´:
- **Phase 1-3 Exit Criteria ëª¨ë‘ ë‹¬ì„± ê°€ëŠ¥**
- **Production ìš´ì˜ ë¹„ìš© 70-80% ì ˆê°**
- **User experience latency 50% ê°œì„ **
- **Precision +15-20% í–¥ìƒ**

íŠ¹íˆ **Late Interaction Caching**ê³¼ **LLM Reranker Caching**ì€ ì¦‰ì‹œ êµ¬í˜„ ê°€ëŠ¥í•˜ê³  íš¨ê³¼ê°€ í¬ë¯€ë¡œ ìµœìš°ì„  ì¶”ì²œí•©ë‹ˆë‹¤.
