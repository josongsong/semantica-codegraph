# R002: SOTA Benchmark System - Rust-Only with Ground Truth Regression

**Status**: Draft
**Author**: Codegraph Team
**Created**: 2025-12-29
**Updated**: 2025-12-29
**RFC Number**: R002
**Goal**: Rust ì „ìš© ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ + Ground Truth ê¸°ë°˜ ì„±ëŠ¥ íšŒê·€ ë°©ì§€

---

## Executive Summary

**í˜„ì¬ ë¬¸ì œ:**
- 30+ ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ ì‚°ì¬ (Rust + Python)
- í†µì¼ëœ ë¦¬í¬íŠ¸ í˜•ì‹ ì—†ìŒ
- **ì„±ëŠ¥ íšŒê·€ ê°ì§€ ë¶ˆê°€** (Ground Truth ì—†ìŒ)
- RFC-CONFIG-SYSTEMê³¼ í†µí•© ì•ˆë¨
- íˆìŠ¤í† ë¦¬ ì¶”ì  ë¶ˆê°€

**ì œì•ˆ:**
```rust
// 90% Use Case: í•œ ì¤„ë¡œ ë²¤ì¹˜ë§ˆí¬ + ìë™ íšŒê·€ ê²€ì‚¬
cargo bench-codegraph --repo typer --preset balanced

// 9% Use Case: íŠ¹ì • ì„¤ì •ìœ¼ë¡œ ë²¤ì¹˜ë§ˆí¬
cargo bench-codegraph --repo django --config security-audit.yaml

// 1% Use Case: Ground Truth ìƒì„± (ë¦´ë¦¬ìŠ¤ ì „)
cargo bench-codegraph --repo typer --preset balanced --save-ground-truth
```

**í•µì‹¬ ê°œì„ ì‚¬í•­:**
- âœ… Rust ì „ìš© (ì¼ê´€ëœ ì¸¡ì •, ë‚®ì€ ì˜¤ë²„í—¤ë“œ)
- âœ… RFC-CONFIG-SYSTEM ì™„ë²½ í†µí•© (Preset + YAML)
- âœ… Ground Truth ê¸°ë°˜ íšŒê·€ í…ŒìŠ¤íŠ¸ (Â±5% í—ˆìš©)
- âœ… í†µì¼ëœ ë¦¬í¬íŠ¸ í˜•ì‹ (JSON + Markdown + HTML)
- âœ… íˆìŠ¤í† ë¦¬ ì¶”ì  (Git ì»¤ë°‹ë³„)
- âœ… CI/CD í†µí•© (ìë™ íšŒê·€ ê²€ì‚¬)
- âœ… ë‹¤ì¤‘ ë¦¬í¬ì§€í† ë¦¬ ë¹„êµ (Small/Medium/Large)

---

## Part 1: Ground Truth Philosophy

### 1.1. What is Ground Truth?

**ì •ì˜**: íŠ¹ì • ì„¤ì • + íŠ¹ì • ë¦¬í¬ì§€í† ë¦¬ì—ì„œ **ê²€ì¦ëœ ì„±ëŠ¥ ê¸°ì¤€ê°’**

```
Ground Truth = (Config, Repo, Expected Performance)

Example:
  Config:   Preset::Balanced
  Repo:     typer (small, 8k LOC)
  Expected:
    - Duration: 2.5s Â± 5%
    - Throughput: 3200 LOC/sec Â± 5%
    - Memory: 150MB Â± 10%
```

### 1.2. Why Ground Truth?

| Without Ground Truth | With Ground Truth |
|---------------------|-------------------|
| âŒ "ì„±ëŠ¥ì´ ëŠë ¤ì§„ ê²ƒ ê°™ì€ë°?" | âœ… **íšŒê·€ ê²€ì¶œ**: "Duration 2.1s â†’ 3.2s (+52%, FAIL)" |
| âŒ ìˆ˜ë™ìœ¼ë¡œ ê³¼ê±° ê²°ê³¼ ë¹„êµ | âœ… **ìë™ ê²€ì¦**: CIì—ì„œ ì¦‰ì‹œ ì‹¤íŒ¨ |
| âŒ ìµœì í™” íš¨ê³¼ ë¶ˆëª…í™• | âœ… **ê°œì„  ì¶”ì **: "Throughput +15% (2800 â†’ 3220)" |
| âŒ ì„¤ì • ë³€ê²½ ì˜í–¥ ì•Œ ìˆ˜ ì—†ìŒ | âœ… **ì„¤ì • ì˜í–¥ ì¸¡ì •**: "Fast vs Balanced: 2.5x ì°¨ì´" |

### 1.3. Ground Truth Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1: Initial Establishment (ë¦´ë¦¬ìŠ¤ ì „)                â”‚
â”‚   - Presetë³„ Ground Truth ìƒì„±                           â”‚
â”‚   - 3íšŒ ì‹¤í–‰ í‰ê·  (ì•ˆì •ì„± í™•ë³´)                            â”‚
â”‚   - Tolerance ì„¤ì • (Duration Â±5%, Memory Â±10%)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 2: Continuous Validation (ë§¤ PR)                  â”‚
â”‚   - CIì—ì„œ Ground Truth ëŒ€ë¹„ ê²€ì¦                        â”‚
â”‚   - í—ˆìš© ë²”ìœ„ ì´ˆê³¼ ì‹œ PR ë¸”ë¡                             â”‚
â”‚   - ì„±ëŠ¥ ì €í•˜ ì›ì¸ ìš”êµ¬ (ì½”ë©˜íŠ¸)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 3: Periodic Update (ì›” 1íšŒ or ë¦´ë¦¬ìŠ¤)              â”‚
â”‚   - ì˜ë„ì  ìµœì í™” í›„ Ground Truth ê°±ì‹                     â”‚
â”‚   - ë³€ê²½ ë¡œê·¸ í•„ìˆ˜: "Why updated? +15% by ..."          â”‚
â”‚   - Team review í•„ìˆ˜                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.4. Tolerance Strategy

**Why Tolerance?**
- âŒ 0% Tolerance: ë…¸ì´ì¦ˆ(CPU throttling, GC)ë¡œ False Positive
- âœ… Â±5% Tolerance: ì‹¤ì œ íšŒê·€ë§Œ ê²€ì¶œ

**Tolerance by Metric:**
```rust
pub struct Tolerance {
    /// Duration tolerance (default: 5%)
    /// - Too strict (<2%): False positives from system noise
    /// - Too loose (>10%): Miss real regressions
    pub duration_pct: f64,  // 5%

    /// Throughput tolerance (default: 5%)
    pub throughput_pct: f64,  // 5%

    /// Memory tolerance (default: 10%)
    /// - Memory more variable than CPU
    pub memory_pct: f64,  // 10%

    /// Node/edge count tolerance (default: 0%)
    /// - Deterministic, should be exact
    pub count_tolerance: usize,  // 0
}
```

**Adaptive Tolerance (Future):**
```rust
// Small repo: ì—„ê²©í•œ tolerance (ë…¸ì´ì¦ˆ ì ìŒ)
if repo.loc < 10_000 {
    tolerance.duration_pct = 3.0;
}

// Large repo: ëŠìŠ¨í•œ tolerance (ë…¸ì´ì¦ˆ ë§ìŒ)
if repo.loc > 100_000 {
    tolerance.duration_pct = 10.0;
}
```

---

## Part 2: Architecture

### 2.1. Overall Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CLI: cargo bench-codegraph                 â”‚
â”‚  (Thin wrapper, parses args, calls BenchmarkRunner)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BenchmarkRunner (Core)                    â”‚
â”‚  1. Load BenchmarkConfig (from RFC-CONFIG)                  â”‚
â”‚  2. Discover repos (small/medium/large)                     â”‚
â”‚  3. Run benchmarks (single or multi-repo)                   â”‚
â”‚  4. Collect BenchmarkResult                                 â”‚
â”‚  5. Validate against Ground Truth                           â”‚
â”‚  6. Generate Reports (JSON + MD + HTML)                     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                  â”‚                  â”‚
     â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Config  â”‚   â”‚ Ground Truth â”‚   â”‚ Report Gen     â”‚
â”‚ (R001)  â”‚   â”‚ Store        â”‚   â”‚ (Multi-format) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2. Module Structure

```
packages/codegraph-ir/src/benchmark/
â”œâ”€â”€ mod.rs                      # Re-exports
â”œâ”€â”€ config.rs                   # BenchmarkConfig (RFC-CONFIG í†µí•©)
â”œâ”€â”€ runner.rs                   # BenchmarkRunner (orchestrator)
â”œâ”€â”€ repository.rs               # Repository (repo metadata)
â”œâ”€â”€ result.rs                   # BenchmarkResult (single run)
â”œâ”€â”€ ground_truth.rs             # GroundTruth (expected values)
â”œâ”€â”€ validator.rs                # GroundTruthValidator (íšŒê·€ ê²€ì‚¬)
â”œâ”€â”€ report/
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ json.rs                 # JSON report
â”‚   â”œâ”€â”€ markdown.rs             # Markdown report
â”‚   â”œâ”€â”€ html.rs                 # HTML report (waterfall chart)
â”‚   â””â”€â”€ terminal.rs             # Terminal output (pretty)
â””â”€â”€ repos/
    â”œâ”€â”€ mod.rs
    â”œâ”€â”€ discovery.rs            # Auto-discover repos
    â””â”€â”€ presets.rs              # Well-known repos (typer, django, etc.)
```

### 2.3. CLI Tool

```bash
# Install as cargo subcommand
cargo install --path packages/codegraph-ir --bin bench-codegraph

# Usage
cargo bench-codegraph --help
```

---

## Part 3: Core Types

### 3.1. BenchmarkConfig (RFC-CONFIG í†µí•©)

```rust
use crate::config::PipelineConfig;  // From RFC-CONFIG

/// Benchmark configuration (extends PipelineConfig)
#[derive(Debug, Clone)]
pub struct BenchmarkConfig {
    /// Pipeline configuration (from RFC-CONFIG)
    pub pipeline: PipelineConfig,

    /// Benchmark-specific settings
    pub benchmark_opts: BenchmarkOptions,
}

#[derive(Debug, Clone)]
pub struct BenchmarkOptions {
    /// Number of warmup runs (default: 1)
    pub warmup_runs: usize,

    /// Number of measured runs (default: 3)
    pub measured_runs: usize,

    /// Enable memory profiling (default: true)
    pub profile_memory: bool,

    /// Enable stage-level timing (default: true)
    pub profile_stages: bool,

    /// Save results to disk (default: true)
    pub save_results: bool,

    /// Output directory (default: "target/benchmark_results")
    pub output_dir: PathBuf,

    /// Ground Truth validation (default: true)
    pub validate_ground_truth: bool,

    /// Tolerance settings
    pub tolerance: Tolerance,
}

impl Default for BenchmarkOptions {
    fn default() -> Self {
        Self {
            warmup_runs: 1,
            measured_runs: 3,
            profile_memory: true,
            profile_stages: true,
            save_results: true,
            output_dir: PathBuf::from("target/benchmark_results"),
            validate_ground_truth: true,
            tolerance: Tolerance::default(),
        }
    }
}

impl BenchmarkConfig {
    /// Create from preset (simplest)
    pub fn from_preset(preset: Preset) -> Self {
        Self {
            pipeline: PipelineConfig::preset(preset).build().unwrap(),
            benchmark_opts: BenchmarkOptions::default(),
        }
    }

    /// Create from YAML (advanced)
    pub fn from_yaml(path: &str) -> Result<Self, BenchmarkError> {
        let pipeline = PipelineConfig::from_yaml(path)?;
        Ok(Self {
            pipeline,
            benchmark_opts: BenchmarkOptions::default(),
        })
    }
}
```

### 3.2. Repository

```rust
/// Repository metadata
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Repository {
    /// Unique identifier (e.g., "typer", "django")
    pub id: String,

    /// Display name
    pub name: String,

    /// Path to repository
    pub path: PathBuf,

    /// Size category
    pub category: RepoCategory,

    /// Source files (auto-discovered)
    pub files: Vec<PathBuf>,

    /// Total LOC
    pub total_loc: usize,

    /// Primary language
    pub language: Language,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub enum RepoCategory {
    Small,   // < 10k LOC
    Medium,  // 10k - 100k LOC
    Large,   // > 100k LOC
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub enum Language {
    Python,
    Rust,
    JavaScript,
    TypeScript,
    Go,
    Java,
    Kotlin,
}

impl Repository {
    /// Auto-discover repository from path
    pub fn from_path(path: PathBuf) -> Result<Self, BenchmarkError> {
        let id = path.file_name()
            .ok_or(BenchmarkError::InvalidRepo)?
            .to_string_lossy()
            .to_string();

        // Scan files
        let files = Self::scan_files(&path)?;
        let total_loc = Self::count_loc(&files)?;

        let category = match total_loc {
            0..=10_000 => RepoCategory::Small,
            10_001..=100_000 => RepoCategory::Medium,
            _ => RepoCategory::Large,
        };

        Ok(Self {
            id: id.clone(),
            name: id,
            path,
            category,
            files,
            total_loc,
            language: Language::Python,  // TODO: detect
        })
    }

    fn scan_files(path: &PathBuf) -> Result<Vec<PathBuf>, BenchmarkError> {
        // Similar to benchmark_large_repos.rs count_files()
        // ...
    }

    fn count_loc(files: &[PathBuf]) -> Result<usize, BenchmarkError> {
        // ...
    }
}
```

### 3.3. BenchmarkResult

```rust
use std::time::Duration;
use std::collections::HashMap;

/// Single benchmark run result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkResult {
    /// Metadata
    pub repo_id: String,
    pub config_name: String,  // e.g., "Preset::Balanced"
    pub timestamp: u64,       // Unix timestamp
    pub git_commit: Option<String>,  // Current HEAD

    /// Repository info
    pub repo_category: RepoCategory,
    pub total_loc: usize,
    pub files_count: usize,

    /// Performance metrics
    pub duration: Duration,
    pub throughput_loc_per_sec: f64,
    pub memory_mb: f64,

    /// Indexing results (from IndexingResult)
    pub files_processed: usize,
    pub files_cached: usize,
    pub files_failed: usize,
    pub cache_hit_rate: f64,

    /// IR metrics
    pub total_nodes: usize,
    pub total_edges: usize,
    pub total_chunks: usize,
    pub total_symbols: usize,

    /// Stage-level breakdown
    pub stage_durations: HashMap<String, Duration>,

    /// Analysis-specific metrics
    pub pta_summary: Option<PTASummary>,
    pub taint_summary: Option<TaintSummary>,
    pub repomap_summary: Option<RepoMapSummary>,

    /// Errors
    pub errors: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PTASummary {
    pub mode_used: String,       // "Fast (Steensgaard)" or "Precise (Andersen)"
    pub variables_count: usize,
    pub constraints_count: usize,
    pub alias_pairs: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaintSummary {
    pub sources_found: usize,
    pub sinks_found: usize,
    pub paths_found: usize,
    pub max_path_length: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RepoMapSummary {
    pub total_nodes: usize,
    pub pagerank_iterations: usize,
    pub top_10_symbols: Vec<String>,
}

impl BenchmarkResult {
    /// Compare with another result (for regression detection)
    pub fn diff(&self, other: &Self) -> BenchmarkDiff {
        BenchmarkDiff {
            duration_change_pct: Self::pct_change(
                self.duration.as_secs_f64(),
                other.duration.as_secs_f64(),
            ),
            throughput_change_pct: Self::pct_change(
                self.throughput_loc_per_sec,
                other.throughput_loc_per_sec,
            ),
            memory_change_pct: Self::pct_change(
                self.memory_mb,
                other.memory_mb,
            ),
            // ... other fields
        }
    }

    fn pct_change(before: f64, after: f64) -> f64 {
        ((after - before) / before) * 100.0
    }
}

#[derive(Debug, Clone)]
pub struct BenchmarkDiff {
    pub duration_change_pct: f64,     // -10.5 = 10.5% faster
    pub throughput_change_pct: f64,   // +15.2 = 15.2% faster
    pub memory_change_pct: f64,       // +5.0 = 5% more memory
}
```

### 3.4. Ground Truth

```rust
/// Ground Truth: Expected performance for (Config, Repo)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GroundTruth {
    /// Unique identifier: "{repo_id}_{config_name}"
    pub id: String,

    pub repo_id: String,
    pub config_name: String,

    /// Expected values (from N runs average)
    pub expected: ExpectedMetrics,

    /// Metadata
    pub established_at: u64,       // Unix timestamp
    pub established_by: String,    // Git commit SHA
    pub last_updated_at: u64,
    pub last_updated_by: String,
    pub update_reason: String,     // "Initial baseline" or "Optimized X by Y%"

    /// Validation history
    pub validation_count: usize,   // How many times validated
    pub last_validated_at: u64,
    pub last_validation_status: ValidationStatus,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExpectedMetrics {
    /// Core metrics
    pub duration_sec: f64,
    pub throughput_loc_per_sec: f64,
    pub memory_mb: f64,

    /// Deterministic metrics (exact match expected)
    pub total_nodes: usize,
    pub total_edges: usize,
    pub total_chunks: usize,
    pub total_symbols: usize,

    /// Cache metrics (informational, not validated)
    pub cache_hit_rate: f64,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub enum ValidationStatus {
    Pass,
    Fail,
    Skip,
}

impl GroundTruth {
    /// Create from benchmark results (average of N runs)
    pub fn from_results(
        repo_id: String,
        config_name: String,
        results: &[BenchmarkResult],
        reason: String,
    ) -> Self {
        assert!(!results.is_empty(), "Need at least 1 result");

        let n = results.len() as f64;

        let avg_duration = results.iter()
            .map(|r| r.duration.as_secs_f64())
            .sum::<f64>() / n;

        let avg_throughput = results.iter()
            .map(|r| r.throughput_loc_per_sec)
            .sum::<f64>() / n;

        let avg_memory = results.iter()
            .map(|r| r.memory_mb)
            .sum::<f64>() / n;

        // Deterministic metrics: use first result (should be same)
        let first = &results[0];

        let git_commit = Self::get_git_commit();

        Self {
            id: format!("{}_{}", repo_id, config_name),
            repo_id,
            config_name,
            expected: ExpectedMetrics {
                duration_sec: avg_duration,
                throughput_loc_per_sec: avg_throughput,
                memory_mb: avg_memory,
                total_nodes: first.total_nodes,
                total_edges: first.total_edges,
                total_chunks: first.total_chunks,
                total_symbols: first.total_symbols,
                cache_hit_rate: first.cache_hit_rate,
            },
            established_at: Self::now(),
            established_by: git_commit.clone(),
            last_updated_at: Self::now(),
            last_updated_by: git_commit,
            update_reason: reason,
            validation_count: 0,
            last_validated_at: 0,
            last_validation_status: ValidationStatus::Skip,
        }
    }

    fn now() -> u64 {
        std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs()
    }

    fn get_git_commit() -> String {
        // Use git2 crate or shell command
        std::process::Command::new("git")
            .args(&["rev-parse", "HEAD"])
            .output()
            .ok()
            .and_then(|o| String::from_utf8(o.stdout).ok())
            .map(|s| s.trim().to_string())
            .unwrap_or_else(|| "unknown".to_string())
    }
}
```

### 3.5. Ground Truth Storage

```rust
/// Ground Truth store (file-based)
pub struct GroundTruthStore {
    /// Storage directory (default: "benchmark/ground_truth")
    pub root_dir: PathBuf,
}

impl GroundTruthStore {
    pub fn new(root_dir: PathBuf) -> Self {
        std::fs::create_dir_all(&root_dir).ok();
        Self { root_dir }
    }

    /// Load ground truth by ID
    pub fn load(&self, id: &str) -> Result<GroundTruth, BenchmarkError> {
        let path = self.root_dir.join(format!("{}.json", id));
        let content = std::fs::read_to_string(&path)?;
        let gt: GroundTruth = serde_json::from_str(&content)?;
        Ok(gt)
    }

    /// Save ground truth
    pub fn save(&self, gt: &GroundTruth) -> Result<(), BenchmarkError> {
        let path = self.root_dir.join(format!("{}.json", gt.id));
        let content = serde_json::to_string_pretty(gt)?;
        std::fs::write(&path, content)?;
        Ok(())
    }

    /// List all ground truths
    pub fn list(&self) -> Result<Vec<GroundTruth>, BenchmarkError> {
        let mut gts = Vec::new();
        for entry in std::fs::read_dir(&self.root_dir)? {
            let entry = entry?;
            if entry.path().extension() == Some("json".as_ref()) {
                let content = std::fs::read_to_string(entry.path())?;
                let gt: GroundTruth = serde_json::from_str(&content)?;
                gts.push(gt);
            }
        }
        Ok(gts)
    }

    /// Find ground truth for repo + config
    pub fn find(&self, repo_id: &str, config_name: &str) -> Option<GroundTruth> {
        let id = format!("{}_{}", repo_id, config_name);
        self.load(&id).ok()
    }
}
```

### 3.6. Ground Truth Validator

```rust
/// Validates benchmark results against ground truth
pub struct GroundTruthValidator {
    pub tolerance: Tolerance,
}

#[derive(Debug, Clone)]
pub struct Tolerance {
    pub duration_pct: f64,      // 5.0 = Â±5%
    pub throughput_pct: f64,    // 5.0 = Â±5%
    pub memory_pct: f64,        // 10.0 = Â±10%
    pub count_tolerance: usize, // 0 = exact match
}

impl Default for Tolerance {
    fn default() -> Self {
        Self {
            duration_pct: 5.0,
            throughput_pct: 5.0,
            memory_pct: 10.0,
            count_tolerance: 0,
        }
    }
}

#[derive(Debug, Clone)]
pub struct ValidationResult {
    pub status: ValidationStatus,
    pub violations: Vec<Violation>,
    pub summary: String,
}

#[derive(Debug, Clone)]
pub struct Violation {
    pub metric: String,
    pub expected: f64,
    pub actual: f64,
    pub diff_pct: f64,
    pub tolerance_pct: f64,
    pub severity: Severity,
}

#[derive(Debug, Clone, Copy)]
pub enum Severity {
    Critical,  // >20% regression
    High,      // 10-20% regression
    Medium,    // 5-10% regression (outside tolerance)
    Low,       // Within tolerance but worth noting
}

impl GroundTruthValidator {
    pub fn new(tolerance: Tolerance) -> Self {
        Self { tolerance }
    }

    /// Validate benchmark result against ground truth
    pub fn validate(
        &self,
        result: &BenchmarkResult,
        ground_truth: &GroundTruth,
    ) -> ValidationResult {
        let mut violations = Vec::new();

        // 1. Duration check
        let duration_diff_pct = Self::pct_diff(
            ground_truth.expected.duration_sec,
            result.duration.as_secs_f64(),
        );
        if duration_diff_pct.abs() > self.tolerance.duration_pct {
            violations.push(Violation {
                metric: "duration".to_string(),
                expected: ground_truth.expected.duration_sec,
                actual: result.duration.as_secs_f64(),
                diff_pct: duration_diff_pct,
                tolerance_pct: self.tolerance.duration_pct,
                severity: Self::classify_severity(duration_diff_pct.abs()),
            });
        }

        // 2. Throughput check
        let throughput_diff_pct = Self::pct_diff(
            ground_truth.expected.throughput_loc_per_sec,
            result.throughput_loc_per_sec,
        );
        if throughput_diff_pct.abs() > self.tolerance.throughput_pct {
            violations.push(Violation {
                metric: "throughput".to_string(),
                expected: ground_truth.expected.throughput_loc_per_sec,
                actual: result.throughput_loc_per_sec,
                diff_pct: throughput_diff_pct,
                tolerance_pct: self.tolerance.throughput_pct,
                severity: Self::classify_severity(throughput_diff_pct.abs()),
            });
        }

        // 3. Memory check
        let memory_diff_pct = Self::pct_diff(
            ground_truth.expected.memory_mb,
            result.memory_mb,
        );
        if memory_diff_pct.abs() > self.tolerance.memory_pct {
            violations.push(Violation {
                metric: "memory".to_string(),
                expected: ground_truth.expected.memory_mb,
                actual: result.memory_mb,
                diff_pct: memory_diff_pct,
                tolerance_pct: self.tolerance.memory_pct,
                severity: Self::classify_severity(memory_diff_pct.abs()),
            });
        }

        // 4. Deterministic metrics (exact match)
        if result.total_nodes != ground_truth.expected.total_nodes {
            violations.push(Violation {
                metric: "total_nodes".to_string(),
                expected: ground_truth.expected.total_nodes as f64,
                actual: result.total_nodes as f64,
                diff_pct: Self::pct_diff(
                    ground_truth.expected.total_nodes as f64,
                    result.total_nodes as f64,
                ),
                tolerance_pct: 0.0,
                severity: Severity::Critical,
            });
        }

        // ... similar for edges, chunks, symbols

        // Determine overall status
        let status = if violations.is_empty() {
            ValidationStatus::Pass
        } else {
            ValidationStatus::Fail
        };

        // Generate summary
        let summary = if violations.is_empty() {
            "âœ… All metrics within tolerance".to_string()
        } else {
            format!(
                "âŒ {} violation(s) detected:\n{}",
                violations.len(),
                violations.iter()
                    .map(|v| format!(
                        "  - {}: {:.1}% (expected: {:.2}, actual: {:.2}, tolerance: Â±{:.1}%)",
                        v.metric, v.diff_pct, v.expected, v.actual, v.tolerance_pct
                    ))
                    .collect::<Vec<_>>()
                    .join("\n")
            )
        };

        ValidationResult {
            status,
            violations,
            summary,
        }
    }

    fn pct_diff(expected: f64, actual: f64) -> f64 {
        ((actual - expected) / expected) * 100.0
    }

    fn classify_severity(diff_pct: f64) -> Severity {
        match diff_pct {
            d if d > 20.0 => Severity::Critical,
            d if d > 10.0 => Severity::High,
            d if d > 5.0 => Severity::Medium,
            _ => Severity::Low,
        }
    }
}
```

---

## Part 4: BenchmarkRunner

```rust
/// Orchestrates the entire benchmark process
pub struct BenchmarkRunner {
    pub config: BenchmarkConfig,
    pub repo: Repository,
    pub ground_truth_store: GroundTruthStore,
    pub validator: GroundTruthValidator,
}

impl BenchmarkRunner {
    pub fn new(
        config: BenchmarkConfig,
        repo: Repository,
    ) -> Self {
        let ground_truth_store = GroundTruthStore::new(
            PathBuf::from("benchmark/ground_truth")
        );

        let validator = GroundTruthValidator::new(
            config.benchmark_opts.tolerance.clone()
        );

        Self {
            config,
            repo,
            ground_truth_store,
            validator,
        }
    }

    /// Run complete benchmark workflow
    pub fn run(&self) -> Result<BenchmarkReport, BenchmarkError> {
        println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        println!("â•‘  Codegraph Benchmark (Rust-Only, Ground Truth)          â•‘");
        println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
        println!();
        println!("Repository: {}", self.repo.name);
        println!("Category:   {:?} ({} LOC)", self.repo.category, self.repo.total_loc);
        println!("Config:     {}", self.config_name());
        println!();

        // Step 1: Warmup runs
        println!("Step 1: Warmup ({} runs)...", self.config.benchmark_opts.warmup_runs);
        for i in 0..self.config.benchmark_opts.warmup_runs {
            println!("  Warmup run {}/{}...", i + 1, self.config.benchmark_opts.warmup_runs);
            self.run_single_benchmark()?;
        }
        println!();

        // Step 2: Measured runs
        println!("Step 2: Measured runs ({})...", self.config.benchmark_opts.measured_runs);
        let mut results = Vec::new();
        for i in 0..self.config.benchmark_opts.measured_runs {
            println!("  Run {}/{}...", i + 1, self.config.benchmark_opts.measured_runs);
            let result = self.run_single_benchmark()?;
            results.push(result);
        }
        println!();

        // Step 3: Aggregate results
        let avg_result = Self::aggregate_results(&results);

        // Step 4: Ground Truth validation
        let validation = if self.config.benchmark_opts.validate_ground_truth {
            println!("Step 3: Ground Truth Validation...");
            let gt = self.ground_truth_store.find(
                &self.repo.id,
                &self.config_name(),
            );

            if let Some(gt) = gt {
                let validation = self.validator.validate(&avg_result, &gt);
                println!("{}", validation.summary);
                println!();
                Some(validation)
            } else {
                println!("  âš ï¸  No ground truth found for {}_{}", self.repo.id, self.config_name());
                println!("  Run with --save-ground-truth to establish baseline");
                println!();
                None
            }
        } else {
            None
        };

        // Step 5: Generate report
        let report = BenchmarkReport {
            repo: self.repo.clone(),
            config_name: self.config_name(),
            results,
            avg_result,
            validation,
            timestamp: Self::now(),
        };

        // Step 6: Save reports
        if self.config.benchmark_opts.save_results {
            self.save_reports(&report)?;
        }

        Ok(report)
    }

    /// Run single benchmark
    fn run_single_benchmark(&self) -> Result<BenchmarkResult, BenchmarkError> {
        let service = IndexingService::new();

        let start = std::time::Instant::now();

        // Use IndexingService API from RFC-CONFIG
        let indexing_result = service.scheduled_index(
            self.repo.path.clone(),
            self.repo.id.clone(),
            true,  // with_full_analysis based on config
        )?;

        let duration = start.elapsed();

        // Collect memory stats (basic, can be enhanced)
        let memory_mb = Self::estimate_memory_mb();

        // Build BenchmarkResult from IndexingResult
        let result = BenchmarkResult {
            repo_id: self.repo.id.clone(),
            config_name: self.config_name(),
            timestamp: Self::now(),
            git_commit: GroundTruth::get_git_commit().into(),
            repo_category: self.repo.category,
            total_loc: self.repo.total_loc,
            files_count: self.repo.files.len(),
            duration,
            throughput_loc_per_sec: self.repo.total_loc as f64 / duration.as_secs_f64(),
            memory_mb,
            files_processed: indexing_result.files_processed,
            files_cached: indexing_result.files_cached,
            files_failed: indexing_result.files_failed,
            cache_hit_rate: indexing_result.cache_hit_rate,
            total_nodes: indexing_result.full_result.nodes.len(),
            total_edges: indexing_result.full_result.edges.len(),
            total_chunks: indexing_result.full_result.chunks.len(),
            total_symbols: indexing_result.full_result.symbols.len(),
            stage_durations: indexing_result.stage_durations,
            pta_summary: indexing_result.full_result.points_to_summary.map(|pta| PTASummary {
                mode_used: pta.mode_used,
                variables_count: pta.variables_count,
                constraints_count: pta.constraints_count,
                alias_pairs: pta.alias_pairs,
            }),
            taint_summary: None,  // TODO: add to IndexingResult
            repomap_summary: None,  // TODO: add to IndexingResult
            errors: indexing_result.errors,
        };

        Ok(result)
    }

    fn aggregate_results(results: &[BenchmarkResult]) -> BenchmarkResult {
        let n = results.len() as f64;
        let mut avg = results[0].clone();

        avg.duration = Duration::from_secs_f64(
            results.iter().map(|r| r.duration.as_secs_f64()).sum::<f64>() / n
        );
        avg.throughput_loc_per_sec = results.iter()
            .map(|r| r.throughput_loc_per_sec).sum::<f64>() / n;
        avg.memory_mb = results.iter()
            .map(|r| r.memory_mb).sum::<f64>() / n;

        avg
    }

    fn config_name(&self) -> String {
        format!("Preset::{:?}", self.config.pipeline.preset)
        // Or extract from PipelineConfig metadata
    }

    fn now() -> u64 {
        std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs()
    }

    fn estimate_memory_mb() -> f64 {
        // TODO: Use jemalloc stats or /proc/self/status
        100.0
    }

    fn save_reports(&self, report: &BenchmarkReport) -> Result<(), BenchmarkError> {
        // Create output directory
        let output_dir = self.config.benchmark_opts.output_dir
            .join(&self.repo.id)
            .join(&self.config_name());
        std::fs::create_dir_all(&output_dir)?;

        // Save JSON
        let json_path = output_dir.join("result.json");
        let json = serde_json::to_string_pretty(report)?;
        std::fs::write(&json_path, json)?;
        println!("ğŸ“„ JSON saved: {:?}", json_path);

        // Save Markdown
        let md_path = output_dir.join("report.md");
        let md = self.generate_markdown_report(report);
        std::fs::write(&md_path, md)?;
        println!("ğŸ“„ Markdown saved: {:?}", md_path);

        // TODO: Save HTML waterfall

        Ok(())
    }

    fn generate_markdown_report(&self, report: &BenchmarkReport) -> String {
        format!(
            r#"# Benchmark Report: {}

**Repository**: {} ({:?}, {} LOC)
**Configuration**: {}
**Timestamp**: {}
**Git Commit**: {}

## Summary

| Metric | Value |
|--------|-------|
| Duration | {:.2}s |
| Throughput | {:.0} LOC/sec |
| Memory | {:.1} MB |
| Nodes | {} |
| Edges | {} |
| Chunks | {} |
| Symbols | {} |

## Ground Truth Validation

{}

## Stage Breakdown

| Stage | Duration | % of Total |
|-------|----------|------------|
{}

"#,
            self.repo.name,
            self.repo.id,
            self.repo.category,
            self.repo.total_loc,
            report.config_name,
            report.timestamp,
            report.avg_result.git_commit.as_ref().unwrap_or(&"N/A".to_string()),
            report.avg_result.duration.as_secs_f64(),
            report.avg_result.throughput_loc_per_sec,
            report.avg_result.memory_mb,
            report.avg_result.total_nodes,
            report.avg_result.total_edges,
            report.avg_result.total_chunks,
            report.avg_result.total_symbols,
            report.validation.as_ref()
                .map(|v| v.summary.clone())
                .unwrap_or_else(|| "N/A".to_string()),
            report.avg_result.stage_durations.iter()
                .map(|(stage, dur)| {
                    let pct = (dur.as_secs_f64() / report.avg_result.duration.as_secs_f64()) * 100.0;
                    format!("| {} | {:.2}s | {:.1}% |", stage, dur.as_secs_f64(), pct)
                })
                .collect::<Vec<_>>()
                .join("\n")
        )
    }
}

#[derive(Debug, Clone)]
pub struct BenchmarkReport {
    pub repo: Repository,
    pub config_name: String,
    pub results: Vec<BenchmarkResult>,
    pub avg_result: BenchmarkResult,
    pub validation: Option<ValidationResult>,
    pub timestamp: u64,
}
```

---

## Part 5: CLI Interface

### 5.1. Command Structure

```bash
# packages/codegraph-ir/src/bin/bench-codegraph.rs

cargo bench-codegraph [OPTIONS] <SUBCOMMAND>

Subcommands:
  run           Run benchmark
  save-gt       Save ground truth
  list-gt       List all ground truths
  update-gt     Update existing ground truth
  compare       Compare multiple benchmarks
  regression    Run regression test suite

Global Options:
  --repo <PATH>         Repository path
  --preset <PRESET>     Preset: fast|balanced|thorough
  --config <YAML>       Custom config YAML
  --output <DIR>        Output directory
```

### 5.2. Subcommand Examples

```bash
# 1. Run benchmark with preset
cargo bench-codegraph run --repo tools/benchmark/repo-test/small/typer --preset balanced

# 2. Run with custom config
cargo bench-codegraph run --repo /path/to/django --config security-audit.yaml

# 3. Save ground truth (after verifying results)
cargo bench-codegraph save-gt --repo tools/benchmark/repo-test/small/typer --preset balanced

# 4. List all ground truths
cargo bench-codegraph list-gt

# 5. Update ground truth (requires --reason)
cargo bench-codegraph update-gt \
  --repo typer \
  --preset balanced \
  --reason "Optimized cross-file resolution by 15%"

# 6. Compare presets
cargo bench-codegraph compare \
  --repo typer \
  --presets fast,balanced,thorough

# 7. Regression test (validate all ground truths)
cargo bench-codegraph regression
```

---

## Part 6: Ground Truth Management Workflow

### 6.1. Initial Setup (Release v1.0.0)

```bash
# 1. Establish ground truth for all presets Ã— all repos

# Small repo (typer)
cargo bench-codegraph save-gt --repo tools/benchmark/repo-test/small/typer --preset fast
cargo bench-codegraph save-gt --repo tools/benchmark/repo-test/small/typer --preset balanced
cargo bench-codegraph save-gt --repo tools/benchmark/repo-test/small/typer --preset thorough

# Medium repo (rich)
cargo bench-codegraph save-gt --repo tools/benchmark/repo-test/medium/rich --preset fast
cargo bench-codegraph save-gt --repo tools/benchmark/repo-test/medium/rich --preset balanced

# Large repo (django)
# (omit thorough preset for large repos - too slow)
cargo bench-codegraph save-gt --repo tools/benchmark/repo-test/large/django --preset fast
cargo bench-codegraph save-gt --repo tools/benchmark/repo-test/large/django --preset balanced

# Result: benchmark/ground_truth/
#   - typer_Preset::Fast.json
#   - typer_Preset::Balanced.json
#   - typer_Preset::Thorough.json
#   - rich_Preset::Fast.json
#   - rich_Preset::Balanced.json
#   - django_Preset::Fast.json
#   - django_Preset::Balanced.json
```

### 6.2. Daily Development (PR Workflow)

```yaml
# .github/workflows/benchmark-regression.yml
name: Benchmark Regression Test

on:
  pull_request:
    paths:
      - 'packages/codegraph-ir/**'
      - 'packages/codegraph-storage/**'

jobs:
  regression:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Install Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1

      - name: Run regression test
        run: |
          cargo bench-codegraph regression --fail-fast

      - name: Post comment on failure
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'âŒ **Performance regression detected!**\n\nPlease investigate or update ground truth with `--reason`.'
            })
```

**Developer Experience:**
1. PR ìƒì„±
2. CIê°€ ìë™ìœ¼ë¡œ `regression` ì‹¤í–‰
3. Ground Truth ëŒ€ë¹„ Â±5% ì´ˆê³¼ ì‹œ â†’ âŒ Fail
4. ë‘ ê°€ì§€ ì„ íƒ:
   - **Option A**: ì„±ëŠ¥ ì €í•˜ ìˆ˜ì • (ì½”ë“œ ìµœì í™”)
   - **Option B**: ì˜ë„ì  ë³€ê²½ì´ë©´ Ground Truth ì—…ë°ì´íŠ¸ (ì´ìœ  ëª…ì‹œ)

### 6.3. Monthly Review (Ground Truth Update)

```bash
# Scenario: Cross-file resolution ìµœì í™”ë¡œ 15% ì„±ëŠ¥ í–¥ìƒ

# 1. ìµœì í™” ì‘ì—… í›„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
cargo bench-codegraph run --repo typer --preset balanced
# Result: Throughput 3200 â†’ 3680 LOC/sec (+15%)

# 2. Ground Truth ì—…ë°ì´íŠ¸ (ì´ìœ  í•„ìˆ˜)
cargo bench-codegraph update-gt \
  --repo typer \
  --preset balanced \
  --reason "RFC-042: Optimized cross-file resolution with caching (+15%)"

# 3. Git commit
git add benchmark/ground_truth/typer_Preset::Balanced.json
git commit -m "chore: Update ground truth for typer/balanced (+15% by RFC-042)"

# 4. Team review í•„ìˆ˜ (PR)
```

**Update Log (in GroundTruth):**
```json
{
  "id": "typer_Preset::Balanced",
  "last_updated_at": 1735488000,
  "last_updated_by": "a1b2c3d4...",
  "update_reason": "RFC-042: Optimized cross-file resolution with caching (+15%)",
  "history": [
    {
      "timestamp": 1735401600,
      "commit": "abc123...",
      "reason": "Initial baseline"
    },
    {
      "timestamp": 1735488000,
      "commit": "a1b2c3d4...",
      "reason": "RFC-042: Optimized cross-file resolution with caching (+15%)"
    }
  ]
}
```

---

## Part 7: Report Formats

### 7.1. Terminal Output (Pretty)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Codegraph Benchmark - typer (Preset::Balanced)         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Repository: typer (Small, 8,234 LOC)
Configuration: Preset::Balanced
Runs: 3 (after 1 warmup)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Performance Summary                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Duration:     2.45s                                     â”‚
â”‚  Throughput:   3,362 LOC/sec                             â”‚
â”‚  Memory:       148 MB                                    â”‚
â”‚  Nodes:        12,456                                    â”‚
â”‚  Edges:        8,921                                     â”‚
â”‚  Chunks:       234                                       â”‚
â”‚  Symbols:      1,089                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ground Truth Validation                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… All metrics within tolerance                         â”‚
â”‚                                                          â”‚
â”‚  Duration:     2.45s vs 2.50s expected (-2.0%, âœ“)       â”‚
â”‚  Throughput:   3,362 vs 3,200 expected (+5.1%, âœ“)       â”‚
â”‚  Memory:       148 MB vs 150 MB expected (-1.3%, âœ“)     â”‚
â”‚  Nodes:        12,456 vs 12,456 expected (exact, âœ“)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage Breakdown                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  L1_IR_Build          0.45s  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  18.4%    â”‚
â”‚  L2_Chunking          0.12s  â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   4.9%    â”‚
â”‚  L3_CrossFile         0.68s  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  27.8%    â”‚
â”‚  L4_Occurrences       0.34s  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  13.9%    â”‚
â”‚  L5_Symbols           0.21s  â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   8.6%    â”‚
â”‚  L6_PointsTo          0.52s  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  21.2%    â”‚
â”‚  L14_TaintAnalysis    0.08s  â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   3.3%    â”‚
â”‚  L16_RepoMap          0.05s  â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   2.0%    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Reports saved:
  ğŸ“„ target/benchmark_results/typer/Preset::Balanced/result.json
  ğŸ“„ target/benchmark_results/typer/Preset::Balanced/report.md
```

### 7.2. JSON Output

```json
{
  "repo": {
    "id": "typer",
    "name": "typer",
    "category": "Small",
    "total_loc": 8234,
    "files_count": 45
  },
  "config_name": "Preset::Balanced",
  "timestamp": 1735488000,
  "avg_result": {
    "duration_sec": 2.45,
    "throughput_loc_per_sec": 3362,
    "memory_mb": 148,
    "total_nodes": 12456,
    "total_edges": 8921,
    "stage_durations": {
      "L1_IR_Build": 0.45,
      "L2_Chunking": 0.12,
      "L3_CrossFile": 0.68
    }
  },
  "validation": {
    "status": "Pass",
    "violations": [],
    "summary": "âœ… All metrics within tolerance"
  }
}
```

### 7.3. Markdown Report

(Already shown in BenchmarkRunner::generate_markdown_report)

### 7.4. HTML Waterfall (Future)

```html
<!-- Interactive waterfall chart with:
     - Timeline visualization
     - Hover tooltips
     - Stage filtering
     - Multi-run comparison
-->
```

---

## Part 8: Integration with RFC-CONFIG

```rust
// Perfect integration example

// 1. Use PipelineConfig from RFC-CONFIG
let pipeline_config = PipelineConfig::preset(Preset::Balanced)
    .taint(|c| c.max_depth(50))
    .build()?;

// 2. Wrap in BenchmarkConfig
let bench_config = BenchmarkConfig {
    pipeline: pipeline_config,
    benchmark_opts: BenchmarkOptions::default(),
};

// 3. Run benchmark
let repo = Repository::from_path(PathBuf::from("tools/benchmark/repo-test/small/typer"))?;
let runner = BenchmarkRunner::new(bench_config, repo);
let report = runner.run()?;

// 4. Validate against ground truth
if let Some(validation) = report.validation {
    if validation.status == ValidationStatus::Fail {
        eprintln!("âŒ Performance regression detected!");
        std::process::exit(1);
    }
}
```

---

## Part 9: Migration Plan

### Phase 1: Core Infrastructure (Week 1)

**Goal**: Basic benchmark framework

- [ ] Create `packages/codegraph-ir/src/benchmark/` module
- [ ] Implement core types:
  - [ ] `BenchmarkConfig`
  - [ ] `Repository`
  - [ ] `BenchmarkResult`
  - [ ] `GroundTruth`
  - [ ] `GroundTruthStore`
  - [ ] `GroundTruthValidator`
- [ ] Implement `BenchmarkRunner::run()`
- [ ] Basic terminal output

### Phase 2: CLI Tool (Week 2)

**Goal**: Usable CLI

- [ ] Create `packages/codegraph-ir/src/bin/bench-codegraph.rs`
- [ ] Implement subcommands:
  - [ ] `run`
  - [ ] `save-gt`
  - [ ] `list-gt`
- [ ] Integrate with RFC-CONFIG `PipelineConfig`
- [ ] Repository auto-discovery

### Phase 3: Reports (Week 3)

**Goal**: Rich reporting

- [ ] JSON export
- [ ] Markdown report
- [ ] Terminal pretty-print
- [ ] HTML waterfall (optional)

### Phase 4: CI Integration (Week 4)

**Goal**: Automated regression testing

- [ ] Implement `regression` subcommand
- [ ] GitHub Actions workflow
- [ ] Ground Truth initial baselines
- [ ] Documentation

---

## Part 10: Success Metrics

### Quantitative

- [ ] **30+ scattered benchmarks â†’ 1 unified tool**
- [ ] **0 ground truths â†’ 7+ baselines** (3 repos Ã— 2-3 presets)
- [ ] **0% regression detection â†’ 100%** (CI blocks bad PRs)
- [ ] **Manual comparison â†’ Automated validation**

### Qualitative

- [ ] "Ground Truthë¡œ ì„±ëŠ¥ íšŒê·€ ì¦‰ì‹œ ë°œê²¬" (DevOps)
- [ ] "RFC-CONFIGì™€ ì™„ë²½ í†µí•©" (Dev)
- [ ] "CIì—ì„œ ìë™ìœ¼ë¡œ ì„±ëŠ¥ ë³´ì¥" (QA)
- [ ] "ë¦¬í¬íŠ¸ê°€ ì½ê¸° ì‰½ê³  ì´í•´í•˜ê¸° ì‰¬ì›€" (PM)

---

## Appendix A: Well-Known Repositories

```rust
// benchmark/repos/presets.rs

/// Curated list of well-known repositories for benchmarking
pub struct WellKnownRepos;

impl WellKnownRepos {
    pub fn list() -> Vec<Repository> {
        vec![
            // Small (< 10k LOC)
            Self::typer(),
            Self::attrs(),

            // Medium (10k - 100k LOC)
            Self::rich(),
            Self::fastapi(),

            // Large (> 100k LOC)
            Self::django(),
            Self::pandas(),
        ]
    }

    fn typer() -> Repository {
        Repository::from_path(
            PathBuf::from("tools/benchmark/repo-test/small/typer")
        ).expect("typer repo not found")
    }

    // ... similar for others
}
```

---

## Appendix B: Tolerance Tuning Guide

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric         â”‚ Default â”‚ Rationale                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Duration       â”‚  Â±5%    â”‚ CPU throttling, GC noise     â”‚
â”‚ Throughput     â”‚  Â±5%    â”‚ Inverse of duration          â”‚
â”‚ Memory         â”‚  Â±10%   â”‚ More variable than CPU       â”‚
â”‚ Nodes/Edges    â”‚  0      â”‚ Deterministic, exact match   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Tuning Tips:
1. Start with default (5% / 10%)
2. Run 10 benchmarks, check stddev
3. If stddev > tolerance â†’ increase tolerance
4. If stddev << tolerance â†’ decrease tolerance
5. Re-run after tuning to validate
```

---

## Appendix C: Comparison with Existing Tools

| Feature | benchmark_large_repos.rs | bench_indexing.py | RFC-002 (This) |
|---------|--------------------------|-------------------|----------------|
| **Language** | Rust | Python | Rust |
| **Ground Truth** | âŒ | âŒ | âœ… |
| **Regression Test** | âŒ | âŒ | âœ… |
| **Config Integration** | âŒ | âŒ | âœ… (RFC-CONFIG) |
| **Multi-repo** | âŒ | âŒ | âœ… |
| **Reports** | CSV, Waterfall | Text | JSON, MD, HTML |
| **CI Integration** | âŒ | âŒ | âœ… |
| **Overhead** | Low | High | Low |

---

## Appendix D: Python Bindings (PyO3)

### D.1. Problem: Rust Build Time

**í˜„ì¬ ë¬¸ì œ:**
- Rust full build: 2-5ë¶„ (clean build)
- Incremental build: 30ì´ˆ-1ë¶„
- ì„¤ì • ë³€ê²½ë§ˆë‹¤ ì¬ë¹Œë“œ í•„ìš” â†’ DX ì €í•˜

**í•´ê²°ì±… 2ê°€ì§€:**

#### Solution A: Python Bindings (PyO3) - ë¹ ë¥¸ ë°˜ë³µ

```python
# Pythonì—ì„œ ì§ì ‘ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (Rust ì¬ë¹Œë“œ ë¶ˆí•„ìš”)
from codegraph_ir import BenchmarkRunner, PipelineConfig, Preset

# 1. Preset ì‚¬ìš©
config = PipelineConfig.preset(Preset.BALANCED)
runner = BenchmarkRunner(config, repo_path="tools/benchmark/repo-test/small/typer")
report = runner.run()

# 2. ì„¤ì • ë™ì  ë³€ê²½ (Rust ì¬ë¹Œë“œ ì—†ìŒ!)
config = PipelineConfig.preset(Preset.BALANCED)
config.taint.max_depth = 50
config.taint.max_paths = 1000
report = runner.run()

# 3. YAML ë¡œë“œ
config = PipelineConfig.from_yaml("my-config.yaml")
report = runner.run()

# 4. ì—¬ëŸ¬ ì„¤ì • ë¹„êµ (í•œ ë²ˆ ì‹¤í–‰)
for preset in [Preset.FAST, Preset.BALANCED, Preset.THOROUGH]:
    config = PipelineConfig.preset(preset)
    runner = BenchmarkRunner(config, repo_path="typer")
    report = runner.run()
    print(f"{preset}: {report.avg_result.duration_sec:.2f}s")
```

**ì¥ì :**
- âœ… Rust ì¬ë¹Œë“œ ë¶ˆí•„ìš”
- âœ… ë¹ ë¥¸ ë°˜ë³µ (ì„¤ì • ë³€ê²½ ì¦‰ì‹œ ì‹¤í–‰)
- âœ… Jupyter Notebook ì§€ì›
- âœ… Pandas/Matplotlibë¡œ ë¶„ì„ ê°€ëŠ¥

**ë‹¨ì :**
- âš ï¸ ì´ˆê¸° maturin build í•„ìš” (1íšŒë§Œ)
- âš ï¸ PyO3 ë°”ì¸ë”© ìœ ì§€ë³´ìˆ˜

#### Solution B: Rust Incremental Build ìµœì í™”

**B.1. Cargo Workspace ë¶„ë¦¬**

```toml
# Current (monolithic):
packages/codegraph-ir/Cargo.toml  # 1ê°œ í° crate â†’ ë³€ê²½ ì‹œ ì „ì²´ ì¬ë¹Œë“œ

# Optimized (split):
packages/codegraph-ir/
â”œâ”€â”€ codegraph-ir-core/       # í•µì‹¬ ë¡œì§ (ë³€ê²½ ì ìŒ)
â”œâ”€â”€ codegraph-ir-config/     # RFC-CONFIG (ë³€ê²½ ë§ìŒ)
â”œâ”€â”€ codegraph-ir-benchmark/  # ë²¤ì¹˜ë§ˆí¬ (ë³€ê²½ ë§ìŒ)
â””â”€â”€ codegraph-ir/            # í†µí•© (re-export)
```

**íš¨ê³¼:**
- Config ë³€ê²½ ì‹œ `codegraph-ir-config`ë§Œ ì¬ë¹Œë“œ (5ì´ˆ)
- Core ë¡œì§ì€ ìºì‹œ ì‚¬ìš©

**B.2. Feature Flagsë¡œ ë¹Œë“œ ì‹œê°„ ë‹¨ì¶•**

```toml
# Cargo.toml
[features]
default = ["benchmark"]
benchmark = []
full-analysis = ["pta", "taint", "repomap"]
pta = []
taint = []
repomap = []

# ë²¤ì¹˜ë§ˆí¬ë§Œ ë¹Œë“œ (PTA/Taint ì œì™¸)
cargo build --no-default-features --features benchmark
# â†’ ë¹Œë“œ ì‹œê°„ 50% ê°ì†Œ
```

**B.3. sccacheë¡œ ë¹Œë“œ ìºì‹œ**

```bash
# ì„¤ì¹˜
cargo install sccache

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export RUSTC_WRAPPER=sccache

# ë¹Œë“œ (ìµœì´ˆ: 2ë¶„, ì´í›„: 10ì´ˆ)
cargo build --release

# ìºì‹œ í†µê³„ í™•ì¸
sccache --show-stats
```

**íš¨ê³¼:**
- CI/ë¡œì»¬ ê°„ ìºì‹œ ê³µìœ 
- Clean buildë„ 10-20ì´ˆ

**B.4. mold ë§ì»¤ (Linux) or lld (Mac)**

```toml
# .cargo/config.toml
[target.x86_64-unknown-linux-gnu]
linker = "clang"
rustflags = ["-C", "link-arg=-fuse-ld=mold"]

[target.x86_64-apple-darwin]
rustflags = ["-C", "link-arg=-fuse-ld=/usr/local/opt/llvm/bin/ld64.lld"]
```

**íš¨ê³¼:**
- ë§í‚¹ ì‹œê°„ 80% ê°ì†Œ (10ì´ˆ â†’ 2ì´ˆ)

---

### D.2. Recommended Approach (Hybrid)

**For Development (ë¹ ë¥¸ ë°˜ë³µ):**
```python
# tools/benchmark/bench_quick.py
from codegraph_ir import BenchmarkRunner, PipelineConfig, Preset

# ì„¤ì •ë§Œ ë°”ê¿”ê°€ë©° ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
configs = [
    PipelineConfig.preset(Preset.FAST),
    PipelineConfig.preset(Preset.BALANCED),
    PipelineConfig.preset(Preset.BALANCED).with_taint(max_depth=100),
]

for i, config in enumerate(configs):
    print(f"\n=== Config {i+1} ===")
    runner = BenchmarkRunner(config, repo_path="typer")
    report = runner.run()
    print(f"Duration: {report.avg_result.duration_sec:.2f}s")
```

**For CI/Production (ì •í™•í•œ ì¸¡ì •):**
```bash
# Rust CLIë¡œ Ground Truth ê²€ì¦
cargo bench-codegraph regression --fail-fast
```

---

### D.3. PyO3 Implementation Plan

**Phase 1: Core Bindings**

```rust
// packages/codegraph-ir/src/python/mod.rs

use pyo3::prelude::*;

#[pyclass]
pub struct PyPipelineConfig {
    inner: crate::config::PipelineConfig,
}

#[pymethods]
impl PyPipelineConfig {
    #[staticmethod]
    fn preset(preset: &str) -> PyResult<Self> {
        let preset = match preset {
            "fast" => crate::config::Preset::Fast,
            "balanced" => crate::config::Preset::Balanced,
            "thorough" => crate::config::Preset::Thorough,
            _ => return Err(PyErr::new::<pyo3::exceptions::PyValueError, _>(
                format!("Unknown preset: {}", preset)
            )),
        };

        Ok(Self {
            inner: crate::config::PipelineConfig::preset(preset)
                .build()
                .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                    format!("{:?}", e)
                ))?,
        })
    }

    #[staticmethod]
    fn from_yaml(path: &str) -> PyResult<Self> {
        Ok(Self {
            inner: crate::config::PipelineConfig::from_yaml(path)
                .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                    format!("{:?}", e)
                ))?,
        })
    }

    // Getter/Setter for dynamic modification
    #[getter]
    fn taint_max_depth(&self) -> usize {
        self.inner.effective_taint().max_depth
    }

    #[setter]
    fn set_taint_max_depth(&mut self, value: usize) {
        // TODO: Implement mutable config
        // self.inner.taint.max_depth = value;
    }
}

#[pyclass]
pub struct PyBenchmarkRunner {
    config: crate::benchmark::BenchmarkConfig,
    repo_path: PathBuf,
}

#[pymethods]
impl PyBenchmarkRunner {
    #[new]
    fn new(config: PyPipelineConfig, repo_path: &str) -> PyResult<Self> {
        let repo = crate::benchmark::Repository::from_path(
            PathBuf::from(repo_path)
        ).map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
            format!("{:?}", e)
        ))?;

        Ok(Self {
            config: crate::benchmark::BenchmarkConfig {
                pipeline: config.inner,
                benchmark_opts: Default::default(),
            },
            repo_path: PathBuf::from(repo_path),
        })
    }

    fn run(&self) -> PyResult<PyBenchmarkReport> {
        let repo = crate::benchmark::Repository::from_path(
            self.repo_path.clone()
        ).map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
            format!("{:?}", e)
        ))?;

        let runner = crate::benchmark::BenchmarkRunner::new(
            self.config.clone(),
            repo,
        );

        let report = runner.run()
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                format!("{:?}", e)
            ))?;

        Ok(PyBenchmarkReport { inner: report })
    }
}

#[pyclass]
pub struct PyBenchmarkReport {
    inner: crate::benchmark::BenchmarkReport,
}

#[pymethods]
impl PyBenchmarkReport {
    #[getter]
    fn duration_sec(&self) -> f64 {
        self.inner.avg_result.duration.as_secs_f64()
    }

    #[getter]
    fn throughput_loc_per_sec(&self) -> f64 {
        self.inner.avg_result.throughput_loc_per_sec
    }

    #[getter]
    fn memory_mb(&self) -> f64 {
        self.inner.avg_result.memory_mb
    }

    fn to_dict(&self) -> PyResult<HashMap<String, PyObject>> {
        Python::with_gil(|py| {
            let mut map = HashMap::new();
            map.insert("duration_sec".to_string(), self.duration_sec().to_object(py));
            map.insert("throughput".to_string(), self.throughput_loc_per_sec().to_object(py));
            map.insert("memory_mb".to_string(), self.memory_mb().to_object(py));
            // ... add more fields
            Ok(map)
        })
    }

    fn __repr__(&self) -> String {
        format!(
            "BenchmarkReport(duration={:.2}s, throughput={:.0} LOC/sec, memory={:.1} MB)",
            self.duration_sec(),
            self.throughput_loc_per_sec(),
            self.memory_mb()
        )
    }
}

#[pymodule]
fn codegraph_ir_benchmark(_py: Python, m: &PyModule) -> PyResult<()> {
    m.add_class::<PyPipelineConfig>()?;
    m.add_class::<PyBenchmarkRunner>()?;
    m.add_class::<PyBenchmarkReport>()?;
    Ok(())
}
```

**Build with maturin:**

```toml
# pyproject.toml
[build-system]
requires = ["maturin>=1.0,<2.0"]
build-backend = "maturin"

[project]
name = "codegraph-ir-benchmark"
requires-python = ">=3.10"
```

```bash
# Development build (incremental, 5-10ì´ˆ)
maturin develop

# Release build (ìµœì´ˆ 1íšŒë§Œ, 2ë¶„)
maturin build --release
pip install target/wheels/*.whl
```

---

### D.4. Python Benchmark Script Example

```python
#!/usr/bin/env python3
"""
Quick benchmark script using Python bindings (no Rust rebuild needed)

Usage:
    python tools/benchmark/bench_quick.py --repo typer --preset balanced
    python tools/benchmark/bench_quick.py --repo typer --config my-config.yaml
    python tools/benchmark/bench_quick.py --repo typer --compare-presets
"""

import argparse
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

from codegraph_ir_benchmark import (
    BenchmarkRunner,
    PipelineConfig,
    Preset,
)


def run_single(repo_path: str, config: PipelineConfig):
    """Run single benchmark"""
    runner = BenchmarkRunner(config, repo_path=repo_path)
    report = runner.run()

    print(f"\n{'='*60}")
    print(f"Duration:    {report.duration_sec:.2f}s")
    print(f"Throughput:  {report.throughput_loc_per_sec:.0f} LOC/sec")
    print(f"Memory:      {report.memory_mb:.1f} MB")
    print(f"{'='*60}\n")

    return report


def compare_presets(repo_path: str):
    """Compare all presets"""
    presets = [Preset.FAST, Preset.BALANCED, Preset.THOROUGH]
    results = []

    for preset in presets:
        print(f"\nğŸ”¥ Running {preset}...")
        config = PipelineConfig.preset(preset)
        runner = BenchmarkRunner(config, repo_path=repo_path)
        report = runner.run()

        results.append({
            'preset': preset,
            'duration_sec': report.duration_sec,
            'throughput': report.throughput_loc_per_sec,
            'memory_mb': report.memory_mb,
        })

    # Create DataFrame
    df = pd.DataFrame(results)
    print("\n" + "="*60)
    print("Comparison Summary:")
    print("="*60)
    print(df.to_string(index=False))

    # Plot
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    axes[0].bar(df['preset'], df['duration_sec'])
    axes[0].set_title('Duration (lower is better)')
    axes[0].set_ylabel('Seconds')

    axes[1].bar(df['preset'], df['throughput'])
    axes[1].set_title('Throughput (higher is better)')
    axes[1].set_ylabel('LOC/sec')

    axes[2].bar(df['preset'], df['memory_mb'])
    axes[2].set_title('Memory Usage')
    axes[2].set_ylabel('MB')

    plt.tight_layout()
    plt.savefig('benchmark_comparison.png')
    print("\nğŸ“Š Chart saved: benchmark_comparison.png")


def sweep_taint_depth(repo_path: str):
    """Sweep taint max_depth parameter"""
    depths = [10, 20, 30, 50, 100, 200]
    results = []

    for depth in depths:
        print(f"\nğŸ”¥ Running with taint.max_depth={depth}...")
        config = PipelineConfig.preset(Preset.BALANCED)
        # TODO: Add setter for taint.max_depth
        # config.taint.max_depth = depth

        runner = BenchmarkRunner(config, repo_path=repo_path)
        report = runner.run()

        results.append({
            'max_depth': depth,
            'duration_sec': report.duration_sec,
        })

    df = pd.DataFrame(results)
    print("\n" + "="*60)
    print("Taint Depth Sweep:")
    print("="*60)
    print(df.to_string(index=False))

    # Plot
    plt.figure(figsize=(10, 6))
    plt.plot(df['max_depth'], df['duration_sec'], marker='o')
    plt.xlabel('Taint Max Depth')
    plt.ylabel('Duration (seconds)')
    plt.title('Impact of Taint Max Depth on Performance')
    plt.grid(True)
    plt.savefig('taint_depth_sweep.png')
    print("\nğŸ“Š Chart saved: taint_depth_sweep.png")


def main():
    parser = argparse.ArgumentParser(description='Quick benchmark (Python)')
    parser.add_argument('--repo', required=True, help='Repository path')
    parser.add_argument('--preset', choices=['fast', 'balanced', 'thorough'], help='Preset')
    parser.add_argument('--config', help='YAML config path')
    parser.add_argument('--compare-presets', action='store_true', help='Compare all presets')
    parser.add_argument('--sweep-taint-depth', action='store_true', help='Sweep taint depth')

    args = parser.parse_args()

    if args.compare_presets:
        compare_presets(args.repo)
    elif args.sweep_taint_depth:
        sweep_taint_depth(args.repo)
    elif args.preset:
        config = PipelineConfig.preset(args.preset.upper())
        run_single(args.repo, config)
    elif args.config:
        config = PipelineConfig.from_yaml(args.config)
        run_single(args.repo, config)
    else:
        parser.error("Specify --preset, --config, --compare-presets, or --sweep-taint-depth")


if __name__ == '__main__':
    main()
```

**Usage:**

```bash
# 1. Initial build (1íšŒë§Œ, 2ë¶„)
maturin develop --release

# 2. ì´í›„ ì„¤ì • ë³€ê²½ì€ Rust ì¬ë¹Œë“œ ë¶ˆí•„ìš”!
python tools/benchmark/bench_quick.py --repo tools/benchmark/repo-test/small/typer --preset balanced

# 3. ëª¨ë“  Preset ë¹„êµ (3ë²ˆ ì‹¤í–‰, Rust ì¬ë¹Œë“œ 0ì´ˆ)
python tools/benchmark/bench_quick.py --repo typer --compare-presets

# 4. Taint depth sweep (6ë²ˆ ì‹¤í–‰, Rust ì¬ë¹Œë“œ 0ì´ˆ)
python tools/benchmark/bench_quick.py --repo typer --sweep-taint-depth
```

---

### D.5. Build Time Comparison

| Approach | Initial Build | Config Change | Total (10 iterations) |
|----------|---------------|---------------|----------------------|
| **Rust CLI only** | 2ë¶„ | 30ì´ˆ | 2ë¶„ + 10Ã—30ì´ˆ = 7ë¶„ |
| **Python + maturin** | 2ë¶„ (1íšŒ) | **0ì´ˆ** | 2ë¶„ + 10Ã—0ì´ˆ = **2ë¶„** |
| **Rust + sccache + workspace split** | 2ë¶„ (1íšŒ) | 5ì´ˆ | 2ë¶„ + 10Ã—5ì´ˆ = 2ë¶„ 50ì´ˆ |

**Winner**: Python bindings (3.5ë°° ë¹ ë¦„)

---

### D.6. Migration Strategy

**Week 1-2: Rust-only (RFC-002 Phase 1-2)**
- Ground Truth ì‹œìŠ¤í…œ êµ¬ì¶•
- CLI ê¸°ë³¸ ê¸°ëŠ¥

**Week 3: Python Bindings**
- PyO3 ë°”ì¸ë”© ì¶”ê°€
- `bench_quick.py` ìŠ¤í¬ë¦½íŠ¸

**Week 4: Both Available**
- Python: ê°œë°œ/ì‹¤í—˜ìš© (ë¹ ë¥¸ ë°˜ë³µ)
- Rust CLI: CI/í”„ë¡œë•ì…˜ìš© (ì •í™•í•œ ì¸¡ì •)

---

## Decision

**Approve**: [ ]
**Revise**: [ ]
**Reject**: [ ]

**Reviewers**: _____________
**Date**: _____________

---

**RFC End**
