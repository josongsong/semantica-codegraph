# Core Architecture ADR êµ¬í˜„ ê³„íš (ADR-001~004)

**ì‘ì„±ì¼**: 2025-12-05  
**ëŒ€ìƒ**: Core Architecture 4ê°œ ADR (P0)  
**ê¸°ê°„**: 8ì£¼ (2ê°œì›”)  
**í˜„ì¬ ì§„í–‰ë¥ **: 45% â†’ 100%

---

## ğŸ“Š Executive Summary

### í˜„ì¬ ìƒíƒœ

| ADR | ì œëª© | ì§„í–‰ë¥  | ìƒíƒœ |
|-----|------|--------|------|
| ADR-001 | 4-Layer Architecture | 30% | ğŸŸ¡ Layer êµ¬ì¡°ë§Œ, í†µí•© ì—†ìŒ |
| ADR-002 | Router vs TaskGraph | 90% | âœ… ë¶„ë¦¬ ì™„ë£Œ, ë¬¸ì„œí™” í•„ìš” |
| ADR-003 | Graph Workflow Engine | 40% | ğŸŸ¡ êµ¬ì¡°ë§Œ, Steps Mock |
| ADR-004 | Sandbox Executor | 20% | âš ï¸ ShadowFSë§Œ, Sandbox ì—†ìŒ |

### ëª©í‘œ

**8ì£¼ í›„**: 4ê°œ ADR ëª¨ë‘ 100% êµ¬í˜„ + Production Ready

```
Week 1-2: ADR-001 Layer í†µí•© (30% â†’ 60%)
Week 3-4: ADR-001 MetaLayer (60% â†’ 95%)
Week 5:   ADR-003 Workflow ì‹¤ì œ êµ¬í˜„ (40% â†’ 85%)
Week 6:   ADR-004 Sandbox êµ¬ì¶• (20% â†’ 90%)
Week 7:   ADR-002 ë¬¸ì„œí™” + E2E (90% â†’ 100%)
Week 8:   í†µí•© í…ŒìŠ¤íŠ¸ + ì„±ëŠ¥ ìµœì í™” (95% â†’ 100%)
```

---

## ADR-001: 4-Layer Agent Architecture

### ğŸ“Œ í˜„ì¬ ìƒíƒœ: 30%

**ì™„ë£Œ**:
- âœ… Layerë³„ ë””ë ‰í† ë¦¬ êµ¬ì¡°
- âœ… ê¸°ë³¸ ë°ì´í„° ëª¨ë¸
- âœ… Phase 0 Orchestrator (Mock)

**ë¯¸ì™„ë£Œ**:
- âŒ Layer ê°„ ì‹¤ì œ ë°ì´í„° íë¦„
- âŒ MetaLayer ì‹¤ì œ ë™ì‘
- âŒ Error propagation

### ğŸ¯ êµ¬í˜„ ê³„íš

#### **Week 1-2: Layer í†µí•© ê¸°ë°˜** (30% â†’ 60%)

**Goal**: Routerê°€ ì‹¤ì œ CodeGraph ì‚¬ìš©

**Day 1-3: ContextAdapter êµ¬í˜„**
```python
# src/agent/adapters/context_adapter.py

class ContextAdapter:
    """Layer 0 (CodeGraph) Facade"""
    
    def __init__(
        self,
        retrieval_service: RetrievalService,
        chunk_store: ChunkStore,
        graph_store: GraphStore,
    ):
        self.retrieval = retrieval_service
        self.chunks = chunk_store
        self.graph = graph_store
    
    async def search_relevant_code(
        self,
        query: str,
        intent: str,
        top_k: int = 10,
    ) -> SearchResult:
        """Intent ê¸°ë°˜ ê²€ìƒ‰ (AutoRRF ì‚¬ìš©)"""
        results = await self.retrieval.search(
            query=query,
            intent=intent,
            top_k=top_k,
        )
        
        context = await self._build_context(results)
        
        return SearchResult(
            chunks=results,
            context=context,
            token_count=sum(c.token_count for c in results),
        )
    
    async def get_symbol_graph(
        self,
        symbol_id: str,
        max_depth: int = 2,
    ) -> CallGraph:
        """í˜¸ì¶œ ê·¸ë˜í”„ (Program Sliceìš©)"""
        return await self.graph.get_subgraph(
            node_id=symbol_id,
            max_depth=max_depth,
        )
```

**Day 4-7: Router â†’ Workflow ì—°ê²°**
```python
# src/agent/orchestrator/orchestrator.py

async def _route(self, user_request: str, context: Dict):
    """Layer 0 â†’ Layer 1"""
    
    # 1. Strategy ê²°ì •
    plan = self.unified_router.route(user_request, budget_ms=5000)
    
    # 2. Context ê²€ìƒ‰ (ì‹¤ì œ CodeGraph í˜¸ì¶œ)
    search_result = await self.context.search_relevant_code(
        query=user_request,
        intent=plan.intent,
        top_k=plan.adaptive_k,
    )
    
    # 3. Context ì—…ë°ì´íŠ¸
    context.update({
        "routing_plan": plan,
        "relevant_code": search_result.chunks,
        "token_budget_remaining": plan.token_budget - search_result.token_count,
    })
    
    return IntentResult(...)

async def _execute_workflow(self, intent_result, task_graph):
    """Layer 1 â†’ Layer 2"""
    
    workflow_state = WorkflowState(
        current_step=WorkflowStep.ANALYZE,
        iteration=0,
        context={
            **intent_result.context,
            "task_graph": task_graph,
            "context_adapter": self.context,  # ì£¼ì…!
            "code_generator": self._build_code_generator(),
            "code_validator": self._build_code_validator(),
        },
    )
    
    return await self.workflow.run(workflow_state)
```

**Day 8-10: Analyze Step ì‹¤ì œ êµ¬í˜„**
```python
# src/agent/workflow/state_machine.py

async def _analyze(self, state: WorkflowState) -> StepResult:
    """ì‹¤ì œ ì½”ë“œ ë¶„ì„ (Mock ì œê±°)"""
    
    context_adapter = state.context["context_adapter"]
    query = state.context.get("user_request", "")
    
    # ì‹¤ì œ ê²€ìƒ‰
    search_result = await context_adapter.search_relevant_code(
        query=query,
        intent=state.context.get("intent", "balanced"),
        top_k=10,
    )
    
    # Symbol ì •ë³´ ì¶”ì¶œ
    symbols = []
    for chunk in search_result.chunks[:5]:  # Top 5ë§Œ
        symbol_info = await context_adapter.get_symbol_info(chunk.symbol_id)
        symbols.append(symbol_info)
    
    analyzed_data = {
        "chunks": search_result.chunks,
        "symbols": symbols,
        "files": list(set(c.file_path for c in search_result.chunks)),
        "token_count": search_result.token_count,
    }
    
    # Token budget ì²´í¬
    if search_result.token_count > state.context.get("token_budget_remaining", 100000):
        return StepResult(
            step=WorkflowStep.ANALYZE,
            success=False,
            output=None,
            error="Token budget exceeded",
        )
    
    return StepResult(
        step=WorkflowStep.ANALYZE,
        success=True,
        output=analyzed_data,
        metadata={
            "total_chunks": len(search_result.chunks),
            "total_symbols": len(symbols),
            "token_used": search_result.token_count,
        }
    )
```

**Tests**:
```python
# tests/integration/test_layer0_layer1.py

async def test_router_uses_real_context():
    """Routerê°€ ì‹¤ì œ CodeGraph ì‚¬ìš©"""
    # Given
    repo_id = "test_repo"
    await index_repository(repo_id, "./fixtures/sample_repo")
    
    # When
    orchestrator = build_orchestrator(repo_id)
    result = await orchestrator.execute(
        user_request="Where is calculate_total defined?",
        context={"repo_id": repo_id},
    )
    
    # Then
    assert result.is_success()
    assert "calculate_total" in str(result.result)
    assert len(result.metadata["relevant_files"]) > 0

async def test_token_budget_enforcement():
    """Token budget ì´ˆê³¼ ì‹œ ì‹¤íŒ¨"""
    result = await orchestrator.execute(
        user_request="Explain entire codebase",
        context={"token_budget": 1000},  # ë§¤ìš° ì‘ì€ budget
    )
    
    assert result.status == ExecutionStatus.FAILED
    assert "Token budget exceeded" in result.error
```

#### **Week 3-4: MetaLayer êµ¬ì¶•** (60% â†’ 85%)

**Goal**: M0/M1/M2 ì‹¤ì œ ë™ì‘

**Day 11-13: M0 (TaskGraph) ë™ì  ìƒì„±**
```python
# src/agent/task_graph/dynamic_planner.py

class DynamicTaskGraphPlanner:
    """LLM ê¸°ë°˜ ë™ì  Task ë¶„í•´"""
    
    def __init__(self, llm, static_planner: TaskGraphPlanner):
        self.llm = llm
        self.static_planner = static_planner
    
    async def plan(
        self,
        user_intent: str,
        context: Dict[str, Any],
        analyzed_data: Optional[Dict] = None,
    ) -> TaskGraph:
        """ë™ì  Task ìƒì„±"""
        
        # Complexity íŒë‹¨
        complexity = self._estimate_complexity(user_intent, analyzed_data)
        
        if complexity == "simple":
            # Rule ê¸°ë°˜ (ë¹ ë¦„)
            return self.static_planner.plan(user_intent, context)
        
        # LLM ê¸°ë°˜ ë¶„í•´
        prompt = f"""# Task Decomposition

User Request: {user_intent}

Context:
- Files: {analyzed_data.get('files', [])}
- Symbols: {[s.name for s in analyzed_data.get('symbols', [])]}

Break down into tasks with dependencies.

Output JSON:
{{
  "tasks": [
    {{
      "id": "task_1",
      "type": "analyze_code",
      "description": "...",
      "depends_on": []
    }}
  ]
}}
"""
        
        response = await self.llm.complete(prompt, temperature=0.2)
        task_graph = self._parse_task_graph(response)
        
        return task_graph
```

**Day 14-17: M1 (Critic) LLM ë¦¬ë·°**
```python
# src/safety/critic/code_critic.py

class CodeCritic:
    """LLM ê¸°ë°˜ ì½”ë“œ ë¦¬ë·°"""
    
    def __init__(self, llm, guardrail: Guardrail):
        self.llm = llm
        self.guardrail = guardrail
    
    async def review(
        self,
        code_change: CodeChange,
        context: Dict[str, Any],
    ) -> CriticResult:
        """ì½”ë“œ ë¦¬ë·° (2-phase)"""
        
        # Phase 1: Guardrail (ë¹ ë¥¸ ê·œì¹™)
        guardrail_result = self.guardrail.check(code_change)
        if not guardrail_result.passed:
            return CriticResult(
                approved=False,
                issues=[
                    Issue(
                        severity="blocker",
                        rule_id=v.rule_id,
                        message=v.description,
                    )
                    for v in guardrail_result.violations
                ],
            )
        
        # Phase 2: LLM (ì˜ë¯¸ë¡ ì  ë¦¬ë·°)
        prompt = f"""# Code Review

## Changed Code
```python
{code_change.content}
```

## Context
- File: {code_change.file_path}
- Intent: {context.get('intent')}
- Original: {context.get('original_code', 'N/A')}

Review for:
1. Correctness
2. Security
3. Performance
4. Maintainability

Output JSON:
{{
  "approved": true/false,
  "issues": [
    {{"severity": "blocker", "message": "..."}}
  ],
  "suggestions": [...]
}}
"""
        
        response = await self.llm.complete(prompt, temperature=0.3)
        review = self._parse_review(response)
        
        return CriticResult(
            approved=review.approved and not review.has_blocker(),
            issues=review.issues,
            suggestions=review.suggestions,
        )
```

**Day 18-20: M2 (Guardrail) Rule Engine**
```python
# src/safety/guardrail/rule_engine.py

@dataclass
class GuardrailRule:
    """ë‹¨ì¼ ê·œì¹™"""
    id: str
    name: str
    description: str
    severity: str  # "blocker" | "warning" | "info"
    check: Callable[[CodeChange, Dict], bool]

class GuardrailEngine:
    """ê·œì¹™ ì—”ì§„ (YAML ê¸°ë°˜)"""
    
    def __init__(self, rules_path: str = "config/guardrail_rules.yaml"):
        self.rules: List[GuardrailRule] = []
        self._load_rules(rules_path)
    
    def _load_rules(self, path: str):
        """YAML â†’ Rule ê°ì²´"""
        config = yaml.safe_load(Path(path).read_text())
        
        for rule_config in config["rules"]:
            self.rules.append(
                self._build_rule(rule_config)
            )
    
    def _build_rule(self, config: Dict) -> GuardrailRule:
        """Rule ìƒì„±"""
        
        if config["name"] == "LOC_LIMIT":
            return GuardrailRule(
                id=config["id"],
                name=config["name"],
                description=config["description"],
                severity=config["severity"],
                check=lambda change, ctx: (
                    change.lines_added < config["params"]["max_lines"]
                ),
            )
        
        elif config["name"] == "NO_SECRET":
            patterns = [re.compile(p) for p in config["patterns"]]
            return GuardrailRule(
                id=config["id"],
                name=config["name"],
                description=config["description"],
                severity=config["severity"],
                check=lambda change, ctx: not any(
                    pattern.search(change.content)
                    for pattern in patterns
                ),
            )
        
        # ... ë‹¤ë¥¸ ê·œì¹™ë“¤
    
    def check(
        self,
        code_change: CodeChange,
        context: Optional[Dict] = None
    ) -> GuardrailResult:
        """ëª¨ë“  ê·œì¹™ ì²´í¬"""
        violations = []
        context = context or {}
        
        for rule in self.rules:
            try:
                if not rule.check(code_change, context):
                    violations.append(Violation(
                        rule_id=rule.id,
                        rule_name=rule.name,
                        description=rule.description,
                        severity=rule.severity,
                    ))
            except Exception as e:
                logger.error(f"Rule {rule.id} failed: {e}")
        
        has_blocker = any(v.severity == "blocker" for v in violations)
        
        return GuardrailResult(
            passed=not has_blocker,
            violations=violations,
        )
```

**YAML ì„¤ì •**:
```yaml
# config/guardrail_rules.yaml

rules:
  - id: G001
    name: LOC_LIMIT
    description: Single change < 500 lines
    severity: blocker
    params:
      max_lines: 500
  
  - id: G002
    name: NO_SECRET
    description: No API keys or passwords
    severity: blocker
    patterns:
      - "(?i)api[_-]?key"
      - "(?i)password"
      - "(?i)secret"
      - "sk-[a-zA-Z0-9]{32,}"  # OpenAI key
  
  - id: G003
    name: FILE_LIMIT
    description: Max 10 files per change
    severity: warning
    params:
      max_files: 10
  
  - id: G004
    name: NO_HARDCODED_URL
    description: No production URLs in code
    severity: warning
    patterns:
      - "https://api\\.production\\.com"
      - "https://[^/]*\\.prod\\."

# Org-level overrides
org_overrides:
  org_enterprise:
    G001:
      max_lines: 1000
    G003:
      max_files: 20
```

**Tests**:
```python
# tests/unit/test_guardrail.py

def test_loc_limit():
    engine = GuardrailEngine("config/guardrail_rules.yaml")
    
    # Under limit
    change = CodeChange(
        file_path="test.py",
        content="x = 1\n" * 100,
        lines_added=100,
    )
    result = engine.check(change)
    assert result.passed
    
    # Over limit
    change = CodeChange(
        file_path="test.py",
        content="x = 1\n" * 600,
        lines_added=600,
    )
    result = engine.check(change)
    assert not result.passed
    assert "G001" in [v.rule_id for v in result.violations]

def test_secret_detection():
    engine = GuardrailEngine()
    
    # With secret
    change = CodeChange(
        file_path="test.py",
        content='api_key = "sk-1234567890abcdef1234567890abcdef"',
        lines_added=1,
    )
    result = engine.check(change)
    assert not result.passed
    assert "G002" in [v.rule_id for v in result.violations]
```

---

## ADR-002: Router vs TaskGraph Boundary

### ğŸ“Œ í˜„ì¬ ìƒíƒœ: 90%

**ì™„ë£Œ**:
- âœ… UnifiedRouter êµ¬í˜„ (Rule ê¸°ë°˜)
- âœ… TaskGraphPlanner êµ¬í˜„ (Rule ê¸°ë°˜)
- âœ… ì±…ì„ ë¶„ë¦¬ (Router=ì‹¤í–‰ê²°ì •, TaskGraph=ê³„íš)

**ë¯¸ì™„ë£Œ**:
- âŒ ê²½ê³„ ë¬¸ì„œí™”
- âŒ Edge case ì²˜ë¦¬ (ì¶©ëŒ í•´ê²°)

### ğŸ¯ êµ¬í˜„ ê³„íš

#### **Week 7: ë¬¸ì„œí™” + Edge Cases** (90% â†’ 100%)

**Day 46-48: ê²½ê³„ ë¬¸ì„œí™”**

```markdown
# docs/adr/ADR-002-BOUNDARY.md

## Router ì±…ì„

1. **Intent ë¶„ë¥˜**
   - User request â†’ Intent (symbol/flow/concept/code)
   - Rule ê¸°ë°˜ (5ms ë¯¸ë§Œ)

2. **Strategy ì„ íƒ**
   - Intent + Budget â†’ Strategy path (symbol/vector/lexical/graph)
   - Top-K ê²°ì • (adaptive)

3. **Advanced Features ê²°ì •**
   - HyDE, Multi-Query, Cross-Encoder í™œì„±í™” ì—¬ë¶€
   - Budget ê¸°ë°˜ (< 500ms: symbolë§Œ, >= 3s: ëª¨ë“  ì „ëµ)

## TaskGraph ì±…ì„

1. **Task ë¶„í•´**
   - Intent + Context â†’ Task ëª©ë¡
   - Dependency ë¶„ì„

2. **ì‹¤í–‰ ìˆœì„œ**
   - Topological sort
   - Parallel groups ê³„ì‚°

3. **ì¬ê³„íš (Phase 1)**
   - ì‹¤í–‰ ì¤‘ ì‹¤íŒ¨ ì‹œ Task ì¬êµ¬ì„±
   - Dynamic replanning

## ê²½ê³„ ê·œì¹™

### Rule 1: RouterëŠ” "What to do", TaskGraphëŠ” "How to do"

```
Router:     "This is a symbol lookup" â†’ strategy=[symbol, lexical]
TaskGraph:  "Search symbols â†’ Validate â†’ Format" â†’ 3 tasks
```

### Rule 2: RouterëŠ” í•œ ë²ˆ, TaskGraphëŠ” ì—¬ëŸ¬ ë²ˆ

```
Router: 1íšŒ ì‹¤í–‰ (user request ë°›ì„ ë•Œ)
TaskGraph: ë§¤ iteration ì¬ê³„íš ê°€ëŠ¥ (Dynamic replanning)
```

### Rule 3: ì¶©ëŒ í•´ê²° ìš°ì„ ìˆœìœ„

```
Routerì˜ Budget/Constraint > TaskGraphì˜ Task ëª©ë¡

Example:
- Router: budget_ms=500, strategy=[symbol]
- TaskGraph: 3 tasks (analyze, generate, test)
- ì¶©ëŒ: testëŠ” 1ì´ˆ ì†Œìš” â†’ Budget ì´ˆê³¼
- í•´ê²°: test ìŠ¤í‚µ or ë¹ ë¥¸ ëª¨ë“œ
```

## Edge Cases

### Case 1: RouterëŠ” "simple", í•˜ì§€ë§Œ TaskGraphëŠ” "complex"

```python
# Router
plan = RoutingPlan(
    intent="symbol",
    complexity="simple",  # ë‹¨ìˆœí•´ ë³´ì„
    budget_ms=1000,
)

# TaskGraph (ì‹¤ì œë¡œëŠ” complex)
task_graph = planner.plan("symbol", context)
estimated_time = planner.estimate_execution_time(task_graph)

if estimated_time > plan.budget_ms / 1000:
    # Budget ì´ˆê³¼ â†’ Task ì¶•ì†Œ
    task_graph = planner.simplify(task_graph, max_time=1.0)
```

### Case 2: TaskGraphê°€ Router Strategy ë¬´ì‹œ

```python
# Router: strategy=[symbol]ë§Œ ì‚¬ìš©
# TaskGraph: vector ê²€ìƒ‰ë„ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨

# í•´ê²°: Router Strategyë¥¼ ìš°ì„ 
if task.strategy not in plan.strategy_path:
    logger.warning(f"Task {task.id} uses {task.strategy} not in Router plan")
    # Option 1: Task ìŠ¤í‚µ
    # Option 2: Router Strategyë¡œ ëŒ€ì²´
    task.strategy = plan.strategy_path[0]
```
```

**Day 49-50: Edge Case Tests**
```python
# tests/integration/test_router_taskgraph_boundary.py

async def test_budget_enforcement():
    """TaskGraphê°€ Router budget ì¤€ìˆ˜"""
    
    # Router: 500ms budget
    orchestrator = build_orchestrator()
    orchestrator.unified_router.budget_ms = 500
    
    # TaskGraph: 3ì´ˆ ì˜ˆìƒë˜ëŠ” task
    result = await orchestrator.execute("Complex refactoring")
    
    # Then: Budget ì´ˆê³¼ â†’ Task ì¶•ì†Œ or ì‹¤íŒ¨
    assert result.execution_time_ms <= 600  # 20% ì—¬ìœ 

async def test_strategy_mismatch():
    """TaskGraphê°€ Router strategy ì¤€ìˆ˜"""
    
    # Router: symbolë§Œ ì‚¬ìš©
    orchestrator.unified_router.force_strategy = ["symbol"]
    
    # Execute
    result = await orchestrator.execute("Find calculate_total")
    
    # Then: vector ê²€ìƒ‰ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    assert "vector" not in result.metadata["strategies_used"]
```

---

## ADR-003: Graph Workflow Engine

### ğŸ“Œ í˜„ì¬ ìƒíƒœ: 40%

**ì™„ë£Œ**:
- âœ… StateMachine êµ¬ì¡°
- âœ… 6-step workflow (Analyze â†’ Self-heal)
- âœ… Early exit ì¡°ê±´

**ë¯¸ì™„ë£Œ**:
- âŒ StepsëŠ” Mock
- âŒ Dynamic replanning
- âŒ Static Analysis First

### ğŸ¯ êµ¬í˜„ ê³„íš

#### **Week 5: Workflow ì‹¤ì œ êµ¬í˜„** (40% â†’ 85%)

**Day 29-31: Generate Step**
```python
# src/agent/workflow/state_machine.py

async def _generate(self, state: WorkflowState) -> StepResult:
    """ì‹¤ì œ ì½”ë“œ ìƒì„±"""
    
    # Analyze ê²°ê³¼
    analyzed_data = state.context["analyzed_data"]
    intent = state.context.get("intent", "fix_bug")
    
    # CodeGenerator
    generator = state.context.get("code_generator")
    if not generator:
        generator = CodeGenerator(llm=get_llm())
    
    # Intentë³„ ë¶„ê¸°
    if intent == "fix_bug":
        code_change = await generator.generate_fix(
            bug_description=state.context["user_request"],
            file_path=analyzed_data["files"][0],
            existing_code=analyzed_data["chunks"][0].content,
            context={
                "symbols": analyzed_data["symbols"],
                "related_files": analyzed_data["files"],
            },
        )
    
    elif intent == "add_feature":
        code_changes = await generator.generate_feature(
            feature_description=state.context["user_request"],
            target_file=analyzed_data["files"][0],
            context=analyzed_data,
        )
        code_change = code_changes[0]
    
    elif intent == "refactor_code":
        code_change = await generator.generate_refactoring(
            refactor_goal=state.context["user_request"],
            file_path=analyzed_data["files"][0],
            existing_code=analyzed_data["chunks"][0].content,
            context=analyzed_data,
        )
    
    else:
        # Generic
        code_change = CodeChange(
            file_path="output.py",
            content="# Generated code\npass",
            explanation="Generic code generation",
        )
    
    return StepResult(
        step=WorkflowStep.GENERATE,
        success=True,
        output=code_change,
        metadata={
            "lines_changed": code_change.lines_added,
            "confidence": code_change.confidence,
        }
    )
```

**Day 32-34: Critic Step (M1 ì—°ë™)**
```python
async def _critic(self, state: WorkflowState) -> StepResult:
    """ì‹¤ì œ Critic (M1 MetaLayer)"""
    
    # ìƒì„±ëœ ì½”ë“œ
    code_change = state.result
    if not code_change:
        return StepResult(
            step=WorkflowStep.CRITIC,
            success=False,
            output=None,
            error="No code to review",
        )
    
    # CodeCritic ì‹¤í–‰
    critic = state.context.get("code_critic")
    if not critic:
        critic = CodeCritic(
            llm=get_llm(),
            guardrail=GuardrailEngine(),
        )
    
    review = await critic.review(code_change, state.context)
    
    # ê²°ê³¼ ì²˜ë¦¬
    if not review.approved:
        # Critic í”¼ë“œë°± ì €ì¥ â†’ ì¬ìƒì„±
        state.context["critic_feedback"] = review.issues
        state.context["should_regenerate"] = True
        
        # Blockerë©´ ì‹¤íŒ¨
        if review.has_blocker():
            return StepResult(
                step=WorkflowStep.CRITIC,
                success=False,
                output=review,
                error=f"Blocked: {review.blocker_issues()}",
            )
    
    return StepResult(
        step=WorkflowStep.CRITIC,
        success=True,
        output=review,
        metadata={
            "approved": review.approved,
            "issue_count": len(review.issues),
        }
    )
```

**Day 35-37: Dynamic Replanning**
```python
# src/agent/workflow/state_machine.py

def run(self, initial_state: WorkflowState) -> WorkflowState:
    """Workflow ì‹¤í–‰ (Dynamic Replanning ì¶”ê°€)"""
    state = initial_state
    
    while state.iteration < self.max_iterations:
        # ê° ë‹¨ê³„ ì‹¤í–‰
        for step in self.steps:
            state.current_step = step
            
            # Step ì‹¤í–‰
            step_result = await self._execute_step(state, step)
            state.add_step_result(step_result)
            
            # ì‹¤íŒ¨ ì²˜ë¦¬
            if not step_result.success:
                # Error ìœ í˜• íŒë‹¨
                error_type = self._classify_error(step_result, state)
                
                if error_type == "code_error":
                    # Code Error â†’ Self-heal ì‹œë„
                    state.current_step = WorkflowStep.SELF_HEAL
                    continue
                
                elif error_type == "plan_error":
                    # Plan Error â†’ TaskGraph ì¬ìƒì„±
                    state.context["replan_reason"] = step_result.error
                    new_task_graph = await self._replan(state)
                    state.context["task_graph"] = new_task_graph
                    state.iteration = 0  # ì¬ì‹œì‘
                    break
                
                else:
                    # ë³µêµ¬ ë¶ˆê°€ â†’ ì‹¤íŒ¨
                    state.error = step_result.error
                    state.current_step = WorkflowStep.FAILED
                    state.exit_reason = WorkflowExitReason.ERROR
                    return state
            
            # ê²°ê³¼ ë°˜ì˜
            if step_result.output:
                self._update_state_from_result(state, step, step_result)
            
            # Critic í”¼ë“œë°± â†’ ì¬ìƒì„±
            if state.context.get("should_regenerate"):
                state.current_step = WorkflowStep.GENERATE
                state.context["should_regenerate"] = False
                break  # í˜„ì¬ iteration ì¤‘ë‹¨, ì¬ì‹œì‘
        
        # Iteration ì¦ê°€
        state.iteration += 1
        
        # Early exit
        if self._should_exit_early(state):
            break
    
    # ì™„ë£Œ
    state.current_step = WorkflowStep.COMPLETED
    state.exit_reason = WorkflowExitReason.SUCCESS
    return state

def _classify_error(
    self,
    step_result: StepResult,
    state: WorkflowState
) -> str:
    """Error ìœ í˜• ë¶„ë¥˜"""
    
    error = step_result.error or ""
    
    # Code Error: Syntax, Import, Lint
    if any(x in error.lower() for x in ["syntax", "import", "lint"]):
        return "code_error"
    
    # Plan Error: Budget ì´ˆê³¼, Task ì‹¤íŒ¨
    if any(x in error.lower() for x in ["budget", "timeout", "task"]):
        return "plan_error"
    
    # Unknown
    return "unknown"

async def _replan(self, state: WorkflowState) -> TaskGraph:
    """TaskGraph ì¬ìƒì„±"""
    
    planner = state.context.get("task_planner")
    reason = state.context.get("replan_reason", "Unknown")
    
    logger.info(f"Replanning due to: {reason}")
    
    # LLM ê¸°ë°˜ ë™ì  ê³„íš
    new_graph = await planner.plan(
        user_intent=state.context.get("intent", "unknown"),
        context={
            **state.context,
            "replan_reason": reason,
            "previous_failures": [
                r.error for r in state.step_history if not r.success
            ],
        },
        analyzed_data=state.context.get("analyzed_data"),
    )
    
    return new_graph
```

**Day 38-40: Static Analysis First**
```python
async def _analyze(self, state: WorkflowState) -> StepResult:
    """Static Analysis First"""
    
    context_adapter = state.context["context_adapter"]
    query = state.context.get("user_request", "")
    intent = state.context.get("intent", "balanced")
    
    # Phase 1: Static Analysis (ë¹ ë¦„, ì •í™•)
    static_result = await self._static_analysis(query, context_adapter)
    
    if static_result.confidence > 0.9 and len(static_result.symbols) > 0:
        # Staticìœ¼ë¡œ ì¶©ë¶„
        logger.info("Static analysis sufficient")
        return StepResult(
            step=WorkflowStep.ANALYZE,
            success=True,
            output={
                "chunks": static_result.chunks,
                "symbols": static_result.symbols,
                "files": static_result.files,
                "analysis_type": "static",
            },
        )
    
    # Phase 2: Semantic Search (ëŠë¦¼, ìœ ì—°)
    logger.info("Falling back to semantic search")
    semantic_result = await context_adapter.search_relevant_code(
        query=query,
        intent=intent,
        top_k=10,
    )
    
    return StepResult(
        step=WorkflowStep.ANALYZE,
        success=True,
        output={
            "chunks": semantic_result.chunks,
            "symbols": await self._extract_symbols(semantic_result),
            "files": list(set(c.file_path for c in semantic_result.chunks)),
            "analysis_type": "semantic",
        },
    )

async def _static_analysis(
    self,
    query: str,
    context_adapter: ContextAdapter
) -> StaticAnalysisResult:
    """Static analysis (Symbol name ê¸°ë°˜)"""
    
    # Queryì—ì„œ Symbol name ì¶”ì¶œ
    # Example: "Where is calculate_total defined?"
    #          â†’ symbol_name = "calculate_total"
    
    symbol_names = self._extract_symbol_names(query)
    
    if not symbol_names:
        return StaticAnalysisResult(
            confidence=0.0,
            symbols=[],
            chunks=[],
        )
    
    # Symbol Graphì—ì„œ ì§ì ‘ ê²€ìƒ‰
    symbols = []
    for name in symbol_names:
        results = await context_adapter.find_symbols_by_name(name)
        symbols.extend(results)
    
    if not symbols:
        return StaticAnalysisResult(confidence=0.0, symbols=[])
    
    # Chunks ê°€ì ¸ì˜¤ê¸°
    chunks = []
    for symbol in symbols:
        chunk = await context_adapter.get_chunk_for_symbol(symbol.id)
        chunks.append(chunk)
    
    return StaticAnalysisResult(
        confidence=0.95,  # Staticì€ ì •í™•
        symbols=symbols,
        chunks=chunks,
        files=list(set(s.file_path for s in symbols)),
    )
```

---

## ADR-004: Sandbox Executor

### ğŸ“Œ í˜„ì¬ ìƒíƒœ: 20%

**ì™„ë£Œ**:
- âœ… ShadowFS (In-memory overlay)
- âœ… commit/rollback

**ë¯¸ì™„ë£Œ**:
- âŒ Sandbox (Docker/containerd)
- âŒ Resource limits
- âŒ Network isolation

### ğŸ¯ êµ¬í˜„ ê³„íš

#### **Week 6: Sandbox êµ¬ì¶•** (20% â†’ 90%)

**Day 41-43: Docker Sandbox**
```python
# src/execution/sandbox/docker_sandbox.py

import docker
from pathlib import Path
from typing import Optional

class DockerSandbox:
    """Docker ê¸°ë°˜ ê²©ë¦¬ ì‹¤í–‰"""
    
    def __init__(
        self,
        image: str = "python:3.11-slim",
        timeout: int = 30,
        memory_limit: str = "512m",
        cpu_limit: float = 1.0,
        network_mode: str = "none",
    ):
        self.image = image
        self.timeout = timeout
        self.memory_limit = memory_limit
        self.cpu_limit = cpu_limit
        self.network_mode = network_mode
        
        self.client = docker.from_env()
        
        # Image pull (ì—†ìœ¼ë©´)
        try:
            self.client.images.get(self.image)
        except docker.errors.ImageNotFound:
            logger.info(f"Pulling image {self.image}...")
            self.client.images.pull(self.image)
    
    async def execute_tests(
        self,
        workspace: Path,
        test_command: str = "pytest -v",
        env: Optional[Dict[str, str]] = None,
    ) -> TestResult:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ê²©ë¦¬ í™˜ê²½)"""
        
        logger.info(f"Running tests in sandbox: {test_command}")
        
        try:
            # Container ì‹¤í–‰
            container = self.client.containers.run(
                image=self.image,
                command=f"bash -c 'cd /workspace && {test_command}'",
                volumes={
                    str(workspace.absolute()): {
                        "bind": "/workspace",
                        "mode": "ro",  # Read-only!
                    }
                },
                environment=env or {},
                mem_limit=self.memory_limit,
                cpu_period=100000,
                cpu_quota=int(self.cpu_limit * 100000),
                network_mode=self.network_mode,  # No network
                remove=True,  # Auto cleanup
                detach=False,
                timeout=self.timeout,
            )
            
            # stdout íŒŒì‹±
            stdout = container.decode("utf-8")
            test_result = self._parse_pytest_output(stdout)
            
            logger.info(f"Tests completed: {test_result}")
            return test_result
            
        except docker.errors.ContainerError as e:
            # Container ì‹¤í–‰ ì‹¤íŒ¨
            logger.error(f"Container error: {e}")
            return TestResult(
                passed=False,
                total=0,
                failed=0,
                error=f"Container error: {e.stderr.decode('utf-8')}",
            )
        
        except docker.errors.APIError as e:
            # Docker API ì˜¤ë¥˜
            logger.error(f"Docker API error: {e}")
            return TestResult(
                passed=False,
                total=0,
                failed=0,
                error=f"Docker error: {e}",
            )
        
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            return TestResult(
                passed=False,
                total=0,
                failed=0,
                error=str(e),
            )
    
    def _parse_pytest_output(self, stdout: str) -> TestResult:
        """pytest ì¶œë ¥ íŒŒì‹±"""
        
        # Example: "3 passed, 1 failed in 0.23s"
        passed = 0
        failed = 0
        
        if "passed" in stdout:
            match = re.search(r"(\d+) passed", stdout)
            if match:
                passed = int(match.group(1))
        
        if "failed" in stdout:
            match = re.search(r"(\d+) failed", stdout)
            if match:
                failed = int(match.group(1))
        
        return TestResult(
            passed=failed == 0,
            total=passed + failed,
            failed=failed,
            output=stdout,
        )
    
    async def execute_lint(
        self,
        workspace: Path,
        lint_command: str = "ruff check .",
    ) -> LintResult:
        """Lint ì‹¤í–‰"""
        # ë™ì¼í•œ íŒ¨í„´
        pass
    
    def cleanup(self):
        """ì •ë¦¬"""
        self.client.close()
```

**Day 44-45: Workflow í†µí•©**
```python
# src/agent/workflow/state_machine.py

async def _test(self, state: WorkflowState) -> StepResult:
    """ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (Sandbox)"""
    
    # ShadowFS â†’ ì„ì‹œ workspace
    shadow_fs = state.context["shadow_fs"]
    session_id = state.context["session_id"]
    workspace = Path("/tmp/agent_workspace") / session_id
    
    # ì‹¤ì œ íŒŒì¼ ìƒì„±
    workspace.mkdir(parents=True, exist_ok=True)
    for file_path, content in shadow_fs.overlay.items():
        full_path = workspace / file_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        full_path.write_text(content)
    
    # Sandbox ì‹¤í–‰
    sandbox = DockerSandbox(
        timeout=30,
        memory_limit="512m",
    )
    
    try:
        test_result = await sandbox.execute_tests(
            workspace=workspace,
            test_command="pytest -v --tb=short",
        )
        
        return StepResult(
            step=WorkflowStep.TEST,
            success=test_result.passed,
            output=test_result,
            error=test_result.error if not test_result.passed else None,
            metadata={
                "total": test_result.total,
                "failed": test_result.failed,
            }
        )
    
    finally:
        # ì •ë¦¬
        shutil.rmtree(workspace, ignore_errors=True)
        sandbox.cleanup()
```

**Tests**:
```python
# tests/integration/test_sandbox.py

async def test_sandbox_isolation():
    """Network ê²©ë¦¬"""
    
    # Network ì ‘ê·¼ ì‹œë„
    workspace = Path("./fixtures/network_test")
    sandbox = DockerSandbox(network_mode="none")
    
    result = await sandbox.execute_tests(
        workspace=workspace,
        test_command="python -c 'import urllib.request; urllib.request.urlopen(\"http://google.com\")'",
    )
    
    # Network ì°¨ë‹¨ â†’ ì‹¤íŒ¨
    assert not result.passed
    assert "Network" in result.error or "connection" in result.error.lower()

async def test_resource_limits():
    """Memory limit"""
    
    # 1GB í• ë‹¹ ì‹œë„
    workspace = Path("./fixtures/memory_test")
    sandbox = DockerSandbox(memory_limit="256m")
    
    result = await sandbox.execute_tests(
        workspace=workspace,
        test_command="python -c 'x = [0] * (1024 ** 3)'",  # 1GB
    )
    
    # Memory limit â†’ ì‹¤íŒ¨
    assert not result.passed

async def test_timeout():
    """Timeout"""
    
    workspace = Path("./fixtures/timeout_test")
    sandbox = DockerSandbox(timeout=5)
    
    result = await sandbox.execute_tests(
        workspace=workspace,
        test_command="python -c 'import time; time.sleep(10)'",  # 10ì´ˆ
    )
    
    # 5ì´ˆ timeout â†’ ì‹¤íŒ¨
    assert not result.passed
    assert "timeout" in result.error.lower()
```

---

## Week 8: í†µí•© í…ŒìŠ¤íŠ¸ + ì„±ëŠ¥ ìµœì í™” (95% â†’ 100%)

### Day 51-53: E2E í†µí•© í…ŒìŠ¤íŠ¸

```python
# tests/e2e/test_full_pipeline.py

async def test_bug_fix_e2e():
    """ë²„ê·¸ ìˆ˜ì • ì „ì²´ íŒŒì´í”„ë¼ì¸"""
    
    # Given: ì‹¤ì œ buggy repo
    repo_path = "./fixtures/buggy_calculator"
    await index_repository("bug_repo", repo_path)
    
    orchestrator = build_production_orchestrator("bug_repo")
    
    # When
    result = await orchestrator.execute(
        user_request="Fix the division by zero bug in calculate()",
        context={
            "repo_id": "bug_repo",
            "enable_full_workflow": True,
        },
    )
    
    # Then
    assert result.is_success()
    assert result.status == ExecutionStatus.COMPLETED
    
    # Layer 0: Context ê²€ìƒ‰
    assert len(result.metadata["relevant_files"]) > 0
    
    # Layer 1: Router
    assert result.metadata["intent"] in ["fix_bug", "code"]
    
    # Layer 2: Workflow
    assert "task_analyze_bug" in result.tasks_completed
    assert "task_generate_fix" in result.tasks_completed
    
    # MetaLayer: Critic
    assert result.metadata["critic_approved"] == True
    
    # Layer 3: Sandbox
    assert result.metadata["test_result"]["passed"] == True

async def test_feature_add_e2e():
    """ê¸°ëŠ¥ ì¶”ê°€"""
    
    result = await orchestrator.execute(
        user_request="Add a subtract() function to calculator",
    )
    
    assert result.is_success()
    assert "subtract" in result.result.content

async def test_guardrail_blocks():
    """Guardrail ì°¨ë‹¨"""
    
    result = await orchestrator.execute(
        user_request="Add 1000 lines of logging to every file",
    )
    
    # Guardrail ì°¨ë‹¨
    assert result.status == ExecutionStatus.FAILED
    assert "G001" in result.error  # LOC_LIMIT
```

### Day 54-55: ì„±ëŠ¥ ìµœì í™”

**Benchmark**:
```python
# tests/performance/test_latency.py

async def test_simple_query_latency():
    """Simple query < 3ì´ˆ"""
    
    start = time.time()
    result = await orchestrator.execute("Find calculate_total")
    latency = time.time() - start
    
    assert latency < 3.0
    assert result.is_success()

async def test_complex_query_latency():
    """Complex query < 10ì´ˆ"""
    
    start = time.time()
    result = await orchestrator.execute(
        "Refactor the entire authentication module",
        context={"enable_full_workflow": True},
    )
    latency = time.time() - start
    
    assert latency < 10.0

async def test_token_efficiency():
    """Token < 10K"""
    
    result = await orchestrator.execute("Explain how user login works")
    
    assert result.tokens_used < 10000
    assert len(result.metadata["relevant_files"]) <= 5
```

**ìµœì í™”**:
1. Context ë³‘ë ¬ ê²€ìƒ‰ (3ê°œ strategy ë³‘ë ¬)
2. LLM í˜¸ì¶œ ìºì‹± (identical prompts)
3. Chunk deduplication

---

## ğŸ“Š ì„±ê³µ ì§€í‘œ

| Metric | Target | í˜„ì¬ | Week 8 ëª©í‘œ |
|--------|--------|------|-------------|
| **ADR-001 ì§„í–‰ë¥ ** | 100% | 30% | 100% |
| **ADR-002 ì§„í–‰ë¥ ** | 100% | 90% | 100% |
| **ADR-003 ì§„í–‰ë¥ ** | 100% | 40% | 100% |
| **ADR-004 ì§„í–‰ë¥ ** | 100% | 20% | 100% |
| **E2E í…ŒìŠ¤íŠ¸** | 10+ | 0 | 15+ |
| **Simple Query Latency** | < 3ì´ˆ | N/A | 2.5ì´ˆ |
| **Complex Query Latency** | < 10ì´ˆ | N/A | 8ì´ˆ |
| **Token íš¨ìœ¨** | < 10K | N/A | 8K |
| **Guardrail ì •í™•ë„** | 95%+ | N/A | 97% |
| **Test ì„±ê³µë¥ ** | 90%+ | N/A | 95% |

---

## ğŸ“ ìµœì¢… íŒŒì¼ êµ¬ì¡°

```
src/
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py          # âœ… Layer í†µí•©
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”œâ”€â”€ router/
â”‚   â”‚   â”œâ”€â”€ unified_router.py        # âœ… ì‹¤ì œ Context ì‚¬ìš©
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”œâ”€â”€ workflow/
â”‚   â”‚   â”œâ”€â”€ state_machine.py         # âœ… Steps ì‹¤ì œ êµ¬í˜„
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”œâ”€â”€ task_graph/
â”‚   â”‚   â”œâ”€â”€ planner.py               # Rule ê¸°ë°˜
â”‚   â”‚   â””â”€â”€ dynamic_planner.py       # âœ… LLM ê¸°ë°˜
â”‚   â””â”€â”€ adapters/
â”‚       â””â”€â”€ context_adapter.py       # âœ… Layer 0 Facade
â”‚
â”œâ”€â”€ execution/
â”‚   â”œâ”€â”€ sandbox/
â”‚   â”‚   â”œâ”€â”€ docker_sandbox.py        # âœ… Docker ê²©ë¦¬
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”œâ”€â”€ shadowfs/
â”‚   â”‚   â””â”€â”€ core.py                  # âœ… ê¸°ì¡´
â”‚   â”œâ”€â”€ code_generation/
â”‚   â”‚   â””â”€â”€ generator.py             # âœ… ê¸°ì¡´
â”‚   â””â”€â”€ validation/
â”‚       â””â”€â”€ validator.py             # âœ… ê¸°ì¡´
â”‚
â”œâ”€â”€ safety/
â”‚   â”œâ”€â”€ critic/
â”‚   â”‚   â””â”€â”€ code_critic.py           # âœ… M1 LLM ë¦¬ë·°
â”‚   â””â”€â”€ guardrail/
â”‚       â”œâ”€â”€ rule_engine.py           # âœ… M2 Rule ì—”ì§„
â”‚       â””â”€â”€ rules.yaml               # âœ… YAML ì„¤ì •
â”‚
â””â”€â”€ contexts/                        # âœ… ê¸°ì¡´ (Layer 0)
    â”œâ”€â”€ retrieval_search/
    â”œâ”€â”€ code_foundation/
    â””â”€â”€ ...

config/
â””â”€â”€ guardrail_rules.yaml             # âœ… Guardrail ì„¤ì •

tests/
â”œâ”€â”€ unit/                            # âœ… 100+ tests
â”œâ”€â”€ integration/                     # âœ… 50+ tests
â”œâ”€â”€ e2e/                             # âœ… 15+ tests
â””â”€â”€ performance/                     # âœ… 10+ benchmarks
```

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ (After Week 8)

Core 4ê°œ ADR ì™„ë£Œ í›„:

1. **ADR-005 (Context Manager)** - Token budget ìµœì í™”
2. **ADR-020 (Tool Taxonomy)** - Tools Layer êµ¬ì¶•
3. **ADR-011 (Guardrail)** - Org-level overrides
4. **ADR-021 (LLM Routing)** - ëª¨ë¸ë³„ routing

**Milestone**: Core Architecture 100% â†’ Agent MVP ì¶œì‹œ (3ê°œì›” ì°¨)

