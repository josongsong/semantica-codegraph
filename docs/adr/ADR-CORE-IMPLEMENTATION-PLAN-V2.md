# Core Architecture ADR êµ¬í˜„ ê³„íš V2 (í”¼ë“œë°± ë°˜ì˜)

**ì‘ì„±ì¼**: 2025-12-05 (Updated)  
**ëŒ€ìƒ**: Core Architecture 4ê°œ ADR (P0)  
**ê¸°ê°„**: 8ì£¼ (2ê°œì›”)  
**ì˜¤í”ˆì†ŒìŠ¤ í™œìš©**: LangGraph, E2B, LiteLLM, Pydantic, Guardrails AI, Playwright, GitPython

---

## ğŸ”¥ Critical Improvements (í”¼ë“œë°± ë°˜ì˜)

### 1. **ADR-004 Sandbox: Session-based Container** âœ…
- `containers.run` ë§¤ë²ˆ í˜¸ì¶œ â†’ `exec_run` ì¬ì‚¬ìš©
- ì„¸ì…˜ë‹¹ 1ê°œ ì»¨í…Œì´ë„ˆ (Keep-alive)
- ì†ë„: 2ì´ˆ/í…ŒìŠ¤íŠ¸ â†’ 0.1ì´ˆ/í…ŒìŠ¤íŠ¸ (20x ê°œì„ )

### 2. **ADR-003 Workflow: State Persistence** âœ…
- LangGraph Checkpoint í™œìš©
- Long-running task ì¬ì‹œì‘ ì§€ì›
- SQLite/Redis ë°±ì—…

### 3. **ADR-001 ContextAdapter: Semantic Cache** âœ…
- LRU Cache + Embedding similarity
- ì¤‘ë³µ ê²€ìƒ‰ ì œê±° (50% ì†ë„ í–¥ìƒ)

### 4. **ADR-002 Router: Budget Enforcement** âœ…
- `asyncio.wait_for(timeout=budget)` ê°•ì œ
- PartialResult fallback

### 5. **ì½”ë“œ ë ˆë²¨ ê°œì„ ** âœ…
- Regex pre-compilation (GuardrailEngine)
- JSON parsing with `json_repair` (DynamicPlanner)
- Volume mount strategy (rw + /tmp)

---

## Week-by-Week ê°œì„  ê³„íš

### **Week 1-2: Layer í†µí•© + Caching** (30% â†’ 60%)

#### Day 1-2: ContextAdapter with Semantic Cache

```python
# src/agent/adapters/context_adapter.py

from typing import Dict, List, Optional
from functools import lru_cache
import hashlib
from src.infra.cache.redis import RedisClient  # ê¸°ì¡´ ì¸í”„ë¼

class ContextAdapter:
    """Layer 0 (CodeGraph) Facade with Caching"""
    
    def __init__(
        self,
        retrieval_service: RetrievalService,
        chunk_store: ChunkStore,
        graph_store: GraphStore,
        cache: Optional[RedisClient] = None,
    ):
        self.retrieval = retrieval_service
        self.chunks = chunk_store
        self.graph = graph_store
        
        # Semantic Cache
        self.cache = cache or RedisClient()
        self.cache_ttl = 3600  # 1 hour
        
        # LRU Cache (ë©”ëª¨ë¦¬)
        self._symbol_cache: Dict[str, SymbolInfo] = {}
        self._max_cache_size = 1000
    
    async def search_relevant_code(
        self,
        query: str,
        intent: str,
        top_k: int = 10,
        use_cache: bool = True,
    ) -> SearchResult:
        """Intent ê¸°ë°˜ ê²€ìƒ‰ (Cached)"""
        
        # 1. Cache key ìƒì„±
        cache_key = self._make_cache_key(query, intent, top_k)
        
        # 2. Cache hit check
        if use_cache:
            cached = await self._get_from_cache(cache_key)
            if cached:
                logger.info(f"Cache hit: {cache_key}")
                return cached
        
        # 3. ì‹¤ì œ ê²€ìƒ‰ (AutoRRF)
        results = await self.retrieval.search(
            query=query,
            intent=intent,
            top_k=top_k,
        )
        
        # 4. Context êµ¬ì„±
        context = await self._build_context(results)
        
        search_result = SearchResult(
            chunks=results,
            context=context,
            token_count=sum(c.token_count for c in results),
        )
        
        # 5. Cache ì €ì¥
        if use_cache:
            await self._save_to_cache(cache_key, search_result)
        
        return search_result
    
    def _make_cache_key(self, query: str, intent: str, top_k: int) -> str:
        """Cache key ìƒì„± (Semantic)"""
        # Option 1: Hash (ë¹ ë¦„, ì •í™• ì¼ì¹˜ë§Œ)
        # return hashlib.sha256(f"{query}:{intent}:{top_k}".encode()).hexdigest()
        
        # Option 2: Embedding similarity (ëŠë¦¼, ìœ ì‚¬ ì¿¼ë¦¬ë„ ìºì‹±)
        # Embeddingì„ ê³„ì‚°í•˜ê³  Redis Vector Searchë¡œ ìœ ì‚¬ ì¿¼ë¦¬ ì°¾ê¸°
        # í˜„ì¬ëŠ” Option 1 ì‚¬ìš©
        return hashlib.sha256(f"{query}:{intent}:{top_k}".encode()).hexdigest()[:16]
    
    async def _get_from_cache(self, key: str) -> Optional[SearchResult]:
        """Cache ì¡°íšŒ"""
        try:
            data = await self.cache.get(key)
            if data:
                return SearchResult.from_dict(data)
        except Exception as e:
            logger.warning(f"Cache get failed: {e}")
        return None
    
    async def _save_to_cache(self, key: str, result: SearchResult):
        """Cache ì €ì¥"""
        try:
            await self.cache.setex(
                key,
                self.cache_ttl,
                result.to_dict(),
            )
        except Exception as e:
            logger.warning(f"Cache save failed: {e}")
    
    async def get_symbol_info(self, symbol_id: str) -> SymbolInfo:
        """Symbol ì •ë³´ (LRU Cached)"""
        
        # In-memory LRU
        if symbol_id in self._symbol_cache:
            return self._symbol_cache[symbol_id]
        
        # DB ì¡°íšŒ
        symbol_info = await self.graph.get_node(symbol_id)
        
        # Cache ì €ì¥
        if len(self._symbol_cache) >= self._max_cache_size:
            # LRU eviction
            oldest_key = next(iter(self._symbol_cache))
            del self._symbol_cache[oldest_key]
        
        self._symbol_cache[symbol_id] = symbol_info
        return symbol_info
```

#### Day 3-5: Router with Budget Enforcement

```python
# src/agent/orchestrator/orchestrator.py

import asyncio
from typing import Optional

class AgentOrchestrator:
    
    async def execute(
        self,
        user_request: str,
        context: Optional[Dict[str, Any]] = None,
    ) -> AgentResult:
        """ë©”ì¸ ì‹¤í–‰ (Budget ê°•ì œ)"""
        
        start_time = time.time()
        context = context or {}
        
        # Budget ì„¤ì • (Default: 30ì´ˆ)
        budget_seconds = context.get("budget_seconds", 30.0)
        
        try:
            # asyncio.wait_forë¡œ Budget ê°•ì œ
            result = await asyncio.wait_for(
                self._execute_internal(user_request, context),
                timeout=budget_seconds,
            )
            
            return result
            
        except asyncio.TimeoutError:
            # Budget ì´ˆê³¼ â†’ PartialResult
            execution_time = (time.time() - start_time) * 1000
            
            logger.warning(f"Budget exceeded: {execution_time:.0f}ms > {budget_seconds * 1000}ms")
            
            return AgentResult(
                intent=context.get("intent", "unknown"),
                confidence=0.0,
                status=ExecutionStatus.PARTIAL,
                result=context.get("partial_result", None),
                error=f"Budget exceeded: {budget_seconds}s",
                error_details={
                    "budget_seconds": budget_seconds,
                    "actual_seconds": execution_time / 1000,
                },
                execution_time_ms=execution_time,
            )
        
        except Exception as e:
            # ë‹¤ë¥¸ ì—ëŸ¬
            execution_time = (time.time() - start_time) * 1000
            return AgentResult(
                intent=context.get("intent", "unknown"),
                confidence=0.0,
                status=ExecutionStatus.FAILED,
                result=None,
                error=str(e),
                execution_time_ms=execution_time,
            )
    
    async def _execute_internal(
        self,
        user_request: str,
        context: Dict[str, Any],
    ) -> AgentResult:
        """ì‹¤ì œ ì‹¤í–‰ ë¡œì§ (ê¸°ì¡´ execute ë‚´ìš©)"""
        
        # Step 1: Router
        intent_result = await self._route(user_request, context)
        
        # Partial result ì €ì¥ (Timeout ëŒ€ë¹„)
        context["partial_result"] = {
            "intent": intent_result.intent,
            "confidence": intent_result.confidence,
        }
        
        # Step 2: TaskGraph
        task_graph = self._plan(intent_result)
        
        # Step 3: Workflow
        final_state = await self._execute_workflow(intent_result, task_graph)
        
        # Step 4: Result
        execution_time = (time.time() - context.get("start_time", time.time())) * 1000
        return self._format_result(intent_result, task_graph, final_state, execution_time)
```

---

### **Week 3-4: MetaLayer + JSON Parsing** (60% â†’ 85%)

#### Day 11-14: DynamicTaskGraphPlanner with json_repair

```python
# src/agent/task_graph/dynamic_planner.py

import json
from json_repair import repair_json  # pip install json-repair
from litellm import completion  # LiteLLM
from pydantic import BaseModel, ValidationError

class TaskSchema(BaseModel):
    """Pydantic Schema (Validation)"""
    id: str
    type: str
    description: str
    depends_on: List[str] = []

class TaskGraphSchema(BaseModel):
    """Task Graph Schema"""
    tasks: List[TaskSchema]

class DynamicTaskGraphPlanner:
    """LLM ê¸°ë°˜ ë™ì  Task ë¶„í•´ (Robust JSON Parsing)"""
    
    def __init__(self, static_planner: TaskGraphPlanner):
        self.static_planner = static_planner
    
    async def plan(
        self,
        user_intent: str,
        context: Dict[str, Any],
        analyzed_data: Optional[Dict] = None,
    ) -> TaskGraph:
        """ë™ì  Task ìƒì„±"""
        
        # Complexity íŒë‹¨
        complexity = self._estimate_complexity(user_intent, analyzed_data)
        
        if complexity == "simple":
            return self.static_planner.plan(user_intent, context)
        
        # LLM í˜¸ì¶œ (LiteLLM)
        prompt = self._build_decomposition_prompt(user_intent, analyzed_data)
        
        # Retry 3íšŒ
        for attempt in range(3):
            try:
                # LiteLLM completion
                response = await completion(
                    model="gpt-4",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.2,
                    response_format={"type": "json_object"},  # JSON mode
                )
                
                raw_json = response.choices[0].message.content
                
                # JSON parsing with repair
                task_graph = self._parse_task_graph_robust(raw_json)
                
                return task_graph
                
            except (json.JSONDecodeError, ValidationError) as e:
                logger.warning(f"JSON parsing failed (attempt {attempt + 1}): {e}")
                
                if attempt == 2:
                    # 3ë²ˆ ì‹¤íŒ¨ â†’ Fallback
                    logger.error("LLM JSON parsing failed 3 times, using static planner")
                    return self.static_planner.plan(user_intent, context)
    
    def _parse_task_graph_robust(self, raw_json: str) -> TaskGraph:
        """Robust JSON parsing (json_repair + Pydantic)"""
        
        # Step 1: json_repair (ê¹¨ì§„ JSON ìˆ˜ì •)
        try:
            repaired = repair_json(raw_json)
            data = json.loads(repaired)
        except Exception as e:
            logger.error(f"json_repair failed: {e}")
            raise
        
        # Step 2: Pydantic validation
        try:
            schema = TaskGraphSchema(**data)
        except ValidationError as e:
            logger.error(f"Pydantic validation failed: {e}")
            raise
        
        # Step 3: TaskGraph ìƒì„±
        task_graph = TaskGraph()
        
        for task_schema in schema.tasks:
            task = Task(
                id=task_schema.id,
                type=TaskType(task_schema.type),
                description=task_schema.description,
                depends_on=task_schema.depends_on,
            )
            task_graph.add_task(task)
        
        # Validation
        task_graph.validate_dag()
        task_graph.topological_sort()
        
        return task_graph
```

#### Day 15-17: GuardrailEngine with Pre-compiled Regex

```python
# src/safety/guardrail/rule_engine.py

import re
import yaml
from pathlib import Path
from typing import Dict, List, Callable, Pattern
from dataclasses import dataclass

@dataclass
class CompiledGuardrailRule:
    """Pre-compiled Rule (ì„±ëŠ¥ ìµœì í™”)"""
    id: str
    name: str
    description: str
    severity: str
    check: Callable
    patterns: List[Pattern] = None  # Pre-compiled regex

class GuardrailEngine:
    """ê·œì¹™ ì—”ì§„ (Regex Pre-compilation)"""
    
    def __init__(self, rules_path: str = "config/guardrail_rules.yaml"):
        self.rules: List[CompiledGuardrailRule] = []
        self._load_rules(rules_path)
    
    def _load_rules(self, path: str):
        """YAML â†’ Compiled Rule"""
        config = yaml.safe_load(Path(path).read_text())
        
        for rule_config in config["rules"]:
            compiled_rule = self._build_rule(rule_config)
            self.rules.append(compiled_rule)
        
        logger.info(f"Loaded {len(self.rules)} guardrail rules")
    
    def _build_rule(self, config: Dict) -> CompiledGuardrailRule:
        """Rule ìƒì„± (Regex Pre-compile)"""
        
        if config["name"] == "LOC_LIMIT":
            return CompiledGuardrailRule(
                id=config["id"],
                name=config["name"],
                description=config["description"],
                severity=config["severity"],
                check=lambda change, ctx: (
                    change.lines_added < config["params"]["max_lines"]
                ),
            )
        
        elif config["name"] == "NO_SECRET":
            # Regex Pre-compile (1íšŒë§Œ)
            compiled_patterns = [
                re.compile(pattern, re.IGNORECASE)
                for pattern in config["patterns"]
            ]
            
            def check_secret(change, ctx):
                """Secret ê²€ì‚¬ (Pre-compiled Regex ì‚¬ìš©)"""
                for pattern in compiled_patterns:
                    if pattern.search(change.content):
                        return False
                return True
            
            return CompiledGuardrailRule(
                id=config["id"],
                name=config["name"],
                description=config["description"],
                severity=config["severity"],
                check=check_secret,
                patterns=compiled_patterns,
            )
        
        elif config["name"] == "FILE_LIMIT":
            return CompiledGuardrailRule(
                id=config["id"],
                name=config["name"],
                description=config["description"],
                severity=config["severity"],
                check=lambda change, ctx: (
                    len(change.files) <= config["params"]["max_files"]
                ),
            )
        
        # ... ë‹¤ë¥¸ ê·œì¹™ë“¤
    
    def check(
        self,
        code_change: CodeChange,
        context: Optional[Dict] = None
    ) -> GuardrailResult:
        """ëª¨ë“  ê·œì¹™ ì²´í¬ (Pre-compiled Regexë¡œ ë¹ ë¦„)"""
        violations = []
        context = context or {}
        
        for rule in self.rules:
            try:
                if not rule.check(code_change, context):
                    violations.append(Violation(
                        rule_id=rule.id,
                        rule_name=rule.name,
                        description=rule.description,
                        severity=rule.severity,
                    ))
            except Exception as e:
                logger.error(f"Rule {rule.id} failed: {e}")
        
        has_blocker = any(v.severity == "blocker" for v in violations)
        
        return GuardrailResult(
            passed=not has_blocker,
            violations=violations,
        )
```

---

### **Week 4.5: Sandbox R&D (ì¡°ê¸° ì‹œì‘)** âš ï¸

**í”¼ë“œë°± ë°˜ì˜**: Week 6ì˜ ë‚œê´€ì„ ëŒ€ë¹„í•˜ì—¬ Week 4 í›„ë°˜ë¶€í„° R&D ì‹œì‘

#### Day 18-20: E2B SDK ì¡°ì‚¬ + Session-based Container PoC

```python
# research/sandbox_poc.py

"""
Sandbox ì „ëµ ë¹„êµ:
1. Docker containers.run (í˜„ì¬) - ëŠë¦¼
2. Docker exec_run (Session-based) - ë¹ ë¦„
3. E2B Code Interpreter SDK - SOTA (ê°€ì¥ ë¹ ë¦„)
"""

# Option 1: ê¸°ì¡´ ë°©ì‹ (ëŠë¦¼)
def test_run_approach():
    import docker
    client = docker.from_env()
    
    start = time.time()
    for i in range(10):
        result = client.containers.run(
            "python:3.11-slim",
            "python -c 'print(1+1)'",
            remove=True,
        )
    print(f"containers.run x10: {time.time() - start:.2f}s")
    # ì˜ˆìƒ: 15-20ì´ˆ

# Option 2: Session-based (ë¹ ë¦„)
def test_exec_approach():
    import docker
    client = docker.from_env()
    
    # 1íšŒë§Œ ë„ì›€
    container = client.containers.run(
        "python:3.11-slim",
        command="tail -f /dev/null",
        detach=True,
    )
    
    start = time.time()
    for i in range(10):
        exec_result = container.exec_run("python -c 'print(1+1)'")
    print(f"exec_run x10: {time.time() - start:.2f}s")
    # ì˜ˆìƒ: 0.5-1ì´ˆ
    
    container.stop()
    container.remove()

# Option 3: E2B SDK (SOTA)
def test_e2b_approach():
    from e2b import CodeInterpreter
    
    with CodeInterpreter() as sandbox:
        start = time.time()
        for i in range(10):
            result = sandbox.run_code("print(1+1)")
        print(f"E2B x10: {time.time() - start:.2f}s")
        # ì˜ˆìƒ: 0.3-0.5ì´ˆ

# ì‹¤í–‰
test_run_approach()
test_exec_approach()
test_e2b_approach()
```

**ê²°ë¡ **: E2B SDKê°€ ê°€ì¥ ë¹ ë¥´ì§€ë§Œ, ì˜ì¡´ì„± ì„¤ì¹˜ ë“±ì€ ì§ì ‘ Dockerë¡œ ì œì–´ í•„ìš”. **í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ** ì±„íƒ:
- ê¸°ë³¸: Session-based Docker (exec_run)
- ê³ ê¸‰: E2B SDK (ê°„ë‹¨í•œ Python ì‹¤í–‰)

---

### **Week 5: Workflow + State Persistence** (40% â†’ 85%)

#### Day 29-31: LangGraph Checkpoint Integration

```python
# src/agent/workflow/state_machine.py

from langgraph.checkpoint.sqlite import SqliteSaver  # LangGraph
from langgraph.graph import StateGraph, END

class WorkflowStateMachine:
    """
    LangGraph ê¸°ë°˜ Workflow (State Persistence í¬í•¨)
    """
    
    def __init__(
        self,
        max_iterations: int = 3,
        enable_full_workflow: bool = False,
        checkpoint_db: str = "checkpoints.db",
    ):
        self.max_iterations = max_iterations
        self.enable_full_workflow = enable_full_workflow
        
        # LangGraph Checkpoint
        self.checkpointer = SqliteSaver.from_conn_string(checkpoint_db)
        
        # Graph ì •ì˜
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """LangGraph ì •ì˜"""
        
        workflow = StateGraph(WorkflowState)
        
        # Nodes
        workflow.add_node("analyze", self._analyze)
        workflow.add_node("plan", self._plan)
        workflow.add_node("generate", self._generate)
        workflow.add_node("critic", self._critic)
        workflow.add_node("test", self._test)
        workflow.add_node("self_heal", self._self_heal)
        
        # Edges
        workflow.add_edge("analyze", "plan")
        workflow.add_edge("plan", "generate")
        workflow.add_edge("generate", "critic")
        
        # Conditional edges
        workflow.add_conditional_edges(
            "critic",
            lambda state: "approved" if state.context.get("critic_approved") else "rejected",
            {
                "approved": "test",
                "rejected": "generate",  # ì¬ìƒì„±
            }
        )
        
        workflow.add_conditional_edges(
            "test",
            lambda state: "passed" if state.context.get("test_passed") else "failed",
            {
                "passed": END,
                "failed": "self_heal",
            }
        )
        
        workflow.add_edge("self_heal", "generate")
        
        # Entry point
        workflow.set_entry_point("analyze")
        
        return workflow.compile(checkpointer=self.checkpointer)
    
    async def run(self, initial_state: WorkflowState) -> WorkflowState:
        """Workflow ì‹¤í–‰ (Checkpoint ìë™ ì €ì¥)"""
        
        session_id = initial_state.context.get("session_id", "default")
        
        # LangGraph invoke (Checkpoint ìë™ ê´€ë¦¬)
        final_state = await self.graph.ainvoke(
            initial_state,
            config={"configurable": {"thread_id": session_id}},
        )
        
        return final_state
    
    async def resume(self, session_id: str) -> WorkflowState:
        """ì¤‘ë‹¨ëœ Workflow ì¬ê°œ"""
        
        # Checkpointì—ì„œ ë§ˆì§€ë§‰ ìƒíƒœ ë¡œë“œ
        last_state = await self.graph.aget_state(
            config={"configurable": {"thread_id": session_id}},
        )
        
        if not last_state:
            raise ValueError(f"No checkpoint found for session: {session_id}")
        
        # ì¬ê°œ
        final_state = await self.graph.ainvoke(
            last_state,
            config={"configurable": {"thread_id": session_id}},
        )
        
        return final_state
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# 1. ì‹¤í–‰
workflow = WorkflowStateMachine(checkpoint_db="checkpoints.db")
state = WorkflowState(session_id="abc123", ...)

# ì¤‘ê°„ì— í¬ë˜ì‹œ ë°œìƒ
final_state = await workflow.run(state)

# 2. ì¬ê°œ (ì„œë²„ ì¬ì‹œì‘ í›„)
workflow = WorkflowStateMachine(checkpoint_db="checkpoints.db")
final_state = await workflow.resume(session_id="abc123")
```

---

### **Week 6: Session-based Sandbox** (20% â†’ 90%)

#### Day 41-45: DockerSandbox with exec_run

```python
# src/execution/sandbox/docker_sandbox.py

import docker
from pathlib import Path
from typing import Optional, Dict
import tempfile
import shutil

class DockerSandbox:
    """Session-based Docker Sandbox (exec_run ì‚¬ìš©)"""
    
    def __init__(
        self,
        image: str = "python:3.11-slim",
        timeout: int = 30,
        memory_limit: str = "512m",
        cpu_limit: float = 1.0,
        network_mode: str = "none",
    ):
        self.image = image
        self.timeout = timeout
        self.memory_limit = memory_limit
        self.cpu_limit = cpu_limit
        self.network_mode = network_mode
        
        self.client = docker.from_env()
        self.container: Optional[docker.models.containers.Container] = None
        
        # Session ìƒíƒœ
        self.is_running = False
        self.workspace_path: Optional[Path] = None
    
    async def start_session(
        self,
        workspace: Path,
        env: Optional[Dict[str, str]] = None,
    ):
        """ì„¸ì…˜ ì‹œì‘: ì»¨í…Œì´ë„ˆë¥¼ ë°ëª¬ìœ¼ë¡œ ë„ì›€"""
        
        if self.is_running:
            logger.warning("Session already running")
            return
        
        logger.info("Starting sandbox session...")
        
        # Workspace ì¤€ë¹„ (ì„ì‹œ ë””ë ‰í† ë¦¬)
        self.workspace_path = workspace
        
        # ì»¨í…Œì´ë„ˆ ì‹œì‘ (Keep-alive)
        self.container = self.client.containers.run(
            image=self.image,
            command="tail -f /dev/null",  # Keep alive
            detach=True,
            tty=True,
            stdin_open=True,
            
            # Volume mount (rw + /tmp ì‚¬ìš©)
            volumes={
                str(workspace.absolute()): {
                    "bind": "/workspace",
                    "mode": "rw",  # Read-write (ìºì‹œ ìƒì„± í—ˆìš©)
                }
            },
            working_dir="/workspace",
            
            # Environment
            environment=env or {},
            
            # Resource limits
            mem_limit=self.memory_limit,
            cpu_period=100000,
            cpu_quota=int(self.cpu_limit * 100000),
            
            # Security
            network_mode=self.network_mode,
            user="nobody",  # Non-root
            
            # Auto-remove
            remove=False,  # ì„¸ì…˜ ì¢…ë£Œ ì‹œ ìˆ˜ë™ ì‚­ì œ
        )
        
        self.is_running = True
        logger.info(f"Sandbox session started: {self.container.id[:12]}")
    
    async def execute_command(
        self,
        command: str,
        timeout: Optional[int] = None,
    ) -> ExecutionResult:
        """ëª…ë ¹ ì‹¤í–‰ (exec_run ì‚¬ìš©)"""
        
        if not self.is_running:
            raise RuntimeError("Session not started. Call start_session() first.")
        
        timeout = timeout or self.timeout
        
        logger.info(f"Executing: {command}")
        
        try:
            # exec_run (ë¹ ë¦„!)
            exec_result = self.container.exec_run(
                cmd=f"bash -c '{command}'",
                stdout=True,
                stderr=True,
                stdin=False,
                tty=False,
                demux=True,  # stdout/stderr ë¶„ë¦¬
                user="nobody",
            )
            
            exit_code = exec_result.exit_code
            stdout, stderr = exec_result.output
            
            return ExecutionResult(
                success=exit_code == 0,
                exit_code=exit_code,
                stdout=stdout.decode("utf-8") if stdout else "",
                stderr=stderr.decode("utf-8") if stderr else "",
            )
            
        except Exception as e:
            logger.error(f"Command execution failed: {e}")
            return ExecutionResult(
                success=False,
                exit_code=-1,
                stdout="",
                stderr=str(e),
            )
    
    async def execute_tests(
        self,
        test_command: str = "pytest -v",
    ) -> TestResult:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        result = await self.execute_command(test_command)
        
        # pytest ì¶œë ¥ íŒŒì‹±
        test_result = self._parse_pytest_output(result.stdout)
        test_result.stderr = result.stderr
        
        return test_result
    
    async def install_dependencies(
        self,
        requirements: str = "requirements.txt",
    ):
        """ì˜ì¡´ì„± ì„¤ì¹˜ (ì„¸ì…˜ ë‚´ ìœ ì§€)"""
        
        result = await self.execute_command(
            f"pip install -r {requirements}",
            timeout=120,  # 2ë¶„
        )
        
        if not result.success:
            logger.error(f"Dependency installation failed: {result.stderr}")
            raise RuntimeError(f"pip install failed: {result.stderr}")
    
    async def stop_session(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        
        if not self.is_running:
            return
        
        logger.info("Stopping sandbox session...")
        
        try:
            self.container.stop(timeout=5)
            self.container.remove()
        except Exception as e:
            logger.error(f"Failed to stop container: {e}")
        
        self.is_running = False
        self.container = None
    
    def __enter__(self):
        """Context manager"""
        return self
    
    async def __aenter__(self):
        """Async context manager"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.stop_session()
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# Session ë°©ì‹ (ë¹ ë¦„)
async with DockerSandbox() as sandbox:
    await sandbox.start_session(workspace=Path("/tmp/my_workspace"))
    
    # ì˜ì¡´ì„± ì„¤ì¹˜ (1íšŒ)
    await sandbox.install_dependencies()
    
    # ì—¬ëŸ¬ ëª…ë ¹ ì‹¤í–‰ (ì»¨í…Œì´ë„ˆ ì¬ì‚¬ìš©)
    await sandbox.execute_tests("pytest test_1.py")
    await sandbox.execute_tests("pytest test_2.py")
    await sandbox.execute_tests("pytest test_3.py")
    
    # ìë™ cleanup
```

#### E2B SDK í™œìš© (ì„ íƒì )

```python
# src/execution/sandbox/e2b_sandbox.py

from e2b import CodeInterpreter

class E2BSandbox:
    """E2B Code Interpreter Sandbox (ê°€ì¥ ë¹ ë¦„)"""
    
    async def execute_python(self, code: str) -> str:
        """Python ì½”ë“œ ì‹¤í–‰ (E2B)"""
        
        with CodeInterpreter() as sandbox:
            result = sandbox.run_code(code)
            
            return result.text
```

---

### **Week 7: Edge Cases + Switchable Mock** (90% â†’ 100%)

#### Day 46-48: Config-based Mock Switching

```python
# src/config/agent_config.py

from pydantic import BaseSettings

class AgentConfig(BaseSettings):
    """Agent ì„¤ì • (í™˜ê²½ë³€ìˆ˜)"""
    
    # Mock switches (ê°œë°œ ì†ë„ ìœ ì§€)
    use_real_sandbox: bool = False  # False: Mock, True: Docker
    use_real_llm: bool = False      # False: Mock, True: LiteLLM
    use_real_cache: bool = True     # False: In-memory, True: Redis
    
    # Budget
    default_budget_seconds: float = 30.0
    max_budget_seconds: float = 300.0
    
    # Sandbox
    sandbox_image: str = "python:3.11-slim"
    sandbox_timeout: int = 30
    sandbox_memory_limit: str = "512m"
    
    class Config:
        env_prefix = "AGENT_"
        env_file = ".env"

# ì‚¬ìš©
config = AgentConfig()

if config.use_real_sandbox:
    sandbox = DockerSandbox()
else:
    sandbox = MockSandbox()
```

---

### **Week 8: E2E + Performance** (95% â†’ 100%)

#### Day 54-55: Performance Optimization

**ìµœì í™” ëª©í‘œ**:
1. Context ë³‘ë ¬ ê²€ìƒ‰ (3ê°œ strategy ë³‘ë ¬)
2. LLM í˜¸ì¶œ ìºì‹±
3. Chunk deduplication

```python
# src/agent/adapters/context_adapter.py

async def search_parallel(
    self,
    query: str,
    strategies: List[str],  # ["symbol", "vector", "lexical"]
) -> Dict[str, SearchResult]:
    """ë³‘ë ¬ ê²€ìƒ‰ (3ê°œ strategy ë™ì‹œ)"""
    
    tasks = []
    for strategy in strategies:
        tasks.append(
            self._search_single_strategy(query, strategy)
        )
    
    # asyncio.gatherë¡œ ë³‘ë ¬ ì‹¤í–‰
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # ê²°ê³¼ ë§¤í•‘
    strategy_results = {}
    for strategy, result in zip(strategies, results):
        if isinstance(result, Exception):
            logger.error(f"Strategy {strategy} failed: {result}")
        else:
            strategy_results[strategy] = result
    
    return strategy_results
```

---

## ğŸ“Š ì˜¤í”ˆì†ŒìŠ¤ í™œìš© ë§¤í•‘

| ì˜¤í”ˆì†ŒìŠ¤ | ì‚¬ìš© ìœ„ì¹˜ | ëª©ì  |
|---------|----------|------|
| **LangGraph** | Workflow StateMachine | State Persistence, Checkpoint |
| **E2B** | Sandbox (ì„ íƒì ) | ë¹ ë¥¸ Python ì‹¤í–‰ |
| **LiteLLM** | DynamicPlanner, CodeCritic | LLM í˜¸ì¶œ í†µí•© |
| **Pydantic** | DynamicPlanner, Config | JSON Validation |
| **Guardrails AI** | GuardrailEngine | Policy ì •ì˜ (ì„ íƒì ) |
| **Playwright** | Week 9 (ADR-025) | Visual Verification |
| **GitPython** | VCS Apply | Git ì¡°ì‘ |

---

## ğŸ¯ ì„±ê³µ ì§€í‘œ (Updated)

| Metric | Before | After (V2) | ê°œì„  |
|--------|--------|------------|------|
| **Sandbox ì†ë„** | 2ì´ˆ/í…ŒìŠ¤íŠ¸ | 0.1ì´ˆ/í…ŒìŠ¤íŠ¸ | **20x** |
| **Context Cache Hit** | 0% | 50% | **2x ë¹ ë¦„** |
| **JSON Parsing ì—ëŸ¬ìœ¨** | 10% | < 1% | **10x** |
| **Long-running Task ë³µêµ¬** | ë¶ˆê°€ëŠ¥ | ê°€ëŠ¥ | âœ… |
| **Budget ì¤€ìˆ˜ìœ¨** | 50% | 95%+ | âœ… |

---

## ğŸš€ Action Items (Day 1ë¶€í„°)

### Day 1 (ì§€ê¸ˆ ë°”ë¡œ)
```bash
# 1. Context Adapter ìƒì„±
touch src/agent/adapters/context_adapter.py

# 2. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install langgraph json-repair guardrails-ai e2b litellm

# 3. Config íŒŒì¼ ìƒì„±
mkdir -p config
touch config/guardrail_rules.yaml
touch .env
```

### Day 2-3
- ContextAdapter êµ¬í˜„ (Semantic Cache í¬í•¨)
- Redis ì—°ë™ í…ŒìŠ¤íŠ¸

### Day 4-5
- Router Budget Enforcement
- `asyncio.wait_for` í†µí•© í…ŒìŠ¤íŠ¸

**Go!** ğŸš€

