# RFC-06 Sub-RFCs - Detailed Specifications

**Version:** 1.0 Final  
**Created:** 2025-12-05  
**Status:** Approved for Implementation

ë³¸ ë¬¸ì„œëŠ” RFC-06 v3.1ì˜ 4ê°œ ì„œë¸Œ RFCë¥¼ ìƒì„¸ ì •ì˜í•©ë‹ˆë‹¤.

---

## RFC-06-EFFECT: Effect System

### 1. Goal

ì½”ë“œ ë³€í™”ì˜ **ë™ì‘ ì˜ë¯¸(behavioral semantics)**ë¥¼ ê°ì§€í•˜ê¸° ìœ„í•´,  
í•¨ìˆ˜/ëª¨ë“ˆ ë‹¨ìœ„ì˜ **EffectSet**(side-effect signature)ì„ ì •ì ìœ¼ë¡œ ì¶”ë¡ í•˜ê³   
ë³€ê²½ ì „í›„ì˜ **EffectDiff**ë¥¼ ì‚°ì¶œí•˜ëŠ” ì‹œìŠ¤í…œì„ ì •ì˜í•œë‹¤.

### 2. Problem

í…ìŠ¤íŠ¸ diff/AST diffëŠ” **"ë™ì‘ ë³€í™”"**ë¥¼ ê°ì§€í•˜ì§€ ëª»í•œë‹¤.

**ì˜ˆì‹œ:**
```python
# Before
def foo():
    x = 1

# After
def foo():
    global_state.counter += 1
    x = 1
```

**ì˜ë¯¸ ë³€í™”:** Side Effect ì¶”ê°€ (Global Mutation)  
â†’ ì´ë¥¼ ê°ì§€í•˜ê¸° ìœ„í•´ Effect Systemì´ í•„ìˆ˜

### 3. Effect Domain

#### 3.1 Effect Types   

"""
#### ì£¼ìš” EffectType ì •ì˜

- PURE: ìƒíƒœ ë³€í˜•/ì™¸ë¶€ íš¨ê³¼ ì—†ìŒ. (ì°¸ì¡° íˆ¬ëª…)
- READ_STATE: ê¸€ë¡œë²Œ/ìŠ¤íƒœí‹±/ë©¤ë²„ ìƒíƒœ ì½ê¸°.
- WRITE_STATE: ê¸€ë¡œë²Œ/ìŠ¤íƒœí‹±/ë©¤ë²„ ìƒíƒœ ì“°ê¸°.
- GLOBAL_MUTATION: ê¸€ë¡œë²Œ/ì‹±ê¸€í„´ ê°ì²´ ë³€í˜•.
- IO: íŒŒì¼/ì½˜ì†”/ì‹œìŠ¤í…œ ì…ì¶œë ¥.
- LOG: ë¡œê¹… ì‘ì—… (ì¼ë°˜ì ìœ¼ë¡œ IOì˜ í•˜ìœ„ ë²”ì£¼).
- DB_READ / DB_WRITE: ì™¸ë¶€ Database ì½ê¸°/ì“°ê¸°.
- NETWORK: ë„¤íŠ¸ì›Œí¬ ìš”ì²­ (ì™¸ë¶€ API í¬í•¨).
- UNKNOWN_EFFECT: ì •ì  ë¶„ì„ ë¶ˆê°€, ë¯¸ìƒ íš¨ê³¼.

ì´ EffectType("ìˆœìˆ˜" â†’ "ë¶€ìˆ˜íš¨ê³¼ ìˆìŒ")ì€ Static Analyzerê°€ í•¨ìˆ˜/ë©”ì„œë“œì˜ ì˜ë¯¸ ë³€í™”ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ê°ì§€í•˜ëŠ” ê¸°ë°˜ì´ ëœë‹¤.
"""


```python
from enum import Enum 

class EffectType(str, Enum):
    """Effect ë¶„ë¥˜"""
    PURE = "pure"
    
    # State Effects
    READ_STATE = "read_state"
    WRITE_STATE = "write_state"
    GLOBAL_MUTATION = "global_mutation"
    
    # I/O Effects (WriteStateì˜ subtype)
    IO = "io"
    LOG = "log"
    
    # External Effects
    DB_READ = "db_read"
    NETWORK = "network"
    
    
    # Unknown
    UNKNOWN_EFFECT = "unknown_effect"

# Effect Hierarchy (ìƒì† ê´€ê³„)
EFFECT_HIERARCHY = {
    EffectType.IO: EffectType.WRITE_STATE,
    EffectType.LOG: EffectType.WRITE_STATE,
    EffectType.DB_WRITE: EffectType.WRITE_STATE,
    EffectType.DB_READ: EffectType.READ_STATE,
    EffectType.NETWORK: EffectType.WRITE_STATE,
}
```

#### 3.2 Idempotency

```python
@dataclass
class EffectSet:
    """í•¨ìˆ˜ì˜ effect ì§‘í•©"""
    symbol_id: str
    effects: set[EffectType]
    idempotent: bool
    confidence: float  # 0.0 ~ 1.0
    source: Literal["static", "inferred", "allowlist", "annotation", "unknown"]
    
    def is_pure(self) -> bool:
        return self.effects == {EffectType.PURE}
    
    def has_side_effect(self) -> bool:
        return not self.is_pure()
    
    def includes(self, effect: EffectType) -> bool:
        """Hierarchy ê³ ë ¤í•œ í¬í•¨ ì—¬ë¶€"""
        if effect in self.effects:
            return True
        
        # Check hierarchy
        for e in self.effects:
            if EFFECT_HIERARCHY.get(e) == effect:
                return True
        
        return False
```

**Idempotency ì˜ˆì‹œ:**
```python
cache.set(k, v)      # WriteState + Idempotent
list.append(x)       # WriteState + NonIdempotent
logging.info(msg)    # Log + Idempotent
counter += 1         # GlobalMutation + NonIdempotent
```

### 4. ì¶”ë¡  ì•Œê³ ë¦¬ì¦˜

#### 4.1 Local Effects

```python
class LocalEffectAnalyzer:
    """ì†ŒìŠ¤ ì½”ë“œì—ì„œ ëª…ì‹œì  effect ì¶”ì¶œ"""
    
    def analyze(self, node: IRNode) -> EffectSet:
        """í•¨ìˆ˜ì˜ local effect ë¶„ì„"""
        effects = set()
        idempotent = True
        
        for stmt in node.body:
            # Global ë³€ìˆ˜ ìˆ˜ì •
            if self._is_global_mutation(stmt):
                effects.add(EffectType.GLOBAL_MUTATION)
                idempotent = False
            
            # I/O ì—°ì‚°
            elif self._is_io_operation(stmt):
                effects.add(EffectType.IO)
                # printëŠ” idempotent, file writeëŠ” ì•„ë‹˜
                if not self._is_idempotent_io(stmt):
                    idempotent = False
            
            # DB ì—°ì‚°
            elif self._is_db_operation(stmt):
                if self._is_read_operation(stmt):
                    effects.add(EffectType.DB_READ)
                else:
                    effects.add(EffectType.DB_WRITE)
                    idempotent = self._is_idempotent_db(stmt)
            
            # Network ì—°ì‚°
            elif self._is_network_operation(stmt):
                effects.add(EffectType.NETWORK)
                idempotent = False
        
        return EffectSet(
            symbol_id=node.id,
            effects=effects or {EffectType.PURE},
            idempotent=idempotent,
            confidence=1.0,
            source="static"
        )
    
    def _is_global_mutation(self, stmt) -> bool:
        """Global ë³€ìˆ˜ ìˆ˜ì • ê°ì§€"""
        # global keyword
        # module-level variable assignment
        pass
    
    def _is_io_operation(self, stmt) -> bool:
        """I/O ì—°ì‚° ê°ì§€"""
        # print, open, write, read
        pass
    
    def _is_idempotent_io(self, stmt) -> bool:
        """Idempotent I/O íŒë³„"""
        # print â†’ True
        # file.write â†’ False
        pass
```

#### 4.2 Interprocedural Propagation

```python
class EffectPropagator:
    """Effect ì „íŒŒ (callee â†’ caller)"""
    
    def propagate(
        self,
        node: IRNode,
        local_effect: EffectSet,
        callee_effects: dict[str, EffectSet],
        graph: GraphDocument
    ) -> EffectSet:
        """Callee effectë¥¼ callerë¡œ ì „íŒŒ"""
        
        propagated_effects = set(local_effect.effects)
        min_confidence = local_effect.confidence
        idempotent = local_effect.idempotent
        
        # Callee effect ìˆ˜ì§‘
        for edge in graph.edges:
            if edge.kind == "CALLS" and edge.source_node_id == node.id:
                callee_id = edge.target_node_id
                
                if callee_id in callee_effects:
                    callee_effect = callee_effects[callee_id]
                    propagated_effects.update(callee_effect.effects)
                    min_confidence = min(min_confidence, callee_effect.confidence)
                    
                    if not callee_effect.idempotent:
                        idempotent = False
                else:
                    # Unknown callee â†’ Pessimistic default
                    propagated_effects.add(EffectType.WRITE_STATE)
                    propagated_effects.add(EffectType.GLOBAL_MUTATION)
                    min_confidence = 0.5
                    idempotent = False
        
        return EffectSet(
            symbol_id=node.id,
            effects=propagated_effects,
            idempotent=idempotent,
            confidence=min_confidence,
            source="inferred"
        )
```

**ì „íŒŒ ê·œì¹™:**
```
Effect(f) = LocalEffect(f) âˆª (âˆª Effect(callees))
```

#### 4.3 Unknown Handling (ë™ì  ì–¸ì–´)

**ê¸°ë³¸ ì›ì¹™:**
- **Pessimistic Default:** Unknown â†’ `WriteState` + `GlobalMutation`

**ì˜ˆì™¸:**
1. **Trusted Library Allowlist** ì ìš©
2. ëª…ì‹œì  annotationì´ ìˆëŠ” ê²½ìš°

```python
class UnknownEffectHandler:
    """Unknown call ì²˜ë¦¬"""
    
    def __init__(self, allowlist: TrustedLibraryDB):
        self.allowlist = allowlist
    
    def handle_unknown_call(
        self,
        callee_name: str,
        call_context: dict
    ) -> EffectSet:
        """Unknown callì˜ effect ì¶”ì •"""
        
        # 1. Allowlist í™•ì¸
        if callee_name in self.allowlist:
            return self.allowlist.get_effect(callee_name)
        
        # 2. Annotation í™•ì¸
        if "effect_annotation" in call_context:
            return self._parse_annotation(call_context["effect_annotation"])
        
        # 3. Pattern matching (íœ´ë¦¬ìŠ¤í‹±)
        pattern_effect = self._match_patterns(callee_name)
        if pattern_effect:
            return pattern_effect
        
        # 4. Pessimistic default
        return EffectSet(
            symbol_id=callee_name,
            effects={EffectType.WRITE_STATE, EffectType.GLOBAL_MUTATION},
            idempotent=False,
            confidence=0.5,
            source="unknown"
        )
    
    def _match_patterns(self, name: str) -> Optional[EffectSet]:
        """íŒ¨í„´ ê¸°ë°˜ ì¶”ë¡ """
        for pattern, effect_spec in EFFECT_PATTERNS:
            if re.match(pattern, name):
                return EffectSet(
                    symbol_id=name,
                    effects=effect_spec["effects"],
                    idempotent=effect_spec["idempotent"],
                    confidence=effect_spec["confidence"],
                    source="inferred"
                )
        return None
```

### 5. Trusted Library Effect DB (Allowlist)

#### 5.1 êµ¬ì¡°

```python
@dataclass
class LibraryEffectSpec:
    """ë¼ì´ë¸ŒëŸ¬ë¦¬ í•¨ìˆ˜ì˜ effect ëª…ì„¸"""
    library: str
    function: str
    effects: set[EffectType]
    idempotent: bool
    confidence: float = 0.95

class TrustedLibraryDB:
    """Trusted library effect database"""
    
    def __init__(self):
        self.specs: dict[str, LibraryEffectSpec] = {}
        self._load_builtin_specs()
    
    def _load_builtin_specs(self):
        """ë‚´ì¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ effect ì •ì˜"""
        
        # Python builtin
        self.add_spec(LibraryEffectSpec(
            library="builtins",
            function="print",
            effects={EffectType.IO},
            idempotent=True
        ))
        
        # NumPy (pure functions)
        self.add_spec(LibraryEffectSpec(
            library="numpy",
            function="array",
            effects={EffectType.PURE},
            idempotent=True
        ))
        
        # Logging
        self.add_spec(LibraryEffectSpec(
            library="logging",
            function="info",
            effects={EffectType.LOG},
            idempotent=True
        ))
        
        # Redis
        self.add_spec(LibraryEffectSpec(
            library="redis",
            function="set",
            effects={EffectType.WRITE_STATE},
            idempotent=True
        ))
        
        self.add_spec(LibraryEffectSpec(
            library="redis",
            function="get",
            effects={EffectType.READ_STATE},
            idempotent=True
        ))
        
        # Database (SQLAlchemy)
        self.add_spec(LibraryEffectSpec(
            library="sqlalchemy",
            function="query",
            effects={EffectType.DB_READ},
            idempotent=True
        ))
        
        self.add_spec(LibraryEffectSpec(
            library="sqlalchemy",
            function="add",
            effects={EffectType.DB_WRITE},
            idempotent=False
        ))
```

#### 5.2 ì •ì±…

1. **Allowlist ìˆ˜ì •ì€ code review í•„ìˆ˜**
2. **ì–¸ì–´/í”„ë ˆì„ì›Œí¬ë³„ ê´€ë¦¬**
   - `allowlist/python_builtin.yaml`
   - `allowlist/python_stdlib.yaml`
   - `allowlist/python_numpy.yaml`
   - `allowlist/javascript_stdlib.yaml`

```yaml
# allowlist/python_stdlib.yaml
specs:
  - library: "logging"
    function: "info"
    effects: ["log"]
    idempotent: true
    confidence: 0.95
    
  - library: "logging"
    function: "error"
    effects: ["log"]
    idempotent: true
    confidence: 0.95
    
  - library: "json"
    function: "dumps"
    effects: ["pure"]
    idempotent: true
    confidence: 1.0
```

#### 5.3 Pattern Database (ë³´ì™„)

Allowlistë§Œìœ¼ë¡œ ë¶€ì¡±í•œ ê²½ìš° íŒ¨í„´ ë§¤ì¹­:

```python
# Effect Pattern Database
EFFECT_PATTERNS = [
    {
        "pattern": r".*\.set\(",
        "effects": {EffectType.WRITE_STATE},
        "idempotent": True,
        "confidence": 0.8,
        "reason": "set method pattern"
    },
    {
        "pattern": r".*\.append\(",
        "effects": {EffectType.WRITE_STATE},
        "idempotent": False,
        "confidence": 0.8,
        "reason": "append method pattern"
    },
    {
        "pattern": r".*\.log\(",
        "effects": {EffectType.LOG},
        "idempotent": True,
        "confidence": 0.7,
        "reason": "log method pattern"
    },
]
```

### 6. Differential Logic (EffectDiff)

```python
@dataclass
class EffectDiff:
    """Effect ë³€í™”"""
    symbol_id: str
    old_effect: EffectSet
    new_effect: EffectSet
    
    added_effects: set[EffectType]
    removed_effects: set[EffectType]
    
    idempotency_changed: bool
    risk_level: Literal["low", "medium", "high"]
    
    def is_behavioral_change(self) -> bool:
        """ë™ì‘ ë³€í™” ì—¬ë¶€"""
        return len(self.added_effects) > 0 or len(self.removed_effects) > 0

class EffectDiffer:
    """Effect ë¹„êµ"""
    
    def diff(
        self,
        old_effect: EffectSet,
        new_effect: EffectSet
    ) -> EffectDiff:
        """Effect diff ê³„ì‚°"""
        
        added = new_effect.effects - old_effect.effects
        removed = old_effect.effects - new_effect.effects
        
        idempotency_changed = (old_effect.idempotent != new_effect.idempotent)
        
        # Risk level ê³„ì‚°
        risk = self._calculate_risk(added, removed, idempotency_changed)
        
        return EffectDiff(
            symbol_id=new_effect.symbol_id,
            old_effect=old_effect,
            new_effect=new_effect,
            added_effects=added,
            removed_effects=removed,
            idempotency_changed=idempotency_changed,
            risk_level=risk
        )
    
    def _calculate_risk(
        self,
        added: set[EffectType],
        removed: set[EffectType],
        idempotency_changed: bool
    ) -> str:
        """ìœ„í—˜ë„ ê³„ì‚°"""
        
        # High risk: DB_WRITE, NETWORK, GLOBAL_MUTATION ì¶”ê°€
        high_risk_effects = {
            EffectType.DB_WRITE,
            EffectType.NETWORK,
            EffectType.GLOBAL_MUTATION
        }
        
        if added & high_risk_effects:
            return "high"
        
        # Medium risk: WRITE_STATE, IO ì¶”ê°€
        medium_risk_effects = {
            EffectType.WRITE_STATE,
            EffectType.IO
        }
        
        if added & medium_risk_effects or idempotency_changed:
            return "medium"
        
        # Low risk: READ_STATE, LOG ì¶”ê°€ ë˜ëŠ” effect ì œê±°
        if len(added) > 0 or len(removed) > 0:
            return "low"
        
        return "low"
```

### 7. ì œí•œì‚¬í•­

**ë™ì  ì–¸ì–´ì—ì„œ "ì •í™•í•œ" ì¶”ë¡ ì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.**

ë³¸ RFCëŠ” **Best Effort Static Approximation**ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

**False Positive í—ˆìš© ì •ì±…:**
- ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ **ë³´ìˆ˜ì ìœ¼ë¡œ effect ì¶”ê°€**
- Confidence scoreë¡œ ë¶ˆí™•ì‹¤ì„± í‘œí˜„
- LLM/Agentê°€ ìµœì¢… íŒë‹¨

---

## RFC-06-VFLOW: Cross-Language Value Flow Graph

### 1. Goal

FE â†’ API â†’ BE â†’ DBê¹Œì§€ ê°’(value)ì˜ íë¦„ì„  
cross-languageë¡œ ì¶”ì í•˜ëŠ” **Cross-Language Value Flow Graph (CVFG)** ì •ì˜.

### 2. Problem

```
TS:    interface User { userId: string }
â†“
Java:  class UserDTO { String userID; }
â†“
Python: class UserModel: user_id: str
â†“
SQL:    users.USER_ID VARCHAR(36)
```

**ë¬¸ì œ:**
- Naming convention ì°¨ì´ (camelCase, snake_case, UPPER_CASE)
- Type ë¶ˆì¼ì¹˜ (string, String, str, VARCHAR)
- ì •ì  ë¶„ì„ë§Œìœ¼ë¡œ ì—°ê²° ì–´ë ¤ì›€

### 3. í•µì‹¬ ìš”ì†Œ

#### 3.1 NFN (Normalized Field Name)

```python
def normalize_field_name(name: str) -> str:
    """í•„ë“œëª… ì •ê·œí™”"""
    # 1. Lower case
    normalized = name.lower()
    
    # 2. CamelCase â†’ snake_case
    # userId â†’ user_id
    normalized = re.sub(r'([a-z])([A-Z])', r'\1_\2', normalized)
    
    # 3. ì—¬ëŸ¬ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    normalized = re.sub(r'_+', '_', normalized)
    
    # 4. ì•ë’¤ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    normalized = normalized.strip('_')
    
    return normalized

# ì˜ˆì‹œ
assert normalize_field_name("userId") == "user_id"
assert normalize_field_name("user_id") == "user_id"
assert normalize_field_name("USER_ID") == "user_id"
assert normalize_field_name("userID") == "user_id"
```

#### 3.2 Type Compatibility Matrix

```python
@dataclass
class TypeCompatibility:
    """íƒ€ì… í˜¸í™˜ì„±"""
    source_type: str
    target_type: str
    compatible: bool
    confidence: float

# Type Compatibility Matrix
TYPE_COMPAT_MATRIX = {
    # String types
    ("string", "String"): TypeCompatibility("string", "String", True, 1.0),
    ("string", "str"): TypeCompatibility("string", "str", True, 1.0),
    ("string", "varchar"): TypeCompatibility("string", "varchar", True, 0.95),
    ("uuid", "string"): TypeCompatibility("uuid", "string", True, 0.9),
    
    # Number types
    ("int", "integer"): TypeCompatibility("int", "integer", True, 1.0),
    ("int", "float"): TypeCompatibility("int", "float", True, 0.8),
    ("int", "decimal"): TypeCompatibility("int", "decimal", True, 0.85),
    
    # Time types
    ("timestamp", "datetime"): TypeCompatibility("timestamp", "datetime", True, 0.95),
    ("date", "datetime"): TypeCompatibility("date", "datetime", True, 0.9),
    
    # Boolean
    ("bool", "boolean"): TypeCompatibility("bool", "boolean", True, 1.0),
}

def are_types_compatible(type1: str, type2: str) -> tuple[bool, float]:
    """ë‘ íƒ€ì…ì´ í˜¸í™˜ë˜ëŠ”ì§€ í™•ì¸"""
    key = (type1.lower(), type2.lower())
    
    if key in TYPE_COMPAT_MATRIX:
        compat = TYPE_COMPAT_MATRIX[key]
        return compat.compatible, compat.confidence
    
    # Reverse lookup
    reverse_key = (type2.lower(), type1.lower())
    if reverse_key in TYPE_COMPAT_MATRIX:
        compat = TYPE_COMPAT_MATRIX[reverse_key]
        return compat.compatible, compat.confidence
    
    # Exact match
    if type1.lower() == type2.lower():
        return True, 1.0
    
    # No match
    return False, 0.0
```

#### 3.3 Structural Hash

```python
def compute_structural_hash(
    schema: dict,
    namespace_salt: str
) -> str:
    """êµ¬ì¡° ê¸°ë°˜ í•´ì‹œ"""
    
    # 1. í•„ë“œ ì •ê·œí™”
    normalized_fields = []
    for field_name, field_type in schema.items():
        nfn = normalize_field_name(field_name)
        normalized_fields.append(f"{nfn}:{field_type.lower()}")
    
    # 2. ì •ë ¬ (ìˆœì„œ ë¬´ê´€)
    normalized_fields.sort()
    
    # 3. Namespace salt í¬í•¨
    content = namespace_salt + "|" + ",".join(normalized_fields)
    
    # 4. Hash
    return hashlib.sha256(content.encode()).hexdigest()

# ì˜ˆì‹œ
ts_schema = {"userId": "string", "userName": "string", "age": "number"}
py_schema = {"user_id": "str", "user_name": "str", "age": "int"}

ts_hash = compute_structural_hash(ts_schema, "frontend/types/User.ts")
py_hash = compute_structural_hash(py_schema, "backend/models/user.py")

# êµ¬ì¡°ê°€ ê°™ìœ¼ë©´ hashê°€ ìœ ì‚¬ (namespaceë§Œ ë‹¤ë¦„)
```

### 4. Edge Confidence

```python
@dataclass
class CrossLangEdge:
    """Cross-language value flow edge"""
    source_id: str
    target_id: str
    field_mappings: list[tuple[str, str]]  # [(source_field, target_field)]
    
    confidence: Literal["high", "medium", "low"]
    confidence_score: float
    reason: str

class EdgeConfidenceCalculator:
    """Edge confidence ê³„ì‚°"""
    
    def calculate(
        self,
        source_schema: dict,
        target_schema: dict,
        source_path: str,
        target_path: str
    ) -> CrossLangEdge:
        """Confidence ê³„ì‚°"""
        
        # 1. NFN matching
        nfn_match_score = self._nfn_match_score(source_schema, target_schema)
        
        # 2. Type compatibility
        type_compat_score = self._type_compat_score(source_schema, target_schema)
        
        # 3. Structural hash
        struct_hash_match = self._struct_hash_match(
            source_schema, source_path,
            target_schema, target_path
        )
        
        # 4. Overall confidence
        overall_score = (
            0.4 * nfn_match_score +
            0.3 * type_compat_score +
            0.3 * float(struct_hash_match)
        )
        
        # Confidence level
        if overall_score >= 0.8 and struct_hash_match:
            confidence = "high"
            reason = "NFN + TypeCompat + StructuralHash + Path match"
        elif overall_score >= 0.6:
            confidence = "medium"
            reason = "StructuralHash match only"
        else:
            confidence = "low"
            reason = "Name similarity only"
        
        return CrossLangEdge(
            source_id=f"{source_path}:{hash(frozenset(source_schema.items()))}",
            target_id=f"{target_path}:{hash(frozenset(target_schema.items()))}",
            field_mappings=self._compute_field_mappings(source_schema, target_schema),
            confidence=confidence,
            confidence_score=overall_score,
            reason=reason
        )
```

**ì‚¬ìš© ì •ì±…:**
- **High confidence:** LLMì´ ê·¼ê±°ë¡œ ì‚¬ìš© ê°€ëŠ¥
- **Medium confidence:** ì°¸ê³  ì •ë³´ë¡œ ì œê³µ
- **Low confidence:** ì‚¬ëŒ í™•ì¸ í•„ìš”, LLMì€ ë¬´ì‹œ

### 5. Boundary Priority

**ìš°ì„ ìˆœìœ„:**
1. **Explicit API spec** (OpenAPI / Protobuf)
2. **DB schema**
3. **ì½”ë“œ annotation**
4. **êµ¬ì¡°ì  ìœ ì‚¬ì„±**

```python
class BoundaryParser:
    """API Boundary íŒŒì‹±"""
    
    def parse_openapi(self, spec_file: str) -> list[BoundaryNode]:
        """OpenAPI â†’ Boundary nodes (ìµœê³  ìš°ì„ ìˆœìœ„)"""
        with open(spec_file) as f:
            spec = yaml.safe_load(f)
        
        boundaries = []
        for path, methods in spec.get("paths", {}).items():
            for method, details in methods.items():
                # Request body schema
                if "requestBody" in details:
                    schema = self._extract_schema(details["requestBody"])
                    boundaries.append(BoundaryNode(
                        id=f"openapi:{method}:{path}:request",
                        name=f"{method.upper()} {path} (request)",
                        schema=schema,
                        source="openapi",
                        priority=1  # ìµœê³  ìš°ì„ ìˆœìœ„
                    ))
                
                # Response schema
                if "responses" in details:
                    for status, response in details["responses"].items():
                        schema = self._extract_schema(response)
                        boundaries.append(BoundaryNode(
                            id=f"openapi:{method}:{path}:response:{status}",
                            name=f"{method.upper()} {path} (response {status})",
                            schema=schema,
                            source="openapi",
                            priority=1
                        ))
        
        return boundaries
    
    def parse_db_schema(self, db_url: str) -> list[BoundaryNode]:
        """DB schema â†’ Boundary nodes (2ìˆœìœ„)"""
        # SQLAlchemy reflection ë“±ì„ ì‚¬ìš©
        pass
```

### 6. Schema Evolution Tracking

```python
@dataclass
class SchemaVersion:
    """Schema ë²„ì „"""
    version: str
    schema: dict
    deprecated: bool
    breaking_changes: list[str]

class SchemaEvolutionTracker:
    """Schema ë³€ê²½ ì¶”ì """
    
    def detect_breaking_change(
        self,
        old_schema: dict,
        new_schema: dict
    ) -> list[str]:
        """Breaking change ê°ì§€"""
        breaking_changes = []
        
        # í•„ë“œ ì œê±°
        for field in old_schema:
            if field not in new_schema:
                breaking_changes.append(f"Field removed: {field}")
        
        # íƒ€ì… ë³€ê²½ (í˜¸í™˜ ë¶ˆê°€)
        for field in old_schema.keys() & new_schema.keys():
            old_type = old_schema[field]
            new_type = new_schema[field]
            
            compatible, confidence = are_types_compatible(old_type, new_type)
            if not compatible:
                breaking_changes.append(
                    f"Incompatible type change: {field} ({old_type} â†’ {new_type})"
                )
        
        return breaking_changes
```

---

## RFC-06-STORAGE: Storage Consistency & Crash Recovery

### 1. Goal

Semantica v6ì˜ IR/Graph/Index ì €ì¥ ì‹œ  
**ì›ìì„±(Atomicity)**, **ì¼ê´€ì„±(Consistency)**, **í¬ë˜ì‹œ ë³µêµ¬(Recovery)**ë¥¼ ë³´ì¥

### 2. Core Mechanisms

#### 2.1 Write-Ahead Log (WAL)

```python
@dataclass
class WALEntry:
    """WAL í•­ëª©"""
    entry_id: str
    timestamp: float
    operation: Literal["create", "update", "delete"]
    object_type: Literal["snapshot", "ir", "graph", "index"]
    object_id: str
    data: Optional[bytes]  # Compressed

class WriteAheadLog:
    """Write-Ahead Log"""
    
    def __init__(self, wal_path: str):
        self.wal_path = Path(wal_path)
        self.wal_path.mkdir(parents=True, exist_ok=True)
        self.current_log = self.wal_path / f"wal_{int(time.time())}.log"
    
    def append(self, entry: WALEntry):
        """WALì— ê¸°ë¡"""
        with open(self.current_log, "ab") as f:
            # Entry ì§ë ¬í™”
            serialized = self._serialize_entry(entry)
            
            # Checksum ì¶”ê°€
            checksum = hashlib.sha256(serialized).digest()
            
            # ê¸°ë¡
            f.write(len(serialized).to_bytes(4, 'big'))
            f.write(serialized)
            f.write(checksum)
            f.flush()
            os.fsync(f.fileno())  # Diskì— ê°•ì œ ì“°ê¸°
    
    def replay(self) -> list[WALEntry]:
        """WAL replay (crash recovery)"""
        entries = []
        
        for log_file in sorted(self.wal_path.glob("wal_*.log")):
            with open(log_file, "rb") as f:
                while True:
                    # Length ì½ê¸°
                    length_bytes = f.read(4)
                    if not length_bytes:
                        break
                    
                    length = int.from_bytes(length_bytes, 'big')
                    
                    # Entry ì½ê¸°
                    serialized = f.read(length)
                    expected_checksum = f.read(32)
                    
                    # Checksum ê²€ì¦
                    actual_checksum = hashlib.sha256(serialized).digest()
                    if actual_checksum != expected_checksum:
                        # Corrupted entry, stop replay
                        break
                    
                    entry = self._deserialize_entry(serialized)
                    entries.append(entry)
        
        return entries
```

#### 2.2 Atomic Update

```python
class AtomicFileWriter:
    """ì›ìì  íŒŒì¼ ì—…ë°ì´íŠ¸"""
    
    def write_atomic(
        self,
        target_path: Path,
        data: bytes
    ):
        """Atomic write (temp â†’ rename)"""
        
        # 1. Temp file ìƒì„±
        temp_path = target_path.with_suffix(".tmp")
        
        # 2. Data ì“°ê¸°
        with open(temp_path, "wb") as f:
            f.write(data)
            f.flush()
            os.fsync(f.fileno())
        
        # 3. Checksum ê¸°ë¡
        checksum = hashlib.sha256(data).hexdigest()
        checksum_path = temp_path.with_suffix(".checksum")
        with open(checksum_path, "w") as f:
            f.write(checksum)
            f.flush()
            os.fsync(f.fileno())
        
        # 4. Atomic rename (OS-level atomicity)
        os.rename(temp_path, target_path)
        os.rename(checksum_path, target_path.with_suffix(".checksum"))
    
    def verify_integrity(self, file_path: Path) -> bool:
        """Checksum ê²€ì¦"""
        checksum_path = file_path.with_suffix(".checksum")
        
        if not checksum_path.exists():
            return False
        
        with open(checksum_path) as f:
            expected_checksum = f.read().strip()
        
        with open(file_path, "rb") as f:
            actual_checksum = hashlib.sha256(f.read()).hexdigest()
        
        return expected_checksum == actual_checksum
```

#### 2.3 Versioned Snapshot

```python
@dataclass
class Snapshot:
    """Immutable snapshot"""
    snapshot_id: str
    repo_id: str
    timestamp: float
    parent_snapshot_id: Optional[str]
    
    ir_path: Path
    graph_path: Path
    index_path: Path
    
    is_full: bool  # Full vs Delta
    pinned: bool
    tags: list[str]

class SnapshotStore:
    """Snapshot ê´€ë¦¬"""
    
    def create_snapshot(
        self,
        repo_id: str,
        ir_data: bytes,
        graph_data: bytes,
        parent_snapshot_id: Optional[str] = None
    ) -> Snapshot:
        """ìƒˆ snapshot ìƒì„±"""
        
        snapshot_id = f"snap_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        
        # Paths
        snapshot_dir = self.base_path / repo_id / snapshot_id
        snapshot_dir.mkdir(parents=True, exist_ok=True)
        
        ir_path = snapshot_dir / "ir.bin.zst"
        graph_path = snapshot_dir / "graph.bin.zst"
        
        # 1. WAL ê¸°ë¡
        self.wal.append(WALEntry(
            entry_id=snapshot_id,
            timestamp=time.time(),
            operation="create",
            object_type="snapshot",
            object_id=snapshot_id,
            data=None
        ))
        
        # 2. Atomic write
        self.writer.write_atomic(ir_path, ir_data)
        self.writer.write_atomic(graph_path, graph_data)
        
        # 3. Metadata ê¸°ë¡
        snapshot = Snapshot(
            snapshot_id=snapshot_id,
            repo_id=repo_id,
            timestamp=time.time(),
            parent_snapshot_id=parent_snapshot_id,
            ir_path=ir_path,
            graph_path=graph_path,
            index_path=snapshot_dir / "index",
            is_full=parent_snapshot_id is None,
            pinned=False,
            tags=[]
        )
        
        self._save_metadata(snapshot)
        
        return snapshot
    
    def get_latest_valid_snapshot(self, repo_id: str) -> Optional[Snapshot]:
        """ê°€ì¥ ìµœê·¼ì˜ ìœ íš¨í•œ snapshot"""
        snapshots = self._list_snapshots(repo_id)
        
        for snapshot in reversed(snapshots):
            # Integrity ê²€ì¦
            if self._verify_snapshot_integrity(snapshot):
                return snapshot
        
        return None
```

### 3. Snapshot Retention Policy

```python
class SnapshotGC:
    """Snapshot Garbage Collector"""
    
    def __init__(
        self,
        keep_last_n: int = 20,
        keep_days: int = 30
    ):
        self.keep_last_n = keep_last_n
        self.keep_days = keep_days
    
    def collect(self, repo_id: str) -> list[str]:
        """GC ì‹¤í–‰"""
        snapshots = self._list_snapshots(repo_id)
        
        # 1. Pinned snapshotì€ ì œì™¸
        unpinned = [s for s in snapshots if not s.pinned]
        
        # 2. ìµœê·¼ Nê°œ ìœ ì§€
        to_keep = set(s.snapshot_id for s in unpinned[-self.keep_last_n:])
        
        # 3. keep_days ì´ë‚´ ìœ ì§€
        cutoff_time = time.time() - (self.keep_days * 86400)
        for snapshot in unpinned:
            if snapshot.timestamp >= cutoff_time:
                to_keep.add(snapshot.snapshot_id)
        
        # 4. ì‚­ì œ ëŒ€ìƒ
        to_delete = [
            s.snapshot_id for s in unpinned
            if s.snapshot_id not in to_keep
        ]
        
        # 5. Cascade ì‚­ì œ
        for snapshot_id in to_delete:
            self._delete_snapshot_cascade(snapshot_id)
        
        return to_delete
    
    def _delete_snapshot_cascade(self, snapshot_id: str):
        """Cascade ì‚­ì œ"""
        snapshot = self._get_snapshot(snapshot_id)
        
        # IR, Graph, Index ì‚­ì œ
        shutil.rmtree(snapshot.ir_path.parent)
```

### 4. Crash Recovery

```python
class CrashRecoveryManager:
    """Crash recovery"""
    
    def recover(self, repo_id: str) -> bool:
        """Crash í›„ ë³µêµ¬"""
        
        # 1. WAL replay
        entries = self.wal.replay()
        
        # 2. ë§ˆì§€ë§‰ ì„±ê³µí•œ snapshot ì°¾ê¸°
        last_valid = self.snapshot_store.get_latest_valid_snapshot(repo_id)
        
        if not last_valid:
            # ë³µêµ¬ ë¶ˆê°€
            return False
        
        # 3. Temp íŒŒì¼ ì •ë¦¬
        self._cleanup_temp_files(repo_id)
        
        # 4. Incomplete snapshot ì‚­ì œ
        self._delete_incomplete_snapshots(repo_id, last_valid.snapshot_id)
        
        # 5. WAL ì •ë¦¬
        self.wal.truncate_before(last_valid.timestamp)
        
        return True
    
    def _cleanup_temp_files(self, repo_id: str):
        """Temp íŒŒì¼ ì œê±°"""
        repo_dir = self.base_path / repo_id
        for temp_file in repo_dir.rglob("*.tmp"):
            temp_file.unlink()
```

### 5. Speculative Isolation

```python
class SpeculativeStorage:
    """Speculative session storage"""
    
    def __init__(self, base_store: SnapshotStore):
        self.base_store = base_store
        self.overlay_cache: dict[str, DeltaLayer] = {}
    
    def create_overlay(
        self,
        base_snapshot_id: str,
        patch_id: str
    ) -> DeltaLayer:
        """Overlay ìƒì„± (baseëŠ” ë³€ê²½ ì•ˆí•¨)"""
        
        delta = DeltaLayer(patch_id=patch_id)
        self.overlay_cache[patch_id] = delta
        
        # Base snapshotì€ ì ˆëŒ€ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ
        # OverlayëŠ” ë©”ëª¨ë¦¬ì—ë§Œ ì¡´ì¬
        
        return delta
    
    def commit_overlay(
        self,
        base_snapshot_id: str,
        patch_id: str
    ) -> str:
        """Overlayë¥¼ ìƒˆ snapshotìœ¼ë¡œ ìŠ¹ê²©"""
        
        delta = self.overlay_cache[patch_id]
        base = self.base_store.get_snapshot(base_snapshot_id)
        
        # 1. Base + delta ë³‘í•©
        merged_ir = self._merge_ir(base, delta)
        merged_graph = self._merge_graph(base, delta)
        
        # 2. ìƒˆ snapshot ìƒì„±
        new_snapshot = self.base_store.create_snapshot(
            repo_id=base.repo_id,
            ir_data=merged_ir,
            graph_data=merged_graph,
            parent_snapshot_id=base_snapshot_id
        )
        
        # 3. Overlay ì œê±°
        del self.overlay_cache[patch_id]
        
        return new_snapshot.snapshot_id
```

### 6. Incremental Compaction

```python
class SnapshotCompactor:
    """Delta snapshot compaction"""
    
    def should_compact(self, repo_id: str) -> bool:
        """Compaction í•„ìš” ì—¬ë¶€"""
        snapshots = self._list_snapshots(repo_id)
        
        # Deltaê°€ 10ê°œ ì´ìƒ ëˆ„ì 
        delta_count = sum(1 for s in snapshots if not s.is_full)
        
        return delta_count >= 10
    
    def compact(self, repo_id: str) -> str:
        """Compaction ì‹¤í–‰"""
        snapshots = self._list_snapshots(repo_id)
        
        # 1. ê°€ì¥ ìµœê·¼ full snapshot ì°¾ê¸°
        full_snapshots = [s for s in snapshots if s.is_full]
        if not full_snapshots:
            return None
        
        base = full_snapshots[-1]
        
        # 2. ì´í›„ì˜ deltaë“¤
        deltas = [
            s for s in snapshots
            if s.timestamp > base.timestamp and not s.is_full
        ]
        
        if len(deltas) < 10:
            return None
        
        # 3. Base + deltas ë³‘í•©
        merged_state = self._apply_deltas(base, deltas)
        
        # 4. ìƒˆ full snapshot ìƒì„±
        new_snapshot = self.snapshot_store.create_snapshot(
            repo_id=repo_id,
            ir_data=merged_state.ir,
            graph_data=merged_state.graph,
            parent_snapshot_id=None  # Full snapshot
        )
        new_snapshot.is_full = True
        
        # 5. ê¸°ì¡´ delta ì‚­ì œ
        for delta in deltas:
            self.snapshot_store.delete_snapshot(delta.snapshot_id)
        
        return new_snapshot.snapshot_id
```

---

## RFC-06-OBS: Observability & Debug UI

### 1. Goal

Semantica ì—”ì§„ì˜ ëª¨ë“  ë™ì‘ì„  
**ì‹¤ì‹œê°„ìœ¼ë¡œ ê´€ì°° ê°€ëŠ¥(observable)**í•˜ë„ë¡ ë§Œë“¤ê¸°

### 2. Metrics

#### 2.1 IR/Graph Build Metrics

```python
from opentelemetry import metrics

meter = metrics.get_meter(__name__)

# Latency
parse_duration = meter.create_histogram(
    name="semantica.parse.duration",
    unit="ms",
    description="Parsing duration"
)

ir_build_duration = meter.create_histogram(
    name="semantica.ir.build.duration",
    unit="ms",
    description="IR generation duration"
)

graph_build_duration = meter.create_histogram(
    name="semantica.graph.build.duration",
    unit="ms",
    description="Graph building duration"
)

# Hit rate
incremental_hit_rate = meter.create_gauge(
    name="semantica.incremental.hit_rate",
    description="Incremental rebuild cache hit rate"
)

# Scope
rebuild_scope = meter.create_histogram(
    name="semantica.rebuild.scope",
    unit="nodes",
    description="Number of nodes rebuilt"
)
```

#### 2.2 Speculative Execution Metrics

```python
# Memory
speculative_memory = meter.create_gauge(
    name="semantica.speculative.memory.bytes",
    unit="bytes",
    description="Speculative execution memory usage"
)

# Overlay depth
overlay_depth = meter.create_gauge(
    name="semantica.speculative.overlay.depth",
    description="Number of active overlays"
)

# Rollback cost
rollback_cost = meter.create_histogram(
    name="semantica.speculative.rollback.cost",
    unit="ms",
    description="Rollback operation cost"
)
```

#### 2.3 Slicing Metrics

```python
# Token usage
slice_tokens = meter.create_histogram(
    name="semantica.slice.tokens",
    description="Slice token usage"
)

# Pruning ratio
slice_pruning_ratio = meter.create_gauge(
    name="semantica.slice.pruning.ratio",
    description="Pruned nodes / total nodes"
)

# Depth
slice_depth = meter.create_histogram(
    name="semantica.slice.depth",
    description="Slice depth (PDG hops)"
)
```

### 3. Dashboards

#### 3.1 Graph Explorer UI

```typescript
// Graph Explorer Component
interface GraphExplorerProps {
  repoId: string;
  snapshotId: string;
}

const GraphExplorer: React.FC<GraphExplorerProps> = ({
  repoId,
  snapshotId
}) => {
  const [view, setView] = useState<'call' | 'pdg' | 'slice' | 'diff'>('call');
  const [selectedNode, setSelectedNode] = useState<string | null>(null);
  
  return (
    <div className="graph-explorer">
      <Toolbar>
        <Button onClick={() => setView('call')}>Call Graph</Button>
        <Button onClick={() => setView('pdg')}>PDG</Button>
        <Button onClick={() => setView('slice')}>Slice</Button>
        <Button onClick={() => setView('diff')}>Semantic Diff</Button>
      </Toolbar>
      
      <GraphCanvas
        view={view}
        repoId={repoId}
        snapshotId={snapshotId}
        selectedNode={selectedNode}
        onNodeClick={setSelectedNode}
      />
      
      {selectedNode && (
        <NodeInspector nodeId={selectedNode} />
      )}
    </div>
  );
};
```

#### 3.2 Performance Dashboard

```python
# Grafana Dashboard (JSON)
dashboard = {
    "title": "Semantica v6 Performance",
    "panels": [
        {
            "title": "Build Latency Histogram",
            "targets": [
                {
                    "expr": "histogram_quantile(0.95, semantica_ir_build_duration_bucket)"
                }
            ]
        },
        {
            "title": "Memory Timeline",
            "targets": [
                {
                    "expr": "semantica_speculative_memory_bytes"
                }
            ]
        },
        {
            "title": "Cache Hit Ratio",
            "targets": [
                {
                    "expr": "rate(semantica_cache_hits_total[5m]) / rate(semantica_cache_requests_total[5m])"
                }
            ]
        },
    ]
}
```

### 4. Distributed Tracing

#### 4.1 Trace Structure

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.jaeger import JaegerExporter

# Setup
tracer_provider = TracerProvider()
jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831
)
tracer_provider.add_span_processor(BatchSpanProcessor(jaeger_exporter))
trace.set_tracer_provider(tracer_provider)

tracer = trace.get_tracer(__name__)

# Usage
with tracer.start_as_current_span(
    "build_full",
    attributes={
        "repo_id": repo_id,
        "snapshot_id": snapshot_id,
        "language": "python"
    }
) as span:
    # Parse
    with tracer.start_as_current_span("parse") as parse_span:
        ast = parser.parse(source)
        parse_span.set_attribute("file_count", len(files))
    
    # IR Build
    with tracer.start_as_current_span("ir_build") as ir_span:
        ir_doc = ir_generator.generate(ast)
        ir_span.set_attribute("node_count", len(ir_doc.nodes))
    
    # Graph Build
    with tracer.start_as_current_span("graph_build") as graph_span:
        graph = graph_builder.build(ir_doc)
        graph_span.set_attribute("edge_count", len(graph.edges))
```

### 5. Alert Rules

```python
@dataclass
class AlertRule:
    """ì•Œë¦¼ ê·œì¹™"""
    name: str
    condition: str  # Python expression
    severity: Literal["info", "warning", "error", "critical"]
    action: str
    cooldown: int = 300  # 5ë¶„

class AlertManager:
    """ì•Œë¦¼ ê´€ë¦¬"""
    
    def __init__(self):
        self.rules: list[AlertRule] = []
        self.last_fired: dict[str, float] = {}
        self._load_rules()
    
    def _load_rules(self):
        """ê·œì¹™ ë¡œë“œ"""
        self.rules = [
            AlertRule(
                name="speculative_memory_high",
                condition="speculative_mem_usage > 2 * base_mem_usage",
                severity="warning",
                action="evict_oldest_overlay"
            ),
            AlertRule(
                name="slice_budget_exceeded",
                condition="slice_token_usage > budget * 1.2",
                severity="error",
                action="abort_slice"
            ),
            AlertRule(
                name="incremental_hit_rate_low",
                condition="incremental_hit_rate < 0.5",
                severity="info",
                action="log_warning"
            ),
        ]
    
    def check_alerts(self, metrics: dict):
        """ì•Œë¦¼ ì²´í¬"""
        for rule in self.rules:
            # Cooldown ì²´í¬
            if rule.name in self.last_fired:
                if time.time() - self.last_fired[rule.name] < rule.cooldown:
                    continue
            
            # Condition í‰ê°€
            try:
                if eval(rule.condition, {}, metrics):
                    self._fire_alert(rule, metrics)
                    self.last_fired[rule.name] = time.time()
            except Exception as e:
                logger.error(f"Failed to evaluate alert rule {rule.name}: {e}")
    
    def _fire_alert(self, rule: AlertRule, metrics: dict):
        """ì•Œë¦¼ ë°œìƒ"""
        logger.log(
            self._severity_to_level(rule.severity),
            f"ALERT: {rule.name} - {rule.condition}",
            extra={"metrics": metrics}
        )
        
        # Action ì‹¤í–‰
        if rule.action == "evict_oldest_overlay":
            self._evict_oldest_overlay()
        elif rule.action == "abort_slice":
            self._abort_current_slice()
```

### 6. Anomaly Detection

```python
import statistics
from collections import deque

class AnomalyDetector:
    """í†µê³„ì  ì´ìƒ ê°ì§€"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.history: dict[str, deque] = {}
    
    def record(self, metric_name: str, value: float):
        """ë©”íŠ¸ë¦­ ê¸°ë¡"""
        if metric_name not in self.history:
            self.history[metric_name] = deque(maxlen=self.window_size)
        
        self.history[metric_name].append(value)
    
    def detect(self, metric_name: str, current_value: float) -> Optional[dict]:
        """ì´ìƒ ê°ì§€"""
        if metric_name not in self.history:
            return None
        
        history = list(self.history[metric_name])
        
        if len(history) < 30:
            return None  # ì¶©ë¶„í•œ ë°ì´í„° í•„ìš”
        
        mean = statistics.mean(history)
        stdev = statistics.stdev(history)
        
        if stdev == 0:
            return None
        
        z_score = (current_value - mean) / stdev
        
        if abs(z_score) > 3:  # 3-sigma
            return {
                "metric": metric_name,
                "value": current_value,
                "mean": mean,
                "stdev": stdev,
                "z_score": z_score,
                "severity": "high" if abs(z_score) > 4 else "medium",
                "expected_range": (mean - 3*stdev, mean + 3*stdev)
            }
        
        return None

# Usage
detector = AnomalyDetector()

# ì •ìƒ ë™ì‘
for i in range(100):
    detector.record("parse_time", 20 + random.gauss(0, 2))

# ì´ìƒ ê°ì§€
anomaly = detector.detect("parse_time", 200)
if anomaly:
    print(f"Anomaly detected: {anomaly}")
```

---

## ì¢…í•© í‰ê°€

### âœ… ê°•ì 

1. **RFC-06-EFFECT:** ì‹¤ìš©ì ì´ê³  êµ¬í˜„ ê°€ëŠ¥. IdempotencyëŠ” í•µì‹¬ ì°¨ë³„í™”.
2. **RFC-06-VFLOW:** Edge Confidenceê°€ í•µì‹¬. Low confidenceëŠ” ë¬´ì‹œ ì •ì±…ì´ í˜„ëª….
3. **RFC-06-STORAGE:** WAL + Atomic Update + Snapshot GCëŠ” ì—…ê³„ í‘œì¤€.
4. **RFC-06-OBS:** Metrics + Tracing + Alerting + Anomaly Detection = ì™„ë²½.

### ğŸ’¡ ê°œì„  ì‚¬í•­ (ë³¸ ë¬¸ì„œì— ë°˜ì˜ë¨)

1. **Effect System:**
   - Effect Hierarchy ì¶”ê°€
   - Confidence Score ì¶”ê°€
   - Pattern Database ì¶”ê°€

2. **VFLOW:**
   - Schema Evolution Tracking
   - Example-based Mapping Hint

3. **Storage:**
   - Incremental Compaction

4. **Observability:**
   - Alert Rules
   - Anomaly Detection

---

## êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1 (í•„ìˆ˜):
- âœ… RFC-06-EFFECT (Effect System)
- âœ… RFC-06-STORAGE (Storage Layer)
- âœ… RFC-06-OBS (Basic Metrics)

### Phase 2:
- âœ… RFC-06-OBS (Tracing + Dashboards)

### Phase 3 (Optional):
- âš ï¸ RFC-06-VFLOW (MSA ê³ ê° í™•ë³´ í›„)

---

**End of Sub-RFCs**

