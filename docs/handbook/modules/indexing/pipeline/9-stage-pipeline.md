# 9ë‹¨ê³„ ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ ìƒì„¸

> ê° Stageì˜ ì—­í• , ì…ì¶œë ¥, ì„±ëŠ¥ íŠ¹ì„±

---

## ëª©ì°¨

1. [íŒŒì´í”„ë¼ì¸ ê°œìš”](#1-íŒŒì´í”„ë¼ì¸-ê°œìš”)
2. [Stage 1: GitStage](#stage-1-gitstage)
3. [Stage 2: DiscoveryStage](#stage-2-discoverystage)
4. [Stage 3: ParsingStage](#stage-3-parsingstage)
5. [Stage 4: IRStage](#stage-4-irstage)
6. [Stage 5: SemanticIRStage](#stage-5-semanticirstage)
7. [Stage 6: GraphStage](#stage-6-graphstage)
8. [Stage 7: ChunkStage](#stage-7-chunkstage)
9. [Stage 8: RepoMapStage](#stage-8-repomapstage)
10. [Stage 9: IndexingStage](#stage-9-indexingstage)
11. [í˜‘ë ¥ì  ì·¨ì†Œ (Graceful Stop)](#í˜‘ë ¥ì -ì·¨ì†Œ)

---

## 1. íŒŒì´í”„ë¼ì¸ ê°œìš”

### ì „ì²´ í”Œë¡œìš°

```
Git â†’ Discovery â†’ Parsing â†’ IR â†’ Semantic IR â†’ Graph â†’ Chunk â†’ RepoMap â†’ Indexing
 â†“        â†“         â†“        â†“       â†“          â†“       â†“        â†“         â†“
ë©”íƒ€    íŒŒì¼ ëª©ë¡    AST    êµ¬ì¡° IR  ì˜ë¯¸ IR    ê·¸ë˜í”„  ì²­í¬   RepoMap   ë‹¤ì¤‘ ì¸ë±ìŠ¤
```

### ë ˆì´ì–´ ë§¤í•‘

| Stage | ë ˆì´ì–´ | ì„¤ëª… |
|-------|--------|------|
| GitStage | L0 | ë³€ê²½ ê°ì§€ |
| DiscoveryStage | L0 | íŒŒì¼ íƒìƒ‰ |
| ParsingStage | L1 | AST íŒŒì‹± |
| IRStage | L2 | êµ¬ì¡° IR |
| SemanticIRStage | L3 | ì˜ë¯¸ IR |
| GraphStage | L3 | ê·¸ë˜í”„ ë¹Œë”© |
| ChunkStage | L2 | ì²­í¬ ìƒì„± |
| RepoMapStage | - | RepoMap |
| IndexingStage | - | ì €ì¥ |

---

## Stage 1: GitStage

### ì—­í• 
Git ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘ (commit, branch, author)

### ì…ë ¥
- `repo_path: Path`
- `snapshot_id: str`

### ì¶œë ¥
```python
@dataclass
class GitMetadata:
    commit_hash: str
    branch: str
    author: str
    commit_date: datetime
    remote_url: str | None
```

### êµ¬í˜„
```python
# src/contexts/analysis_indexing/infrastructure/stages/git_stage.py
class GitStage(IndexingStage):
    async def execute(self, ctx: StageContext) -> StageResult:
        git_helper = GitHelper(ctx.repo_path)

        metadata = GitMetadata(
            commit_hash=git_helper.get_head_commit(),
            branch=git_helper.get_current_branch(),
            author=git_helper.get_last_author(),
            commit_date=git_helper.get_commit_date(),
            remote_url=git_helper.get_remote_url(),
        )

        return StageResult(success=True, data=metadata)
```

### ì„±ëŠ¥
- **ì‹œê°„:** <
- **ë©”ëª¨ë¦¬:** <1MB
- **ì˜ì¡´ì„±:** git CLI

### ì‹¤íŒ¨ ì¼€ì´ìŠ¤
- Git repo ì•„ë‹˜ â†’ WARNING, ê³„ì† ì§„í–‰
- Detached HEAD â†’ branch="HEAD"
- No remote â†’ remote_url=None

---

## Stage 2: DiscoveryStage

### ì—­í• 
ì†ŒìŠ¤ íŒŒì¼ íƒìƒ‰ (extensions ê¸°ë°˜)

### ì…ë ¥
- `repo_path: Path`
- `exclude_patterns: list[str]`
- `supported_extensions: list[str]`

### ì¶œë ¥
```python
@dataclass
class DiscoveryResult:
    files: list[Path]           # ë°œê²¬ëœ íŒŒì¼ë“¤
    total_size: int             # ì´ í¬ê¸° (bytes)
    by_language: dict[str, int] # ì–¸ì–´ë³„ ê°œìˆ˜
```

### êµ¬í˜„
```python
# src/contexts/analysis_indexing/infrastructure/stages/discovery_stage.py
class DiscoveryStage(IndexingStage):
    async def execute(self, ctx: StageContext) -> StageResult:
        files = []

        for ext in SUPPORTED_EXTENSIONS:
            pattern = f"**/*{ext}"
            found = repo_path.glob(pattern)
            files.extend([f for f in found if not self._is_excluded(f)])

        # ì–¸ì–´ ë¶„ë¥˜
        by_language = defaultdict(int)
        for file in files:
            lang = self._detect_language(file.suffix)
            by_language[lang] += 1

        return StageResult(
            success=True,
            data=DiscoveryResult(
                files=files,
                total_size=sum(f.stat().st_size for f in files),
                by_language=dict(by_language),
            )
        )
```

### ì„±ëŠ¥
- **ì‹œê°„:** ~1ì´ˆ (10K íŒŒì¼)
- **ë©”ëª¨ë¦¬:** ~10MB
- **ë³‘ë ¬í™”:** ê°€ëŠ¥ (ì–¸ì–´ë³„)

### ì œì™¸ íŒ¨í„´ (ê¸°ë³¸ê°’)
```python
DEFAULT_EXCLUDE = [
    ".git", "node_modules", "__pycache__", ".venv",
    "*.pyc", "*.log", ".DS_Store"
]
```

---

## Stage 3: ParsingStage

### ì—­í• 
Tree-sitter AST íŒŒì‹±

### ì…ë ¥
- `files: list[Path]`
- `languages: dict[str, Language]`

### ì¶œë ¥
```python
@dataclass
class ParseResult:
    file_path: str
    language: str
    ast_root: Node          # Tree-sitter Node
    parse_time_ms: float
    success: bool
    error: str | None
```

### êµ¬í˜„
```python
# src/contexts/analysis_indexing/infrastructure/stages/parsing_stage.py
class ParsingStage(IndexingStage):
    async def execute(self, ctx: StageContext) -> StageResult:
        results = []

        for file_path in ctx.files:
            lang = detect_language(file_path)
            parser = self._get_parser(lang)

            with open(file_path, 'rb') as f:
                content = f.read()

            tree = parser.parse(content)

            results.append(ParseResult(
                file_path=str(file_path),
                language=lang,
                ast_root=tree.root_node,
                success=True,
            ))

        return StageResult(success=True, data=results)
```

### ì„±ëŠ¥
- **ì‹œê°„:** ~/íŒŒì¼ (í‰ê· )
- **ë©”ëª¨ë¦¬:** ~2MB/íŒŒì¼ (AST)
- **ë³‘ë ¬í™”:** í•„ìˆ˜ (ì–¸ì–´ë³„ íŒŒì„œ ì¬ì‚¬ìš©)

### ì§€ì› ì–¸ì–´
```python
SUPPORTED_LANGUAGES = [
    "python", "typescript", "javascript", "java",
    "go", "rust", "c", "cpp", "kotlin"
]
```

### ì‹¤íŒ¨ ì²˜ë¦¬
```python
# Syntax error â†’ ParseResult(success=False, error=str(e))
# íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ â†’ Skip, WARNING ë¡œê·¸
```

---

## Stage 4: IRStage

### ì—­í• 
êµ¬ì¡° IR ìƒì„± (L2)

### ì…ë ¥
- `parse_results: list[ParseResult]`

### ì¶œë ¥
```python
@dataclass
class IRDocument:
    file_path: str
    language: str
    imports: list[Import]
    symbols: list[Symbol]     # classes, functions, variables
    occurrences: list[Occurrence]
    diagnostics: list[Diagnostic]
```

### êµ¬í˜„
```python
# src/contexts/analysis_indexing/infrastructure/stages/ir_stage.py
class IRStage(IndexingStage):
    async def execute(self, ctx: StageContext) -> StageResult:
        ir_builder = LayeredIRBuilder()
        results = []

        for parse_result in ctx.parse_results:
            ir_doc = await ir_builder.build(
                file_path=parse_result.file_path,
                ast_root=parse_result.ast_root,
                language=parse_result.language,
            )
            results.append(ir_doc)

        return StageResult(success=True, data=results)
```

### ì„±ëŠ¥
- **ì‹œê°„:** ~/íŒŒì¼
- **ë©”ëª¨ë¦¬:** ~5MB/íŒŒì¼
- **ë³‘ë ¬í™”:** ì–¸ì–´ë³„

### ìƒì„± ì •ë³´
- **Imports:** ëª¨ë“ˆ, í´ë˜ìŠ¤, í•¨ìˆ˜ import
- **Symbols:** FQN, kind (class/func/var), location
- **Occurrences:** Symbol usage ì¶”ì 
- **Diagnostics:** íƒ€ì… ì—ëŸ¬, unused imports

---

## Stage 5: SemanticIRStage

### ì—­í• 
ì˜ë¯¸ IR ìƒì„± (L3 - CFG/DFG)

### ì…ë ¥
- `ir_documents: list[IRDocument]`

### ì¶œë ¥
```python
@dataclass
class SemanticIR:
    cfg: ControlFlowGraph
    dfg: DataFlowGraph
    type_info: TypeInfo
    signatures: dict[str, Signature]
```

### êµ¬í˜„
```python
# src/contexts/analysis_indexing/infrastructure/stages/semantic_ir_stage.py
class SemanticIRStage(IndexingStage):
    async def execute(self, ctx: StageContext) -> StageResult:
        semantic_builder = SemanticIRBuilder()
        results = []

        for ir_doc in ctx.ir_documents:
            sem_ir = await semantic_builder.build(ir_doc)
            results.append(sem_ir)

        return StageResult(success=True, data=results)
```

### ì„±ëŠ¥
- **ì‹œê°„:** ~/íŒŒì¼ (L3), ~ (L4)
- **ë©”ëª¨ë¦¬:** ~10MB/íŒŒì¼
- **ì œí•œ:** BALANCEDëŠ” CFG ë…¸ë“œ 100ê°œê¹Œì§€

### L3 vs L4

| í•­ëª© | L3 (BALANCED) | L4 (DEEP) |
|------|--------------|-----------|
| CFG | 100 ë…¸ë“œ | Unlimited |
| DFG | Single function | Cross-function |
| Git History | 10 commits | All |

---

## Stage 6: GraphStage

### ì—­í• 
ì½”ë“œ ê·¸ë˜í”„ ë¹Œë”© (Node, Edge)

### ì…ë ¥
- `ir_documents: list[IRDocument]`
- `semantic_irs: list[SemanticIR]`

### ì¶œë ¥
```python
@dataclass
class CodeGraph:
    nodes: list[GraphNode]    # Files, Symbols
    edges: list[GraphEdge]    # IMPORTS, CALLS, INHERITS
```

### êµ¬í˜„
```python
# src/contexts/analysis_indexing/infrastructure/stages/graph_stage.py
class GraphStage(IndexingStage):
    async def execute(self, ctx: StageContext) -> StageResult:
        graph_builder = GraphBuilder()

        # 1. Add nodes
        for ir_doc in ctx.ir_documents:
            graph_builder.add_file_node(ir_doc.file_path)
            for symbol in ir_doc.symbols:
                graph_builder.add_symbol_node(symbol)

        # 2. Add edges
        for ir_doc in ctx.ir_documents:
            for imp in ir_doc.imports:
                graph_builder.add_import_edge(
                    from_file=ir_doc.file_path,
                    to_module=imp.module,
                )

        graph = graph_builder.build()
        return StageResult(success=True, data=graph)
```

### ì„±ëŠ¥
- **ì‹œê°„:** ~ (1000 íŒŒì¼)
- **ë©”ëª¨ë¦¬:** ~50MB
- **ë³‘ë ¬í™”:** ë¶ˆê°€ (global state)

### Edge íƒ€ì…
```python
class EdgeType(Enum):
    IMPORTS = "imports"
    CALLS = "calls"
    INHERITS = "inherits"
    DEFINES = "defines"
    REFERENCES = "references"
```

---

## Stage 7: ChunkStage

### ì—­í• 
LLM-friendly ì²­í¬ ìƒì„±

### ì…ë ¥
- `ir_documents: list[IRDocument]`
- `code_graph: CodeGraph`

### ì¶œë ¥
```python
@dataclass
class Chunk:
    id: str
    level: ChunkLevel    # REPO/MODULE/FILE/CLASS/FUNCTION
    content: str
    metadata: ChunkMetadata
    embeddings: list[float] | None
```

### êµ¬í˜„
```python
# src/contexts/analysis_indexing/infrastructure/stages/chunk_stage.py
class ChunkStage(IndexingStage):
    async def execute(self, ctx: StageContext) -> StageResult:
        chunk_builder = ChunkBuilder()
        chunks = []

        for ir_doc in ctx.ir_documents:
            # File-level chunk
            file_chunk = chunk_builder.create_file_chunk(ir_doc)
            chunks.append(file_chunk)

            # Symbol-level chunks
            for symbol in ir_doc.symbols:
                if symbol.kind in (SymbolKind.CLASS, SymbolKind.FUNCTION):
                    chunk = chunk_builder.create_symbol_chunk(symbol)
                    chunks.append(chunk)

        return StageResult(success=True, data=chunks)
```

### ì„±ëŠ¥
- **ì‹œê°„:** ~/íŒŒì¼
- **ë©”ëª¨ë¦¬:** ~20MB
- **ë³‘ë ¬í™”:** ê°€ëŠ¥

### Chunk ê³„ì¸µ
```
Repo
 â”œâ”€ Module (src/core/)
 â”‚   â”œâ”€ File (main.py)
 â”‚   â”‚   â”œâ”€ Class (MyClass)
 â”‚   â”‚   â”‚   â””â”€ Function (method)
 â”‚   â”‚   â””â”€ Function (top_level_func)
```

---

## Stage 8: RepoMapStage

### ì—­í• 
RepoMap ë¹Œë”© (íŠ¸ë¦¬ + PageRank)

### ì…ë ¥
- `code_graph: CodeGraph`
- `chunks: list[Chunk]`

### ì¶œë ¥
```python
@dataclass
class RepoMap:
    tree: RepoTree          # ë””ë ‰í† ë¦¬ êµ¬ì¡°
    pagerank: dict[str, float]  # íŒŒì¼ ì¤‘ìš”ë„
    summaries: dict[str, str]   # LLM ìš”ì•½
```

### êµ¬í˜„
```python
# src/contexts/analysis_indexing/infrastructure/stages/repomap_stage.py
class RepoMapStage(IndexingStage):
    async def execute(self, ctx: StageContext) -> StageResult:
        # 1. Build tree
        tree = RepoTree.from_graph(ctx.code_graph)

        # 2. PageRank (rustworkx 400x faster)
        pagerank = self._compute_pagerank(ctx.code_graph)

        # 3. LLM summaries (optional, expensive)
        summaries = {}
        if ctx.enable_summaries:
            summaries = await self._generate_summaries(ctx.chunks)

        return StageResult(
            success=True,
            data=RepoMap(tree, pagerank, summaries)
        )
```

### ì„±ëŠ¥
- **ì‹œê°„:** ~1ì´ˆ (tree + pagerank)
- **LLM ìš”ì•½:** ~10ì´ˆ (expensive)
- **ë©”ëª¨ë¦¬:** ~30MB

### PageRank ì•Œê³ ë¦¬ì¦˜
- **Library:** rustworkx (400x faster than NetworkX)
- **Damping:** 0.85
- **Iterations:** 100

---

## Stage 9: IndexingStage

### ì—­í• 
ë‹¤ì¤‘ ì¸ë±ìŠ¤ ì €ì¥

### ì…ë ¥
- ëª¨ë“  ì´ì „ Stage ê²°ê³¼

### ì¶œë ¥
```python
@dataclass
class IndexingResult:
    indexed_files: int
    lexical_indexed: bool
    vector_indexed: bool
    symbol_indexed: bool
    duration_ms: float
```

### êµ¬í˜„
```python
# src/contexts/analysis_indexing/infrastructure/stages/indexing_stage.py
class IndexingStage(IndexingStage):
    async def execute(self, ctx: StageContext) -> StageResult:
        # ë³‘ë ¬ ì¸ë±ì‹±
        tasks = [
            self._index_lexical(ctx),    # Zoekt/Tantivy
            self._index_vector(ctx),     # Qdrant
            self._index_symbol(ctx),     # PostgreSQL/Memgraph
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        return StageResult(success=True, data=results)
```

### ì¸ë±ìŠ¤ íƒ€ì…

| ì¸ë±ìŠ¤ | ê¸°ìˆ  | ìš©ë„ |
|--------|------|------|
| Lexical (Base) | Zoekt | ì „ë¬¸ ê²€ìƒ‰ |
| Lexical (Delta) | Tantivy | ì¦ë¶„ ë³€ê²½ |
| Vector | Qdrant | ì˜ë¯¸ ê²€ìƒ‰ |
| Symbol | PostgreSQL | ì‹¬ë³¼ ì¡°íšŒ |

### ì„±ëŠ¥
- **ì‹œê°„:** ~5ì´ˆ (10K íŒŒì¼)
- **ë³‘ë ¬í™”:** 3ê°œ ì¸ë±ìŠ¤ ë™ì‹œ
- **ë©”ëª¨ë¦¬:** ~100MB

---

## í˜‘ë ¥ì  ì·¨ì†Œ (Graceful Stop)

### ë©”ì»¤ë‹ˆì¦˜

```python
# IndexingOrchestratorSlim
async def execute_with_stop(
    self,
    repo_path: Path,
    stop_event: asyncio.Event,
    progress: JobProgress,
) -> IndexingResult:
    for file_path in files:
        # ğŸ”¥ Cooperative cancellation check
        if stop_event.is_set():
            logger.info("Stop requested, saving progress")
            progress.save()
            return IndexingResult(partial=True)

        # íŒŒì¼ ì²˜ë¦¬
        progress.processing_file = str(file_path)
        await self._process_file(file_path)
        progress.completed_files.add(str(file_path))
```

### ì‚¬ìš© ì˜ˆ
```python
# BackgroundSchedulerì—ì„œ
stop_event = asyncio.Event()
progress = JobProgress(job_id="job-123")

# BALANCED ì‹œì‘
task = orchestrator.execute_with_stop(repo_path, stop_event, progress)

# ì‚¬ìš©ì í™œë™ ê°ì§€
stop_event.set()  # Graceful stop ìš”ì²­

# í˜„ì¬ íŒŒì¼ ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
await asyncio.wait_for(task, timeout=30.0)

# progress.completed_filesë¡œ ì¬ê°œ ê°€ëŠ¥
```

---

## ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

### ì½”ë“œ ì˜ˆì œ

```python
from src.contexts.analysis_indexing.infrastructure.orchestrator import IndexingOrchestrator

# ì´ˆê¸°í™”
orchestrator = IndexingOrchestrator(
    parser_service=parser,
    ir_builder=ir_builder,
    graph_builder=graph_builder,
    # ... ê¸°íƒ€ ì»´í¬ë„ŒíŠ¸
)

# ì‹¤í–‰
result = await orchestrator.execute(
    repo_path=Path("/path/to/repo"),
    repo_id="my-repo",
    snapshot_id="snapshot-123",
    mode=IndexingMode.BALANCED,
)

# ê²°ê³¼
print(f"Indexed {result.indexed_files} files in {result.duration_ms}ms")
```

### ì‹¤í–‰ ì‹œê°„ (10K íŒŒì¼)

| Stage | FAST | BALANCED | DEEP |
|-------|------|----------|------|
| Git | < | < | < |
| Discovery | ~1s | ~1s | ~1s |
| Parsing | ~20s | ~20s | ~20s |
| IR | ~50s | ~50s | ~50s |
| Semantic IR | Skip | ~200s | ~1000s |
| Graph | ~ | ~ | ~ |
| Chunk | ~100s | ~100s | ~100s |
| RepoMap | ~1s | ~1s | ~1s |
| Indexing | ~5s | ~5s | ~5s |
| **Total** | **~3min** | **~6min** | **~20min** |

---

## ì‹¤íŒ¨ ì²˜ë¦¬

### Stage ì‹¤íŒ¨ ì •ì±…

```python
@dataclass
class StageResult:
    success: bool
    data: Any
    error: str | None
    partial: bool = False  # ë¶€ë¶„ ì„±ê³µ
```

### ì „ëµ

| Stage | ì‹¤íŒ¨ ì‹œ | ê³„ì† ì§„í–‰? |
|-------|---------|----------|
| GitStage | WARNING | âœ… Yes |
| DiscoveryStage | ERROR | âŒ No (íŒŒì¼ ì—†ìŒ) |
| ParsingStage | Skip íŒŒì¼ | âœ… Yes (ë‚˜ë¨¸ì§€ ê³„ì†) |
| IRStage | Skip íŒŒì¼ | âœ… Yes |
| SemanticIRStage | Skip íŒŒì¼ | âœ… Yes |
| GraphStage | ERROR | âŒ No (critical) |
| ChunkStage | Skip íŒŒì¼ | âœ… Yes |
| RepoMapStage | WARNING | âœ… Yes |
| IndexingStage | Retry 3íšŒ | âŒ No (ì €ì¥ ì‹¤íŒ¨) |

---

## ì°¸ê³ 

### êµ¬í˜„ íŒŒì¼
```
src/contexts/analysis_indexing/infrastructure/stages/
â”œâ”€â”€ base.py              # BaseStage ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ git_stage.py
â”œâ”€â”€ discovery_stage.py
â”œâ”€â”€ parsing_stage.py
â”œâ”€â”€ ir_stage.py
â”œâ”€â”€ graph_stage.py
â”œâ”€â”€ chunk_stage.py
â”œâ”€â”€ repomap_stage.py
â””â”€â”€ indexing_stage.py
```

### ê´€ë ¨ ë¬¸ì„œ
- `pipelines-detailed.md` - íŒŒì´í”„ë¼ì¸ ì—£ì§€ì¼€ì´ìŠ¤
- `job-orchestrator.md` - Job ì‹œìŠ¤í…œ
- `configuration.md` - Stageë³„ ì„¤ì •

---

**Last 
