# RepoMap Rust í¬íŒ… ê³„íšì„œ

**Version**: 1.0 (2025-12-28)  
**Status**: Planning  
**Total Python LOC**: ~6,149  
**Target Rust LOC**: ~8,000

---

## ğŸ“Š í˜„ì¬ Python RepoMap êµ¬í˜„ ë¶„ì„

### ì•„í‚¤í…ì²˜

```
repo_structure/infrastructure/
â”œâ”€â”€ tree/                    # íŠ¸ë¦¬ ë¹Œë“œ (651 LOC)
â”‚   â”œâ”€â”€ builder.py          # RepoMapTreeBuilder - Chunk â†’ Tree
â”‚   â””â”€â”€ metrics.py          # EntrypointDetector, TestDetector
â”œâ”€â”€ builder/                 # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (639 LOC)
â”‚   â””â”€â”€ orchestrator.py     # RepoMapBuilder - ì „ì²´ í”Œë¡œìš°
â”œâ”€â”€ pagerank/               # ì¤‘ìš”ë„ ê³„ì‚° (875 LOC)
â”‚   â”œâ”€â”€ engine.py           # PageRankEngine - rustworkx ê¸°ë°˜
â”‚   â”œâ”€â”€ aggregator.py       # ê²°ê³¼ ì§‘ê³„
â”‚   â”œâ”€â”€ graph_adapter.py    # ê·¸ë˜í”„ ì–´ëŒ‘í„°
â”‚   â””â”€â”€ incremental.py      # ì ì§„ì  ì—…ë°ì´íŠ¸
â”œâ”€â”€ summarizer/             # LLM ìš”ì•½ (989 LOC)
â”‚   â”œâ”€â”€ hierarchical_summarizer.py  # ê³„ì¸µì  ìš”ì•½
â”‚   â”œâ”€â”€ llm_summarizer.py   # LLM í˜¸ì¶œ
â”‚   â”œâ”€â”€ cost_control.py     # ë¹„ìš© ì œì–´
â”‚   â””â”€â”€ cache.py            # ìš”ì•½ ìºì‹œ
â”œâ”€â”€ models.py               # ë°ì´í„° ëª¨ë¸ (417 LOC)
â”œâ”€â”€ git_history.py          # Git ë¶„ì„ (744 LOC)
â”œâ”€â”€ incremental.py          # ì ì§„ì  ì—…ë°ì´íŠ¸ (416 LOC)
â”œâ”€â”€ storage_*.py            # ì €ì¥ì†Œ (875 LOC)
â””â”€â”€ id_strategy.py          # ID ìƒì„± (107 LOC)
```

### í•µì‹¬ ê¸°ëŠ¥

| ê¸°ëŠ¥ | Python LOC | ì„¤ëª… |
|------|-----------|------|
| **Tree Building** | 651 | Chunk â†’ RepoMapNode ë³€í™˜ |
| **PageRank** | 875 | rustworkx ê¸°ë°˜ ì¤‘ìš”ë„ ê³„ì‚° |
| **Summarizer** | 989 | LLM ê¸°ë°˜ ê³„ì¸µì  ìš”ì•½ |
| **Git History** | 744 | ë³€ê²½ ë¹ˆë„ ë¶„ì„ |
| **Orchestrator** | 639 | ì „ì²´ ë¹Œë“œ í”Œë¡œìš° |
| **Storage** | 875 | PostgreSQL/JSON ì €ì¥ |
| **Models** | 417 | ë°ì´í„° ëª¨ë¸ |
| **Incremental** | 416 | Delta ì—…ë°ì´íŠ¸ |

---

## ğŸ“ í•™ê³„/ì—…ê³„ SOTA ë¹„êµ

### 1. Aider RepoMap (Open Source)

**ì ‘ê·¼ë²•**: Tree-sitter + íƒœê·¸ ê¸°ë°˜ ì‹¬ë³¼ ì¶”ì¶œ

```
ì¥ì :
âœ… ê²½ëŸ‰ (tree-sitterë§Œ ì‚¬ìš©)
âœ… ì¤‘ìš”ë„ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ í•„í„°ë§
âœ… í† í° ì˜ˆì‚° ê´€ë¦¬

ë‹¨ì :
âŒ PageRank ì—†ìŒ (ë‹¨ìˆœ ì°¸ì¡° ì¹´ìš´íŠ¸)
âŒ ê³„ì¸µì  ìš”ì•½ ì—†ìŒ
âŒ Git history ë¯¸í™œìš©
```

**ìš°ë¦¬ì˜ ê°œì„ ì **:
- âœ… PageRankë¡œ ì •êµí•œ ì¤‘ìš”ë„ ê³„ì‚°
- âœ… 2-Level ê³„ì¸µì  ìš”ì•½
- âœ… Git ë³€ê²½ ë¹ˆë„ í†µí•©

### 2. Sourcegraph (Enterprise)

**ì ‘ê·¼ë²•**: SCIP/LSIF ê¸°ë°˜ ì‹¬ë³¼ ê·¸ë˜í”„

```
ì¥ì :
âœ… ì •ë°€í•œ ì‹¬ë³¼ í•´ìƒë„ (LSP ìˆ˜ì¤€)
âœ… Cross-repo ì°¸ì¡°
âœ… ì¦ë¶„ ì¸ë±ì‹±

ë‹¨ì :
âŒ ë¬´ê±°ì›€ (ì–¸ì–´ë³„ ì¸ë±ì„œ í•„ìš”)
âŒ ìš”ì•½/ì¤‘ìš”ë„ ì—†ìŒ
âŒ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì—†ìŒ
```

**ìš°ë¦¬ì˜ ê°œì„ ì **:
- âœ… Chunk ê¸°ë°˜ ê²½ëŸ‰ êµ¬ì¡°
- âœ… AI ì—ì´ì „íŠ¸ ìµœì í™” (í† í° ì˜ˆì‚°)
- âœ… LLM ìš”ì•½ìœ¼ë¡œ ë¹ ë¥¸ ì´í•´

### 3. GitHub CodeSearch

**ì ‘ê·¼ë²•**: Zoekt + Tree-sitter

```
ì¥ì :
âœ… ëŒ€ê·œëª¨ í™•ì¥ì„±
âœ… ë¹ ë¥¸ ê²€ìƒ‰

ë‹¨ì :
âŒ êµ¬ì¡° ë¶„ì„ ì œí•œì 
âŒ ì˜ë¯¸ë¡ ì  ì´í•´ ì—†ìŒ
```

### 4. í•™ê³„ ì—°êµ¬

| ë…¼ë¬¸/ê¸°ë²• | í•µì‹¬ ì•„ì´ë””ì–´ | ì ìš© |
|----------|-------------|------|
| **HITS Algorithm** (Kleinberg, 1999) | Hub/Authority ìŠ¤ì½”ì–´ | PageRank ë³´ì™„ |
| **Personalized PageRank** (Haveliwala, 2002) | ì‹œì‘ì  ê¸°ë°˜ ë­í‚¹ | Query-aware ì¤‘ìš”ë„ |
| **Graph Neural Networks** (GNN) | ë…¸ë“œ ì„ë² ë”© í•™ìŠµ | ì‹¬ë³¼ ìœ ì‚¬ë„ |
| **Incremental Graph Algorithms** (VLDB 2020) | Delta ê¸°ë°˜ ì—…ë°ì´íŠ¸ | ì ì§„ì  PageRank |
| **Code Summarization** (ACL 2021) | Transformer ê¸°ë°˜ | LLM ìš”ì•½ |

---

## ğŸš€ Rust í¬íŒ… ì „ëµ

### Phase 1: Core Tree Builder (1ì£¼)

**ëª©í‘œ**: Chunk â†’ RepoMapNode ë³€í™˜ì˜ Rust êµ¬í˜„

```rust
// src/features/repomap/infrastructure/tree_builder.rs

pub struct RepoMapTreeBuilder {
    repo_id: String,
    snapshot_id: String,
    nodes: DashMap<String, RepoMapNode>,
    id_gen: RepoMapIdGenerator,
    // O(1) ì¸ë±ìŠ¤
    chunk_to_node: DashMap<String, String>,
    fqn_to_node: DashMap<(String, String), String>,
}

impl RepoMapTreeBuilder {
    /// ë³‘ë ¬ ë¹Œë“œ (Rayon)
    pub fn build_parallel(
        &self,
        chunks: &[Chunk],
        chunk_to_graph: &HashMap<String, HashSet<String>>,
    ) -> Vec<RepoMapNode> {
        // Step 1: ë ˆë²¨ë³„ ë³‘ë ¬ ë¶„ë¥˜
        let chunks_by_level = self.classify_by_level_parallel(chunks);
        
        // Step 2: ë””ë ‰í† ë¦¬ ë…¸ë“œ ë³‘ë ¬ ìƒì„±
        self.build_directories_parallel(chunks);
        
        // Step 3: Chunk ë…¸ë“œ ë³‘ë ¬ ìƒì„±
        self.create_chunk_nodes_parallel(chunks, chunk_to_graph);
        
        // Step 4: Bottom-up ë©”íŠ¸ë¦­ ì§‘ê³„ (ë³‘ë ¬)
        self.aggregate_metrics_parallel();
        
        self.nodes.iter().map(|e| e.value().clone()).collect()
    }
    
    /// O(N) ë³‘ë ¬ ë©”íŠ¸ë¦­ ì§‘ê³„ (vs Python O(N log N))
    fn aggregate_metrics_parallel(&self) {
        // ë ˆë²¨ë³„ ë³‘ë ¬ ì²˜ë¦¬
        let max_depth = self.nodes.iter()
            .map(|n| n.depth)
            .max()
            .unwrap_or(0);
        
        for depth in (0..=max_depth).rev() {
            let nodes_at_depth: Vec<_> = self.nodes.iter()
                .filter(|n| n.depth == depth)
                .collect();
            
            nodes_at_depth.par_iter().for_each(|node| {
                if let Some(parent_id) = &node.parent_id {
                    if let Some(mut parent) = self.nodes.get_mut(parent_id) {
                        // Atomic ì—…ë°ì´íŠ¸
                        parent.metrics.loc.fetch_add(node.metrics.loc, Ordering::Relaxed);
                        parent.metrics.symbol_count.fetch_add(
                            node.metrics.symbol_count, Ordering::Relaxed
                        );
                    }
                }
            });
        }
    }
}
```

**SOTA ê°œì„ **:
- **ë³‘ë ¬ ë¹Œë“œ**: Rayon work-stealing (Python: ìˆœì°¨)
- **Lock-free ì¸ë±ìŠ¤**: DashMap (Python: dict + set)
- **Atomic ë©”íŠ¸ë¦­ ì§‘ê³„**: ë ˆë²¨ë³„ ë³‘ë ¬ (Python: ìˆœì°¨ O(N log N))

**ì˜ˆìƒ ì„±ëŠ¥**: 10-20x faster

---

### Phase 2: PageRank Engine (0.5ì£¼)

**ëª©í‘œ**: SOTA PageRank + HITS ì•Œê³ ë¦¬ì¦˜

```rust
// src/features/repomap/infrastructure/pagerank.rs

pub struct PageRankEngine {
    config: PageRankConfig,
    graph: DiGraph<String, f64>,
    // Personalized PageRank ì§€ì›
    teleport_set: Option<HashSet<String>>,
}

impl PageRankEngine {
    /// Standard PageRank (rustworkx ë˜í•‘)
    pub fn compute_pagerank(
        &self,
        graph_doc: &GraphDocument,
    ) -> HashMap<String, f64> {
        let rx_graph = self.build_rx_graph(graph_doc);
        rx::pagerank(
            &rx_graph,
            self.config.damping,
            self.config.max_iterations,
            self.config.tolerance,
        )
    }
    
    /// SOTA: Personalized PageRank (Query-aware)
    /// ë…¼ë¬¸: "Topic-Sensitive PageRank" (Haveliwala, 2002)
    pub fn compute_personalized_pagerank(
        &self,
        graph_doc: &GraphDocument,
        query_nodes: &[String],  // ì¿¼ë¦¬ ê´€ë ¨ ë…¸ë“œ
    ) -> HashMap<String, f64> {
        let rx_graph = self.build_rx_graph(graph_doc);
        
        // Teleport probabilityë¥¼ query_nodesì— ì§‘ì¤‘
        let personalization: HashMap<usize, f64> = query_nodes.iter()
            .filter_map(|id| self.node_map.get(id).map(|&idx| (idx, 1.0)))
            .collect();
        
        rx::pagerank_personalized(
            &rx_graph,
            personalization,
            self.config.damping,
            self.config.max_iterations,
        )
    }
    
    /// SOTA: HITS Algorithm (Hub/Authority)
    /// ë…¼ë¬¸: "Authoritative Sources" (Kleinberg, 1999)
    pub fn compute_hits(
        &self,
        graph_doc: &GraphDocument,
    ) -> (HashMap<String, f64>, HashMap<String, f64>) {
        let rx_graph = self.build_rx_graph(graph_doc);
        
        let (hubs, authorities) = rx::hits(
            &rx_graph,
            self.config.max_iterations,
            self.config.tolerance,
        );
        
        (self.map_scores(hubs), self.map_scores(authorities))
    }
    
    /// SOTA: Combined Score (PageRank + HITS + Degree)
    pub fn compute_combined_importance(
        &self,
        graph_doc: &GraphDocument,
        weights: &ImportanceWeights,
    ) -> HashMap<String, ImportanceScore> {
        let pagerank = self.compute_pagerank(graph_doc);
        let (hubs, authorities) = self.compute_hits(graph_doc);
        let degree = self.compute_degree_centrality(graph_doc);
        
        // Weighted combination
        pagerank.keys().map(|id| {
            let score = ImportanceScore {
                pagerank: pagerank.get(id).copied().unwrap_or(0.0),
                hub: hubs.get(id).copied().unwrap_or(0.0),
                authority: authorities.get(id).copied().unwrap_or(0.0),
                degree: degree.get(id).copied().unwrap_or(0.0),
                combined: weights.pagerank * pagerank.get(id).unwrap_or(&0.0)
                    + weights.authority * authorities.get(id).unwrap_or(&0.0)
                    + weights.degree * degree.get(id).unwrap_or(&0.0),
            };
            (id.clone(), score)
        }).collect()
    }
}

/// SOTA: Incremental PageRank (Delta ê¸°ë°˜)
/// ë…¼ë¬¸: "Incremental Graph Pattern Matching" (VLDB 2020)
pub struct IncrementalPageRank {
    base_scores: HashMap<String, f64>,
    affected_nodes: HashSet<String>,
}

impl IncrementalPageRank {
    /// Deltaë§Œ ì¬ê³„ì‚° (ì „ì²´ ì¬ê³„ì‚° ëŒ€ì‹ )
    pub fn update_incremental(
        &mut self,
        delta: &GraphDelta,
        max_propagation_depth: usize,
    ) -> HashMap<String, f64> {
        // 1. ì˜í–¥ë°›ëŠ” ë…¸ë“œ ì‹ë³„ (BFS, depth ì œí•œ)
        self.affected_nodes = self.find_affected_nodes(delta, max_propagation_depth);
        
        // 2. ì˜í–¥ë°›ëŠ” ì„œë¸Œê·¸ë˜í”„ë§Œ ì¬ê³„ì‚°
        let subgraph_scores = self.compute_subgraph_pagerank(&self.affected_nodes);
        
        // 3. ê¸°ì¡´ ìŠ¤ì½”ì–´ì™€ ë³‘í•©
        for (id, score) in subgraph_scores {
            self.base_scores.insert(id, score);
        }
        
        self.base_scores.clone()
    }
}
```

**SOTA ê°œì„ **:
- **Personalized PageRank**: Query-aware ì¤‘ìš”ë„
- **HITS Algorithm**: Hub/Authority ë¶„ë¦¬
- **Incremental PageRank**: Delta ê¸°ë°˜ ì—…ë°ì´íŠ¸
- **Combined Score**: ë‹¤ì¤‘ ë©”íŠ¸ë¦­ ê°€ì¤‘ í•©ì‚°

**ì˜ˆìƒ ì„±ëŠ¥**: 5x faster (ì´ë¯¸ rustworkx ì‚¬ìš© ì¤‘ì´ë¯€ë¡œ ì•Œê³ ë¦¬ì¦˜ ê°œì„ ì´ ì£¼)

---

### Phase 3: Git History Analyzer (0.5ì£¼)

**ëª©í‘œ**: ë³€ê²½ ë¹ˆë„ + ì½”ë“œ ì—°ë ¹ ë¶„ì„

```rust
// src/features/repomap/infrastructure/git_history.rs

pub struct GitHistoryAnalyzer {
    repo_path: PathBuf,
    cache: LruCache<String, ChangeMetrics>,
}

impl GitHistoryAnalyzer {
    /// íŒŒì¼ë³„ ë³€ê²½ ë¹ˆë„ ê³„ì‚°
    pub fn compute_change_frequency(
        &mut self,
        file_paths: &[String],
        days: u32,
    ) -> HashMap<String, ChangeMetrics> {
        // Git log ë³‘ë ¬ ì‹¤í–‰
        file_paths.par_iter()
            .map(|path| {
                let metrics = self.analyze_file_history(path, days);
                (path.clone(), metrics)
            })
            .collect()
    }
    
    /// SOTA: Code Churn Analysis
    /// ë…¼ë¬¸: "Predicting Faults" (IEEE TSE, 2005)
    pub fn compute_code_churn(
        &self,
        file_path: &str,
        days: u32,
    ) -> ChurnMetrics {
        let commits = self.get_commits_for_file(file_path, days);
        
        let mut total_added = 0;
        let mut total_deleted = 0;
        let mut unique_authors = HashSet::new();
        
        for commit in commits {
            let diff = self.get_diff_stats(&commit, file_path);
            total_added += diff.additions;
            total_deleted += diff.deletions;
            unique_authors.insert(commit.author.clone());
        }
        
        ChurnMetrics {
            total_changes: total_added + total_deleted,
            churn_rate: (total_added + total_deleted) as f64 / days as f64,
            author_count: unique_authors.len(),
            // Normalized churn (per 100 LOC)
            normalized_churn: (total_added + total_deleted) as f64 / 
                (self.get_file_loc(file_path) as f64 / 100.0),
        }
    }
    
    /// SOTA: Hot Spot Detection
    /// ë…¼ë¬¸: "Code Red" (ESEC/FSE 2020)
    pub fn detect_hotspots(
        &self,
        file_paths: &[String],
        config: &HotspotConfig,
    ) -> Vec<Hotspot> {
        let change_freq = self.compute_change_frequency(file_paths, config.days);
        let churn = file_paths.par_iter()
            .map(|p| (p.clone(), self.compute_code_churn(p, config.days)))
            .collect::<HashMap<_, _>>();
        
        // Combined hotspot score
        let mut hotspots: Vec<_> = file_paths.iter()
            .map(|path| {
                let freq = change_freq.get(path).map(|m| m.commit_count).unwrap_or(0);
                let churn_val = churn.get(path).map(|c| c.churn_rate).unwrap_or(0.0);
                
                Hotspot {
                    path: path.clone(),
                    score: config.freq_weight * freq as f64 + config.churn_weight * churn_val,
                    change_frequency: freq,
                    churn_rate: churn_val,
                }
            })
            .collect();
        
        hotspots.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap());
        hotspots
    }
}
```

**SOTA ê°œì„ **:
- **Code Churn**: ë³€ê²½ëŸ‰ + ì‚­ì œëŸ‰ ì¶”ì 
- **Hot Spot Detection**: ë¬¸ì œ ì½”ë“œ ì¡°ê¸° ë°œê²¬
- **Multi-author Tracking**: í˜‘ì—… ë³µì¡ë„ ì¸¡ì •

**ì˜ˆìƒ ì„±ëŠ¥**: 8x faster (ë³‘ë ¬ Git ë¶„ì„)

---

### Phase 4: Incremental Update Engine (1ì£¼)

**ëª©í‘œ**: Delta ê¸°ë°˜ ì ì§„ì  ì—…ë°ì´íŠ¸

```rust
// src/features/repomap/infrastructure/incremental.rs

/// SOTA: Merkle Hash ê¸°ë°˜ ë³€ê²½ ê°ì§€
/// ê¸°ë²•: Git/IPFS ìŠ¤íƒ€ì¼
pub struct MerkleTreeCache {
    root_hash: [u8; 32],
    node_hashes: DashMap<String, [u8; 32]>,
}

impl MerkleTreeCache {
    /// ë³€ê²½ëœ ë…¸ë“œë§Œ ì‹ë³„
    pub fn detect_changes(
        &self,
        new_nodes: &[RepoMapNode],
    ) -> ChangeSet {
        let mut added = Vec::new();
        let mut modified = Vec::new();
        let mut removed = Vec::new();
        
        // ë³‘ë ¬ í•´ì‹œ ë¹„êµ
        let new_hashes: HashMap<String, [u8; 32]> = new_nodes.par_iter()
            .map(|n| (n.id.clone(), self.compute_node_hash(n)))
            .collect();
        
        // ë³€ê²½ ê°ì§€
        for (id, new_hash) in &new_hashes {
            match self.node_hashes.get(id) {
                Some(old_hash) if *old_hash != *new_hash => {
                    modified.push(id.clone());
                }
                None => {
                    added.push(id.clone());
                }
                _ => {}
            }
        }
        
        // ì‚­ì œëœ ë…¸ë“œ
        for entry in self.node_hashes.iter() {
            if !new_hashes.contains_key(entry.key()) {
                removed.push(entry.key().clone());
            }
        }
        
        ChangeSet { added, modified, removed }
    }
    
    /// Merkle í•´ì‹œ ê³„ì‚° (leaf â†’ root)
    fn compute_node_hash(&self, node: &RepoMapNode) -> [u8; 32] {
        use blake3::Hasher;
        
        let mut hasher = Hasher::new();
        hasher.update(node.id.as_bytes());
        hasher.update(node.name.as_bytes());
        hasher.update(&node.metrics.loc.to_le_bytes());
        
        // Children í•´ì‹œ í†µí•©
        for child_id in &node.children_ids {
            if let Some(child_hash) = self.node_hashes.get(child_id) {
                hasher.update(child_hash.value());
            }
        }
        
        *hasher.finalize().as_bytes()
    }
}

/// ì ì§„ì  RepoMap ì—…ë°ì´íŠ¸
pub struct IncrementalRepoMapBuilder {
    base_snapshot: RepoMapSnapshot,
    merkle_cache: MerkleTreeCache,
    pagerank_cache: IncrementalPageRank,
}

impl IncrementalRepoMapBuilder {
    /// Delta ê¸°ë°˜ ì—…ë°ì´íŠ¸
    pub fn update_incremental(
        &mut self,
        chunk_delta: &ChunkDelta,
        graph_delta: &GraphDelta,
    ) -> RepoMapSnapshot {
        // 1. ë³€ê²½ëœ ë…¸ë“œ ì‹ë³„
        let changes = self.merkle_cache.detect_changes(&self.base_snapshot.nodes);
        
        // 2. ë³€ê²½ëœ ë…¸ë“œë§Œ ì¬ë¹Œë“œ
        let updated_nodes = self.rebuild_affected_nodes(&changes, chunk_delta);
        
        // 3. PageRank ì ì§„ì  ì—…ë°ì´íŠ¸
        let updated_pagerank = self.pagerank_cache.update_incremental(
            graph_delta,
            2, // MAX_PROPAGATION_DEPTH
        );
        
        // 4. ë©”íŠ¸ë¦­ ë³‘í•©
        for node in &mut updated_nodes {
            if let Some(&score) = updated_pagerank.get(&node.id) {
                node.metrics.pagerank = score;
            }
        }
        
        // 5. ìŠ¤ëƒ…ìƒ· ì—…ë°ì´íŠ¸
        self.base_snapshot.set_nodes(updated_nodes);
        self.base_snapshot.clone()
    }
}
```

**SOTA ê°œì„ **:
- **Merkle Hash**: O(ë³€ê²½) ë³€ê²½ ê°ì§€ (vs O(N) ì „ì²´ ë¹„êµ)
- **Incremental PageRank**: ì˜í–¥ë°›ëŠ” ë…¸ë“œë§Œ ì¬ê³„ì‚°
- **Blake3 Hash**: ë¹ ë¥¸ ì•”í˜¸í™” í•´ì‹œ

**ì˜ˆìƒ ì„±ëŠ¥**: 50-100x faster (ëŒ€ê·œëª¨ ì €ì¥ì†Œ ì¦ë¶„ ì—…ë°ì´íŠ¸)

---

### Phase 5: Storage Adapters (0.5ì£¼)

**ëª©í‘œ**: PostgreSQL + JSON ì €ì¥ì†Œ

```rust
// src/features/repomap/infrastructure/storage.rs

/// ì €ì¥ì†Œ Port (DIP)
pub trait RepoMapStorage: Send + Sync {
    async fn save_snapshot(&self, snapshot: &RepoMapSnapshot) -> Result<(), StorageError>;
    async fn load_snapshot(&self, repo_id: &str, snapshot_id: &str) -> Result<Option<RepoMapSnapshot>, StorageError>;
    async fn list_snapshots(&self, repo_id: &str) -> Result<Vec<SnapshotMeta>, StorageError>;
    async fn delete_snapshot(&self, repo_id: &str, snapshot_id: &str) -> Result<bool, StorageError>;
}

/// PostgreSQL ì €ì¥ì†Œ
pub struct PostgresRepoMapStorage {
    pool: PgPool,
}

impl RepoMapStorage for PostgresRepoMapStorage {
    async fn save_snapshot(&self, snapshot: &RepoMapSnapshot) -> Result<(), StorageError> {
        // Batch insert (1000ê°œì”©)
        for chunk in snapshot.nodes.chunks(1000) {
            let values: Vec<_> = chunk.iter()
                .map(|n| (
                    &n.id, &n.repo_id, &n.snapshot_id,
                    &n.kind, &n.name, &n.path,
                    serde_json::to_value(&n.metrics)?,
                ))
                .collect();
            
            sqlx::query!(
                r#"
                INSERT INTO repomap_nodes (id, repo_id, snapshot_id, kind, name, path, metrics)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
                ON CONFLICT (id) DO UPDATE SET
                    metrics = EXCLUDED.metrics,
                    updated_at = NOW()
                "#,
                // ... values
            )
            .execute(&self.pool)
            .await?;
        }
        Ok(())
    }
}

/// JSON ì €ì¥ì†Œ (ë¡œì»¬ ê°œë°œìš©)
pub struct JsonRepoMapStorage {
    base_path: PathBuf,
}

impl RepoMapStorage for JsonRepoMapStorage {
    async fn save_snapshot(&self, snapshot: &RepoMapSnapshot) -> Result<(), StorageError> {
        let path = self.snapshot_path(&snapshot.repo_id, &snapshot.snapshot_id);
        
        // ì••ì¶• ì €ì¥ (gzip)
        let json = serde_json::to_vec(snapshot)?;
        let compressed = self.compress_gzip(&json)?;
        
        tokio::fs::write(&path, compressed).await?;
        Ok(())
    }
}
```

---

### Phase 6: PyO3 Bindings (0.5ì£¼)

**ëª©í‘œ**: Python í†µí•©

```rust
// src/adapters/pyo3/repomap_bindings.rs

#[pyfunction]
fn build_repomap(
    py: Python,
    chunks: Vec<PyObject>,
    chunk_to_graph: HashMap<String, HashSet<String>>,
    config: PyObject,
) -> PyResult<PyObject> {
    py.allow_threads(|| {
        let chunks: Vec<Chunk> = convert_chunks(chunks)?;
        let config: RepoMapConfig = extract_config(config)?;
        
        let builder = RepoMapTreeBuilder::new(config);
        let nodes = builder.build_parallel(&chunks, &chunk_to_graph);
        
        Ok(convert_to_pyobject(nodes))
    })
}

#[pyfunction]
fn compute_importance_scores(
    py: Python,
    graph_doc: PyObject,
    config: PyObject,
) -> PyResult<HashMap<String, PyObject>> {
    py.allow_threads(|| {
        let engine = PageRankEngine::new(config);
        let scores = engine.compute_combined_importance(&graph_doc, &weights);
        Ok(scores)
    })
}

#[pyfunction]
fn update_repomap_incremental(
    py: Python,
    base_snapshot: PyObject,
    chunk_delta: PyObject,
    graph_delta: PyObject,
) -> PyResult<PyObject> {
    py.allow_threads(|| {
        let builder = IncrementalRepoMapBuilder::from_snapshot(base_snapshot);
        let updated = builder.update_incremental(&chunk_delta, &graph_delta);
        Ok(convert_to_pyobject(updated))
    })
}

#[pymodule]
fn codegraph_repomap(_py: Python, m: &PyModule) -> PyResult<()> {
    m.add_function(wrap_pyfunction!(build_repomap, m)?)?;
    m.add_function(wrap_pyfunction!(compute_importance_scores, m)?)?;
    m.add_function(wrap_pyfunction!(update_repomap_incremental, m)?)?;
    Ok(())
}
```

---

## ğŸ“Š ì˜ˆìƒ LOC ë° ì¼ì •

| Phase | ê¸°ëŠ¥ | Python LOC | Rust LOC | ê¸°ê°„ | ì„±ëŠ¥ í–¥ìƒ |
|-------|------|-----------|----------|------|----------|
| **1** | Tree Builder | 651 | ~1,000 | 1ì£¼ | 10-20x |
| **2** | PageRank Engine | 875 | ~1,200 | 0.5ì£¼ | 5x |
| **3** | Git History | 744 | ~900 | 0.5ì£¼ | 8x |
| **4** | Incremental Update | 416 | ~1,500 | 1ì£¼ | 50-100x |
| **5** | Storage | 875 | ~1,000 | 0.5ì£¼ | 3x |
| **6** | PyO3 Bindings | - | ~500 | 0.5ì£¼ | - |
| **7** | Models + Utils | 417 + 107 | ~800 | 0.5ì£¼ | - |
| | **í•©ê³„** | **~6,149** | **~8,000** | **4.5ì£¼** | **10-100x** |

---

## ğŸ¯ í•™ê³„/ì—…ê³„ SOTA ëŒ€ë¹„ ì°¨ë³„ì 

| ê¸°ëŠ¥ | Aider | Sourcegraph | ìš°ë¦¬ (Rust) |
|------|-------|-------------|------------|
| **Tree Building** | ìˆœì°¨ | ìˆœì°¨ | âœ… ë³‘ë ¬ (Rayon) |
| **PageRank** | âŒ | âŒ | âœ… + HITS + PPR |
| **Incremental** | âŒ | O(N) | âœ… O(ë³€ê²½) Merkle |
| **Git History** | âŒ | âŒ | âœ… Churn + Hotspot |
| **LLM ìš”ì•½** | âŒ | âŒ | âœ… ê³„ì¸µì  |
| **Query-aware** | âŒ | âŒ | âœ… PPR |

---

## ğŸ“ Rust ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
codegraph-ir/src/features/repomap/
â”œâ”€â”€ mod.rs
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ mod.rs
â”‚   â””â”€â”€ models.rs           # RepoMapNode, Metrics, Snapshot
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ tree_builder.rs     # ë³‘ë ¬ íŠ¸ë¦¬ ë¹Œë“œ
â”‚   â”œâ”€â”€ pagerank.rs         # PageRank + HITS + PPR
â”‚   â”œâ”€â”€ git_history.rs      # ë³€ê²½ ë¹ˆë„ + Churn
â”‚   â”œâ”€â”€ incremental.rs      # Merkle + Delta ì—…ë°ì´íŠ¸
â”‚   â”œâ”€â”€ storage_postgres.rs # PostgreSQL ì €ì¥ì†Œ
â”‚   â””â”€â”€ storage_json.rs     # JSON ì €ì¥ì†Œ
â””â”€â”€ ports/
    â””â”€â”€ mod.rs              # RepoMapStorage trait
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: Tree Builder
- [ ] `RepoMapNode` Rust êµ¬ì¡°ì²´
- [ ] `RepoMapTreeBuilder` ë³‘ë ¬ ë¹Œë“œ
- [ ] DashMap ê¸°ë°˜ ì¸ë±ìŠ¤
- [ ] Atomic ë©”íŠ¸ë¦­ ì§‘ê³„
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ 10ê°œ+

### Phase 2: PageRank
- [ ] Standard PageRank (rustworkx)
- [ ] Personalized PageRank
- [ ] HITS Algorithm
- [ ] Combined Score
- [ ] ë²¤ì¹˜ë§ˆí¬

### Phase 3: Git History
- [ ] ë³‘ë ¬ Git log ë¶„ì„
- [ ] Code Churn ê³„ì‚°
- [ ] Hot Spot Detection
- [ ] LRU ìºì‹œ

### Phase 4: Incremental
- [ ] Merkle Hash ìºì‹œ
- [ ] Delta ë³€ê²½ ê°ì§€
- [ ] Incremental PageRank
- [ ] ìŠ¤ëƒ…ìƒ· ë³‘í•©

### Phase 5: Storage
- [ ] PostgreSQL ì–´ëŒ‘í„°
- [ ] JSON ì–´ëŒ‘í„°
- [ ] Batch insert ìµœì í™”

### Phase 6: PyO3
- [ ] `build_repomap()`
- [ ] `compute_importance_scores()`
- [ ] `update_repomap_incremental()`
- [ ] Python í…ŒìŠ¤íŠ¸

---

---

## ğŸ”— Rust ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ í†µí•©

### í˜„ì¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°

```
pipeline/
â”œâ”€â”€ config.rs           # íŒŒì´í”„ë¼ì¸ ì„¤ì •
â”œâ”€â”€ core.rs             # í•µì‹¬ íŒŒì´í”„ë¼ì¸ ë¡œì§
â”œâ”€â”€ end_to_end_orchestrator.rs  # E2E ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
â”œâ”€â”€ sota_pipeline.rs    # SOTA íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ stages.rs           # ìŠ¤í…Œì´ì§€ ì •ì˜
â””â”€â”€ stage_dag.rs        # DAG ê¸°ë°˜ ì‹¤í–‰
```

### RepoMap ìŠ¤í…Œì´ì§€ ì¶”ê°€

```rust
// pipeline/stages.rs ì— ì¶”ê°€

/// L8: RepoMap ìŠ¤í…Œì´ì§€
pub struct RepoMapStage {
    config: RepoMapConfig,
}

impl Stage for RepoMapStage {
    fn name(&self) -> &'static str {
        "L8_RepoMap"
    }
    
    fn dependencies(&self) -> Vec<&'static str> {
        vec!["L2_Chunking", "L3_CrossFile"]  // Chunk + GraphDocument í•„ìš”
    }
    
    fn execute(&self, ctx: &mut PipelineContext) -> Result<(), PipelineError> {
        // 1. Chunkì™€ GraphDocument ê°€ì ¸ì˜¤ê¸°
        let chunks = ctx.get_stage_result::<Vec<Chunk>>("L2_Chunking")?;
        let graph_doc = ctx.get_stage_result::<GraphDocument>("L3_CrossFile")?;
        
        // 2. RepoMap ë¹Œë“œ
        let builder = RepoMapTreeBuilder::new(ctx.repo_id(), ctx.snapshot_id());
        let chunk_to_graph = ctx.get_chunk_to_graph_mapping()?;
        let nodes = builder.build_parallel(&chunks, &chunk_to_graph);
        
        // 3. PageRank ê³„ì‚°
        let pagerank_engine = PageRankEngine::new(&self.config);
        let scores = pagerank_engine.compute_combined_importance(&graph_doc, &self.config.weights);
        
        // 4. ë©”íŠ¸ë¦­ ë³‘í•©
        let enriched_nodes = self.merge_pagerank_scores(nodes, scores);
        
        // 5. ìŠ¤ëƒ…ìƒ· ìƒì„±
        let snapshot = RepoMapSnapshot::new(ctx.repo_id(), ctx.snapshot_id(), enriched_nodes);
        
        ctx.set_stage_result("L8_RepoMap", snapshot);
        Ok(())
    }
}
```

### E2E ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í†µí•©

```rust
// pipeline/end_to_end_orchestrator.rs ìˆ˜ì •

impl IRIndexingOrchestrator {
    pub fn execute(&self) -> Result<E2EPipelineResult, PipelineError> {
        // ... ê¸°ì¡´ ìŠ¤í…Œì´ì§€ë“¤ ...
        
        // L8: RepoMap (ì„ íƒì )
        if self.config.stages.enable_repomap {
            let repomap_stage = RepoMapStage::new(&self.config.repomap_config);
            ctx.execute_stage(&repomap_stage)?;
            
            // ê²°ê³¼ ì¶”ì¶œ
            let snapshot = ctx.get_stage_result::<RepoMapSnapshot>("L8_RepoMap")?;
            result.repomap_snapshot = Some(snapshot);
        }
        
        Ok(result)
    }
}
```

### Python API ì¶”ê°€

```rust
// lib.rs ì— ì¶”ê°€

/// Build RepoMap from E2E pipeline result
#[cfg(feature = "python")]
#[pyfunction]
fn build_repomap_from_pipeline(
    py: Python,
    chunks: Vec<PyObject>,
    graph_doc: PyObject,
    config: Option<PyObject>,
) -> PyResult<Py<PyDict>> {
    init_rayon();
    
    // GIL RELEASE - Build RepoMap in Rust
    let snapshot = py.allow_threads(|| {
        let builder = RepoMapTreeBuilder::new("", "");
        let nodes = builder.build_parallel(&chunks, &chunk_to_graph);
        RepoMapSnapshot::new("", "", nodes)
    });
    
    convert_repomap_to_python(py, snapshot)
}

/// Compute PageRank importance scores
#[cfg(feature = "python")]
#[pyfunction]
fn compute_repomap_pagerank(
    py: Python,
    graph_doc: PyObject,
    config: Option<PyObject>,
) -> PyResult<Py<PyDict>> {
    init_rayon();
    
    let scores = py.allow_threads(|| {
        let engine = PageRankEngine::new(config);
        engine.compute_combined_importance(&graph_doc, &weights)
    });
    
    Ok(convert_scores_to_python(py, scores))
}

/// Update RepoMap incrementally
#[cfg(feature = "python")]
#[pyfunction]
fn update_repomap_incremental(
    py: Python,
    base_snapshot: PyObject,
    chunk_delta: PyObject,
    graph_delta: PyObject,
) -> PyResult<Py<PyDict>> {
    init_rayon();
    
    let updated = py.allow_threads(|| {
        let builder = IncrementalRepoMapBuilder::from_snapshot(base_snapshot);
        builder.update_incremental(&chunk_delta, &graph_delta)
    });
    
    convert_repomap_to_python(py, updated)
}
```

### íŒŒì´í”„ë¼ì¸ ì„¤ì •

```rust
// pipeline/config.rs ì— ì¶”ê°€

#[derive(Clone, Debug)]
pub struct RepoMapConfig {
    /// Enable RepoMap building
    pub enabled: bool,
    
    /// PageRank settings
    pub pagerank_damping: f64,
    pub pagerank_max_iterations: usize,
    
    /// Importance weights
    pub weights: ImportanceWeights,
    
    /// Incremental settings
    pub enable_incremental: bool,
    pub merkle_cache_size: usize,
    
    /// Git history settings
    pub enable_git_history: bool,
    pub git_history_days: u32,
}

impl Default for RepoMapConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            pagerank_damping: 0.85,
            pagerank_max_iterations: 20,
            weights: ImportanceWeights::default(),
            enable_incremental: true,
            merkle_cache_size: 100_000,
            enable_git_history: true,
            git_history_days: 90,
        }
    }
}
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ëª©í‘œ

### í˜„ì¬ Python ì„±ëŠ¥ (ì˜ˆìƒ)

| ì‘ì—… | 1K íŒŒì¼ | 10K íŒŒì¼ | 100K íŒŒì¼ |
|------|---------|----------|-----------|
| Tree Build | ~500ms | ~5s | ~50s |
| PageRank | ~100ms | ~1s | ~10s |
| Git History | ~2s | ~20s | ~200s |
| Full Build | ~3s | ~30s | ~300s |
| Incremental | ~300ms | ~3s | ~30s |

### Rust ëª©í‘œ ì„±ëŠ¥ (10-100x í–¥ìƒ)

| ì‘ì—… | 1K íŒŒì¼ | 10K íŒŒì¼ | 100K íŒŒì¼ |
|------|---------|----------|-----------|
| Tree Build | ~50ms | ~500ms | ~5s |
| PageRank | ~10ms | ~100ms | ~1s |
| Git History | ~200ms | ~2s | ~20s |
| Full Build | ~300ms | ~3s | ~30s |
| Incremental | ~30ms | ~300ms | ~3s |

### ë²¤ì¹˜ë§ˆí¬ í™˜ê²½

```rust
// benches/repomap_bench.rs

#[bench]
fn bench_tree_build_1k_files(b: &mut Bencher) {
    let chunks = generate_test_chunks(1000);
    let chunk_to_graph = generate_test_mapping(1000);
    
    b.iter(|| {
        let builder = RepoMapTreeBuilder::new("test", "v1");
        builder.build_parallel(&chunks, &chunk_to_graph)
    });
}

#[bench]
fn bench_pagerank_10k_nodes(b: &mut Bencher) {
    let graph_doc = generate_test_graph(10_000, 50_000);
    let engine = PageRankEngine::new(PageRankConfig::default());
    
    b.iter(|| {
        engine.compute_pagerank(&graph_doc)
    });
}

#[bench]
fn bench_incremental_update(b: &mut Bencher) {
    let base_snapshot = generate_test_snapshot(10_000);
    let chunk_delta = generate_test_delta(100);  // 1% ë³€ê²½
    
    b.iter(|| {
        let builder = IncrementalRepoMapBuilder::from_snapshot(&base_snapshot);
        builder.update_incremental(&chunk_delta, &GraphDelta::empty())
    });
}
```

---

## ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

### Phase 1: Python ìœ ì§€ + Rust Opt-in (2ì£¼)

```python
# Python ë˜í¼ (ê¸°ì¡´ API ìœ ì§€)
class RepoMapBuilder:
    def __init__(self, config: RepoMapBuildConfig, use_rust: bool = False):
        self.use_rust = use_rust and _RUST_AVAILABLE
        
    def build(self, chunks, graph_doc):
        if self.use_rust:
            # Rust ê°€ì†
            return codegraph_ir.build_repomap_from_pipeline(chunks, graph_doc)
        else:
            # Python í´ë°±
            return self._build_python(chunks, graph_doc)
```

### Phase 2: Rust ê¸°ë³¸ê°’ (1ì£¼)

```python
# use_rust=Trueë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ
class RepoMapBuilder:
    def __init__(self, config: RepoMapBuildConfig, use_rust: bool = True):
        ...
```

### Phase 3: Python ì œê±° (1ì£¼)

```python
# Python êµ¬í˜„ deprecated
class RepoMapBuilder:
    def __init__(self, config: RepoMapBuildConfig):
        if not _RUST_AVAILABLE:
            raise RuntimeError("Rust module required. Install with: maturin develop")
        ...
```

---

## ğŸ“ ìš”ì•½

| í•­ëª© | í˜„ì¬ (Python) | ëª©í‘œ (Rust) | ê°œì„  |
|------|--------------|-------------|------|
| **LOC** | 6,149 | ~8,000 | +30% |
| **ì„±ëŠ¥** | 1x | 10-100x | ë¹ ë¦„ |
| **ì•Œê³ ë¦¬ì¦˜** | ê¸°ë³¸ PageRank | PPR + HITS + Incremental | SOTA |
| **ë³‘ë ¬í™”** | ìˆœì°¨ | Rayon | ë©€í‹°ì½”ì–´ |
| **ìºì‹œ** | ì—†ìŒ | Merkle Hash | Delta O(ë³€ê²½) |
| **Git** | ë‹¨ìˆœ ë¹ˆë„ | Churn + Hotspot | ê³ ê¸‰ |

### í•µì‹¬ ì°¨ë³„ì 

1. **Personalized PageRank**: ì¿¼ë¦¬ ê¸°ë°˜ ì¤‘ìš”ë„
2. **HITS Algorithm**: Hub/Authority ë¶„ë¦¬
3. **Incremental Merkle**: O(ë³€ê²½) ì—…ë°ì´íŠ¸
4. **Code Churn**: ë³€ê²½ëŸ‰ ì¶”ì 
5. **Hot Spot Detection**: ë¬¸ì œ ì½”ë“œ ë°œê²¬

### ì˜ˆìƒ ì¼ì •

| Phase | ê¸°ëŠ¥ | ê¸°ê°„ |
|-------|------|------|
| 1 | Tree Builder + Models | 1ì£¼ |
| 2 | PageRank (PPR + HITS) | 0.5ì£¼ |
| 3 | Git History (Churn + Hotspot) | 0.5ì£¼ |
| 4 | Incremental Update (Merkle) | 1ì£¼ |
| 5 | Storage + PyO3 | 1ì£¼ |
| 6 | íŒŒì´í”„ë¼ì¸ í†µí•© + í…ŒìŠ¤íŠ¸ | 0.5ì£¼ |
| | **ì´ê³„** | **4.5ì£¼** |

**Last Updated**: 2025-12-28  
**Author**: Claude (Opus 4.5)  
**Status**: Planning â†’ Implementation


